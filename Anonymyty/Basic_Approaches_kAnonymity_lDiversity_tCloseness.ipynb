{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neohack22/IASD/blob/projets/Anonymyty/Basic_Approaches_kAnonymity_lDiversity_tCloseness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8TgnBBu4ajC"
      },
      "source": [
        "# K-Anonymity + L-Diversity + T-Closeness\n",
        "\n",
        "In the late 1990s Sweeney published several articles in which she proposed the concept of k-anonymity. Among them we cite the most famous one [1] published in 2002. At the time when anonymization was referring to the concept now known as depseudonymization. As we already discusses pseudonymization is the replacement of all directly identifying data (such as the social security number) with a random value (pseudonym). Sweeney showed in [1] that it was possible to re-identify a pseudonymized relational database (SQL or more generally a tabular file) and proposed a simple notion to prevent these kind of leak from hapenning.\n",
        "\n",
        "## K-Anonymity\n",
        "\n",
        "Let us suppose that we have a dataset that contains $N$ entries.\n",
        "Each entry consists of a list of $D$ attributes $X_i$ ($i \\in [0,D]$) that contain (non-sensitive) information about a person, such as age, gender, zip code of residence, etc. These attributes are called \"quasi-identifiers\", as combining several of them into a \"super-identifier\" can often uniquely identify a person even in large datasets (e.g. the combination of gender, age and zip code might be so specific that only a single person in a dataset has a given combination see [1] for more details).\n",
        "\n",
        "k-anonymity protects the privacy of individual persons by pooling their attributes into groups of at least $k$ people. In addition, the model assumes that the dataset contains a single sensitive attribute that contains e.g. information about a person's income and that we want to protect. The method can also be generalized to datasets with more than one sensitive attribute or datasets where there's no clear distinction between quasi-identifiers and sensitive attributes. For this case study we will look at the simple case though. Now, k-anonymity demands that we group individual rows/persons of our dataset into group of at least $k$ rows/persons and replace the quasi-identifier attributes of these rows with aggregate quantities, such that it is no longer possible to read the individual values. This protects people by ensuring that an adversary who knows all values of a person's quasi-identifier attributes can only find out which group a person might belong to but not know if the person is really in the dataset.\n",
        "\n",
        "The main advantage of k-anonymity is that it is easy to understand. However, the model is not robust when the sensitive values associated with a given quasi-identifier value are all identical. In this case, we can deduce that all people with this quasi-identifier value have the same sensitive data, which we are able to deduce. It is therefore impossible to ensure that the risk of re-dentification is limited, which means that a use of k-anonymity alone does not offer reasonable guarantees of anonymity. This problem can be fixed by using an extension of k-anonymity called \"l-diversity\".\n",
        "\n",
        "## L-Diversity \n",
        "\n",
        "l-diversity, first introduced in 2007 in [2], ensures that each k-anonymous group contains at least l different values of the sensitive attribute. Therefore, even if an adversary can identify the group of a person he/she still would not be able to find out the value of that person's sensitive attribute with certainty. However, even when using l-diversity an adversary could still learn some information about a person's sensitive attribute using probabilistic reasoning: If, for example, 4 out of 5 people in a 5-anonymous group possess a given value of the sensitive attribute, an attacker can reason that a given person who he/she knows is part of the group will -with high probability- possess that value.\n",
        "Again, this problem can be fixed by extending k-anonymity using a so-called \"t-closeness\" criterion. \n",
        "\n",
        "## T-Closeness\n",
        "\n",
        "t-closeness, also introduced in 2007 in [3], demands that the statistical distribution of the sensitive attribute values in each k-anonymous group is \"close\" to the overall distribution of that attribute in the entire dataset. Typically, the closeness between two probability vectors $p=(p_1,...,p_l)$ and $q=(q_1,...,q_l)$ can be measured using e.g. the Kullback-Leibler (KL) divergence defined as follows: $$ D_{KL}(p,q) = \\sum_{i=1}^{l}p_i \\log(\\frac{p_i}{q_i}).$$\n",
        "An adversary could then only learn a limited amount of information from comparing the distribution of the values in the group to the distribution in the entire dataset.\n",
        "\n",
        "Then begins to ask the question of the usefulness of the data. Due to proximity constraints, the data does not necessarily seem directly usable. However, it is still possible to identify trends, or perform general calculations or correlations on the whole table.\n",
        "Of course, k-anonymity, l-diversity and t-closeness all limit the amount of information that a legitimate user can learn from the data as well, so typically we need to balance the degree of privacy against the utility of the resulting data. Eventhough the data does not necessarily seem directly usable. However, it is still possible to identify trends, or perform general calculations or correlations on the whole table.\n",
        "\n",
        "## Implementing privacy conditions\n",
        "\n",
        "Turning a dataset into a k-anonymous (and possibly l-diverse or t-close) dataset is a complex problem. Meyerson and Williams have shown in [4] that finding the optimal transformation of the database NP-difficult. Fortunately, several practical algorithms exists that often produce \"good enough\" results by employing greedy search techniques.\n",
        "\n",
        "In this notebook, we will explore the so-called \"Mondrian\" (see [5] for more details) algorithm, which uses a greedy search method to partition the original data into smaller and smaller groups. The algorithm assumes that we have converted all attributes into numerical or categorical values and that we're able to measure the \"span\" of a given data attribute $X_i$ (\"span\" will be detailed later).\n",
        "\n",
        "### Partitioning\n",
        "\n",
        "The algorithm proceeds then as follows to partition the data into k groups:\n",
        "\n",
        "1. Initialize the final set of partitions to an empty set $P_{final} = \\{\\}$.\n",
        "2. Initialize the temporary set of paritions to a set containing a partition with the entire dataset $P_{temp} = \\{\\{1, 2,\\dots ,N\\}\\}$.\n",
        "4. While there are partitions in the temporary set, pop one partition from it.\n",
        "  * Calculate the relative spans of all columns in the partition.\n",
        "  * Sort the resulting columns by their span (in descending order) and iterate over them. For each column,\n",
        "      * Try to split the partition along that column using the median of the column values as the split point.\n",
        "      * Check if the resulting partitions are valid according to k-anonymity (and possibly additional) criteria.\n",
        "      * If yes, add the two new partitions to the temporary set and break out of the loop.\n",
        "  * If no column produced a valid split, add the original partition to the set of final partitions.\n",
        "5. Return the final set of partitions\n",
        "\n",
        "### Data Aggregation\n",
        "\n",
        "After obtaining the partitions we still need to aggregate the values of the quasi identifiers and the sensitive attributes in each k-anonymous group. For this, we can e.g. replace numerical attributes with their range (e.g. \"age: 24-28\") and categorical attributes with their union (e.g. \"employment-group: [self-employed, employee, worker]\"), though other aggregations are possible. An open source tool developed by the Technological University of Munich, ARX (see [6]) allows anonymisation according to many models from original data in tabular format. The tool also makes it possible to estimate the risks of de-anonymization according to the models presented above, and even in a finer way by calculating the distribution of the probabilities of de-anonymization and not simply the maximum values.\n",
        "\n",
        "## Biliography\n",
        "\n",
        "- [1]Â [k-Anonymity: A Model For Protecting Privacy](https://epic.org/privacy/reidentification/Sweeney_Article.pdf)\n",
        "- [2] [l-Diversity: Privacy Beyond k-Anonymity](https://personal.utdallas.edu/~muratk/courses/privacy08f_files/ldiversity.pdf)\n",
        "- [3] [t-Closeness: Privacy Beyond k-Anonymity and l-Diversity](https://www.cs.purdue.edu/homes/ninghui/papers/t_closeness_icde07.pdf)\n",
        "- [4] [On the Complexity of Optimal K-Anonymity](http://www.aladdin.cs.cmu.edu/papers/pdfs/y2004/kanonim.pdf)\n",
        "- [5] [Mondrian - Multidimensional k-Anonymity](https://www.utdallas.edu/~muratk/courses/privacy08f_files/MultiDim.pdf)\n",
        "- [6] [Putting statistical disclosure control into practice: The ARX data anonymization tool](https://link.springer.com/chapter/10.1007/978-3-319-23633-9_6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkDTsJNc4ajI"
      },
      "outputs": [],
      "source": [
        "# we use Pandas to work with the data as it makes working with categorical data very easy\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we use Pandas to work with the data as it makes working with categorical data very easy\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "KoRCChxYAOnL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLLR46nq4ajJ"
      },
      "outputs": [],
      "source": [
        "# this is a list of the column names in our dataset (as the file doesn't contain any headers)\n",
        "names = (\n",
        "    'age',\n",
        "    'workclass', #Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
        "    'fnlwgt', # \"weight\" of that person in the dataset (i.e. how many people does that person represent) -> https://www.kansascityfed.org/research/datamuseum/cps/coreinfo/keyconcepts/weights\n",
        "    'education',\n",
        "    'education-num',\n",
        "    'marital-status',\n",
        "    'occupation',\n",
        "    'relationship',\n",
        "    'race',\n",
        "    'sex',\n",
        "    'capital-gain',\n",
        "    'capital-loss',\n",
        "    'hours-per-week',\n",
        "    'native-country',\n",
        "    'income',\n",
        ")\n",
        "\n",
        "# some fields are categorical and will require special treatment\n",
        "categorical = set((\n",
        "    'workclass',\n",
        "    'education',\n",
        "    'marital-status',\n",
        "    'occupation',\n",
        "    'relationship',\n",
        "    'sex',\n",
        "    'native-country',\n",
        "    'race',\n",
        "    'income',\n",
        "))\n",
        "df = pd.read_csv(\"data/adult.all.txt\", sep=\", \", header=None, names=names, index_col=False, engine='python');# We load the data using Pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is a list of the column names in our dataset (as the file doesn't contain any headers)\n",
        "names = (\n",
        "    'age',\n",
        "    'workclass', #Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
        "    'fnlwgt', # \"weight\" of that person in the dataset (i.e. how many people does that person represent) -> https://www.kansascityfed.org/research/datamuseum/cps/coreinfo/keyconcepts/weights\n",
        "    'education',\n",
        "    'education-num',\n",
        "    'marital-status',\n",
        "    'occupation',\n",
        "    'relationship',\n",
        "    'race',\n",
        "    'sex',\n",
        "    'capital-gain',\n",
        "    'capital-loss',\n",
        "    'hours-per-week',\n",
        "    'native-country',\n",
        "    'income',\n",
        ")\n",
        "\n",
        "# some fields are categorical and will require special treatment\n",
        "categorical = set((\n",
        "    'workclass',\n",
        "    'education',\n",
        "    'marital-status',\n",
        "    'occupation',\n",
        "    'relationship',\n",
        "    'sex',\n",
        "    'native-country',\n",
        "    'race',\n",
        "    'income',\n",
        "))\n",
        "df = pd.read_csv(\"/content/adult.all.txt\", sep=\", \", header=None, names=names, index_col=False, engine='python'); # We load the data using Pandas"
      ],
      "metadata": {
        "id": "hSyO-9KfAgHa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqTGCijr4ajK"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "iEgLUvGZEzVW",
        "outputId": "c50d013b-4235-4c26-abef-ac8cdecaaafb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   age         workclass  fnlwgt  education  education-num  \\\n",
              "0   39         State-gov   77516  Bachelors             13   \n",
              "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
              "2   38           Private  215646    HS-grad              9   \n",
              "3   53           Private  234721       11th              7   \n",
              "4   28           Private  338409  Bachelors             13   \n",
              "\n",
              "       marital-status         occupation   relationship   race     sex  \\\n",
              "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
              "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
              "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
              "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
              "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
              "\n",
              "   capital-gain  capital-loss  hours-per-week native-country income  \n",
              "0          2174             0              40  United-States  <=50k  \n",
              "1             0             0              13  United-States  <=50k  \n",
              "2             0             0              40  United-States  <=50k  \n",
              "3             0             0              40  United-States  <=50k  \n",
              "4             0             0              40           Cuba  <=50k  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3ad68a2e-5c42-4449-a857-2e4988f9236f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>income</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39</td>\n",
              "      <td>State-gov</td>\n",
              "      <td>77516</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>2174</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50k</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>Self-emp-not-inc</td>\n",
              "      <td>83311</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50k</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>215646</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50k</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>Private</td>\n",
              "      <td>234721</td>\n",
              "      <td>11th</td>\n",
              "      <td>7</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50k</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>Private</td>\n",
              "      <td>338409</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Wife</td>\n",
              "      <td>Black</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>Cuba</td>\n",
              "      <td>&lt;=50k</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ad68a2e-5c42-4449-a857-2e4988f9236f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3ad68a2e-5c42-4449-a857-2e4988f9236f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3ad68a2e-5c42-4449-a857-2e4988f9236f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcAiMaC64ajK"
      },
      "outputs": [],
      "source": [
        "for name in categorical:\n",
        "    df[name] = df[name].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name in categorical:\n",
        "  df[name] = df[name].astype(\n",
        "      'category'\n",
        "  )"
      ],
      "metadata": {
        "id": "Sa4Y7pekE4HN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UWEz5GH4ajL"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "**Implement a function that returns the spans (max-min for numerical columns, number of different values for categorical columns) of all columns for a partition of a dataframe.** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7B4FSIh4ajL"
      },
      "outputs": [],
      "source": [
        "def get_spans(df, partition, scale=None):\n",
        "    \"\"\"\n",
        "    :param        df: the dataframe of reference\n",
        "    :param partition: the partition for which to calculate the spans\n",
        "    :param     scale: if given, the spans of each column will be divided\n",
        "                      by the value in `scale` for that column\n",
        "    :        returns: The spans of all columns in the partition\n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozSbfdSG4ajL"
      },
      "outputs": [],
      "source": [
        "full_spans = get_spans(df, df.index)\n",
        "full_spans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXSBz0qK4ajL"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "**Implement a `split` function that takes a dataframe, a current partition and a column as inputs and returns two ensembles that split the given partition such that all rows with values of the column `column` below the median are in one ensemble and all rows with values above or equal to the median are in the other.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtY-Hh344ajM"
      },
      "outputs": [],
      "source": [
        "def split(df, partition, column):\n",
        "    \"\"\"\n",
        "    :param        df: The dataframe of reference\n",
        "    :param partition: The partition to split\n",
        "    :param    column: The column along which to split\n",
        "    :        returns: A tuple containing a split of the original partition\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBjvwbnW4ajM"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Now that we have all helper functions in place, we can implement the partition algorithm discussed above:\n",
        "\n",
        "**Implement the partitioning algorithm discussed above, using a k-anonymous criterion for the partitions you create.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPSzl0t74ajM"
      },
      "outputs": [],
      "source": [
        "def is_k_anonymous(df, partition, sensitive_column, k=3):\n",
        "    \"\"\"\n",
        "    :param               df: The dataframe on which to check the partition.\n",
        "    :param        partition: The partition of the dataframe to check.\n",
        "    :param sensitive_column: The name of the sensitive column\n",
        "    :param                k: The desired k\n",
        "    :returns               : True if the partition is valid according to our k-anonymity criteria, False otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "def partition_dataset(df, feature_columns, sensitive_column, scale, is_valid):\n",
        "    \"\"\"\n",
        "    :param               df: The dataframe to be partitioned.\n",
        "    :param  feature_columns: A list of column names along which to partition the dataset.\n",
        "    :param sensitive_column: The name of the sensitive column (to be passed on to the `is_valid` function)\n",
        "    :param            scale: The column spans as generated before.\n",
        "    :param         is_valid: A function that takes a dataframe and a partition and returns True if the partition is valid.\n",
        "    :returns               : A list of valid partitions that cover the entire dataframe.\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPSc6o4e4ajN"
      },
      "source": [
        "Now let's try this on our dataset! To keep things simple, we will at first select only two columns from the dataset that we apply the partitioning to. This makes it easier to check/visualize the result and speed up the execution (the naive algorithms can take several minutes when running on the entire dataset) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4kEKe4H4ajN"
      },
      "outputs": [],
      "source": [
        "# we apply our partitioning method to two columns of our dataset, using \"income\" as the sensitive attribute\n",
        "feature_columns = ['age', 'hours-per-week']\n",
        "sensitive_column = 'income'\n",
        "final_partitions = partition_dataset(df, feature_columns, sensitive_column, full_spans, is_k_anonymous)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHnB73jx4ajQ"
      },
      "outputs": [],
      "source": [
        "# we get the number of partitions that were created\n",
        "len(final_partitions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiYF_WZx4ajQ"
      },
      "source": [
        "Let's visualize the created partitions! To do that, we will write functions to get the rectangular bounds of a partition along two columns. We can then plot these rectangles to see how our partitioning function divides the dataset. If we perform the partition only along the two columns selected for plotting then the resulting rectangles should not overlap and cover the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-WR-DMH4ajQ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pylab as pl\n",
        "import matplotlib.patches as patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D3bivag4ajR"
      },
      "outputs": [],
      "source": [
        "def build_indexes(df):\n",
        "    indexes = {}\n",
        "    for column in categorical:\n",
        "        values = sorted(df[column].unique())\n",
        "        indexes[column] = { x : y for x, y in zip(values, range(len(values)))}\n",
        "    return indexes\n",
        "\n",
        "def get_coords(df, column, partition, indexes, offset=0.1):\n",
        "    if column in categorical:\n",
        "        sv = df[column][partition].sort_values()\n",
        "        l, r = indexes[column][sv[sv.index[0]]], indexes[column][sv[sv.index[-1]]]+1.0\n",
        "    else:\n",
        "        sv = df[column][partition].sort_values()\n",
        "        next_value = sv[sv.index[-1]]\n",
        "        larger_values = df[df[column] > next_value][column]\n",
        "        if len(larger_values) > 0:\n",
        "            next_value = larger_values.min()\n",
        "        l = sv[sv.index[0]]\n",
        "        r = next_value\n",
        "    # we add some offset to make the partitions more easily visible\n",
        "    l -= offset\n",
        "    r += offset\n",
        "    return l, r\n",
        "\n",
        "def get_partition_rects(df, partitions, column_x, column_y, indexes, offsets=[0.1, 0.1]):\n",
        "    rects = []\n",
        "    for partition in partitions:\n",
        "        xl, xr = get_coords(df, column_x, partition, indexes, offset=offsets[0])\n",
        "        yl, yr = get_coords(df, column_y, partition, indexes, offset=offsets[1])\n",
        "        rects.append(((xl, yl),(xr, yr)))\n",
        "    return rects\n",
        "\n",
        "def get_bounds(df, column, indexes, offset=1.0):\n",
        "    if column in categorical:\n",
        "        return 0-offset, len(indexes[column])+offset\n",
        "    return df[column].min()-offset, df[column].max()+offset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFLUJsx-4ajR"
      },
      "outputs": [],
      "source": [
        "feature_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BCV7dDQ4ajR"
      },
      "outputs": [],
      "source": [
        "# we calculate the bounding rects of all partitions that we created\n",
        "indexes = build_indexes(df)\n",
        "column_x, column_y = feature_columns[:2]\n",
        "rects = get_partition_rects(df, final_partitions, column_x, column_y, indexes, offsets=[0.0, 0.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z32WVJYD4ajS"
      },
      "outputs": [],
      "source": [
        "# let's see how our rectangles look like\n",
        "rects[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU3_gDd_4ajS"
      },
      "outputs": [],
      "source": [
        "# we plot the rectangles\n",
        "def plot_rects(df, ax, rects, column_x, column_y, edgecolor='black', facecolor='none'):\n",
        "    for (xl, yl),(xr, yr) in rects:\n",
        "        ax.add_patch(patches.Rectangle((xl,yl),xr-xl,yr-yl,linewidth=1,edgecolor=edgecolor,facecolor=facecolor, alpha=0.5))\n",
        "    ax.set_xlim(*get_bounds(df, column_x, indexes))\n",
        "    ax.set_ylim(*get_bounds(df, column_y, indexes))\n",
        "    ax.set_xlabel(column_x)\n",
        "    ax.set_ylabel(column_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppaomiKU4ajS"
      },
      "outputs": [],
      "source": [
        "pl.figure(figsize=(20,20))\n",
        "ax = pl.subplot(111)\n",
        "plot_rects(df, ax, rects, column_x, column_y, facecolor='r')\n",
        "#pl.scatter(df[column_x], df[column_y])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT54-KUm4ajT"
      },
      "source": [
        "**Brifely analyze the result we get.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUDIfQX04ajT"
      },
      "source": [
        "# Generating an k-Anonymous Dataset\n",
        "\n",
        "Of course, to use the data we want to produce a new dataset that contains one row for each partition and value of the sensitive attribute. To do this, we need to aggregate the columns in each partition.  Let's do this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qtr6TWBt4ajT"
      },
      "outputs": [],
      "source": [
        "def agg_categorical_column(series):\n",
        "    return [','.join(set(series))]\n",
        "\n",
        "def agg_numerical_column(series):\n",
        "    return [series.mean()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lX-eFO6W4ajT"
      },
      "outputs": [],
      "source": [
        "def build_anonymized_dataset(df, partitions, feature_columns, sensitive_column, max_partitions=None):\n",
        "    aggregations = {}\n",
        "    for column in feature_columns:\n",
        "        if column in categorical:\n",
        "            aggregations[column] = agg_categorical_column\n",
        "        else:\n",
        "            aggregations[column] = agg_numerical_column\n",
        "    rows = []\n",
        "    for i, partition in enumerate(partitions):\n",
        "        if i % 100 == 1:\n",
        "            print(\"Final {} partitions...\".format(i))\n",
        "        if max_partitions is not None and i > max_partitions:\n",
        "            break\n",
        "        grouped_columns = df.loc[partition].agg(aggregations, squeeze=False)\n",
        "        sensitive_counts = df.loc[partition].groupby(sensitive_column).agg({sensitive_column : 'count'})\n",
        "        values = grouped_columns.iloc[0].to_dict()\n",
        "        for sensitive_value, count in sensitive_counts[sensitive_column].items():\n",
        "            if count == 0:\n",
        "                continue\n",
        "            values.update({\n",
        "                sensitive_column : sensitive_value,\n",
        "                'count' : count,\n",
        "\n",
        "            })\n",
        "            rows.append(values.copy())\n",
        "    return pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1czYA_d4ajU"
      },
      "outputs": [],
      "source": [
        "dfn = build_anonymized_dataset(df, final_partitions, feature_columns, sensitive_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbPRHK6h4ajU"
      },
      "outputs": [],
      "source": [
        "# we sort the resulting dataframe using the feature columns and the sensitive attribute\n",
        "dfn.sort_values(feature_columns+[sensitive_column])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPY1T3t14ajU"
      },
      "source": [
        "# Implementing l-diversity (the naive way)\n",
        "\n",
        "Now let's see how we can implement l-diversity in order to protect the privacy of the persons in the dataset even better. To implement l-diversity, we can do the following things:\n",
        "\n",
        "* Modify our `is_valid` function to not only check for the size of a given partition but also ensure that the values of the sensitive attribute in the partition are diverse enough.\n",
        "* Modify the `split` function to produce splits that are diverse (if possible)\n",
        "\n",
        "Here we will only implement the first point to keep things simple, please keep in mind that this is not the smartest way to implement l-diversity, as our \"naive\" splitting function might produce invalid splits even when it would actually be possible to produce a valid one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9JqABPx4ajU"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "**Implement a validator function that returns `True` if a given partition contains at least `l` different values of the sensitive attribute, `False` otherwise.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohIOhZ_44ajV"
      },
      "outputs": [],
      "source": [
        "def is_l_diverse(df, partition, sensitive_column, l=2):\n",
        "    \"\"\"\n",
        "    :param               df: The dataframe for which to check l-diversity\n",
        "    :param        partition: The partition of the dataframe on which to check l-diversity\n",
        "    :param sensitive_column: The name of the sensitive column\n",
        "    :param                l: The minimum required diversity of sensitive attribute values in the partition\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPyi4_1V4ajV"
      },
      "outputs": [],
      "source": [
        "# now let's apply this method to our data and see how the result changes\n",
        "final_l_diverse_partitions = partition_dataset(df, feature_columns, sensitive_column, full_spans, lambda *args: is_k_anonymous(*args) and is_l_diverse(*args))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ao5kSVKp4ajV"
      },
      "outputs": [],
      "source": [
        "len(final_l_diverse_partitions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1c2yL3c4ajW"
      },
      "outputs": [],
      "source": [
        "column_x, column_y = feature_columns[:2]\n",
        "l_diverse_rects = get_partition_rects(df, final_l_diverse_partitions, column_x, column_y, indexes, offsets=[0.0, 0.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lpm4jClK4ajW"
      },
      "outputs": [],
      "source": [
        "pl.figure(figsize=(20,20))\n",
        "ax = pl.subplot(111)\n",
        "plot_rects(df, ax, l_diverse_rects, column_x, column_y, edgecolor='b', facecolor='b')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxXARFQ94ajW"
      },
      "source": [
        "**Brifely analyze the result we get. Compare with simple k-anonymity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXqxCIK74ajW"
      },
      "outputs": [],
      "source": [
        "# again we build an anonymized dataset from the l-diverse partitions\n",
        "dfl = build_anonymized_dataset(df, final_l_diverse_partitions, feature_columns, sensitive_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-rT_9uU4ajX"
      },
      "outputs": [],
      "source": [
        "# Let's see how l-diversity improves the anonymity of our dataset\n",
        "dfl.sort_values([column_x, column_y, sensitive_column])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeZoIDxr4ajX"
      },
      "source": [
        "# Implementing t-closeness\n",
        "\n",
        "As we can see, for regions where the value diversity is low, our l-diverse method produces partitions that contain a very large number of entries for one value of the sensitive attribute and only one entry for the other value. This is not ideal as while there is \"plausible deniability\" for a person in the dataset (after all the person could be the one \"outlier\") but an adversary can still be very certain about the person's attribute value in that case.\n",
        "\n",
        "t-closeness solves this problem by making sure that the distribution of sensitive attribute values in a given partition is similar to the distribution of the values in the overall dataset. We'll implement a naive (and not efficient / correct) version of t-closeness below. As with the l-diversity case, it would be better to tailor the `split` function to produce partitions that are t-close, which would increase the efficiency of the method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCOE2uwt4ajX"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "**Implement a version of the `is_valid` function that returns `True` if the partition is diverse enough and `False` otherwise. To measure diversity, calculate the total variation distance (easier to implement compared to KL) between the empirical probability distribution of the sensitive attribute over the entire dataset vs. the distribution over\n",
        "the partition. Hint: the total variation distance is the maximum pointwise absolute difference between the two distributions. You can assume that the sensitive attribute is a categorical value.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMma82yI4ajX"
      },
      "outputs": [],
      "source": [
        "# here we generate the global frequencies for the sensitive column \n",
        "global_freqs = {}\n",
        "total_count = float(len(df))\n",
        "group_counts = df.groupby(sensitive_column)[sensitive_column].agg('count')\n",
        "for value, count in group_counts.to_dict().items():\n",
        "    p = count/total_count\n",
        "    global_freqs[value] = p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODXFsmNP4ajY"
      },
      "outputs": [],
      "source": [
        "global_freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eom6Jx8y4ajY"
      },
      "outputs": [],
      "source": [
        "def is_t_close(df, partition, sensitive_column, global_freqs, p=0.2):\n",
        "    \"\"\"\n",
        "    :param               df: The dataframe for which to check l-diversity\n",
        "    :param        partition: The partition of the dataframe on which to check l-diversity\n",
        "    :param sensitive_column: The name of the sensitive column\n",
        "    :param     global_freqs: The global frequencies of the sensitive attribute values\n",
        "    :param                p: The maximum allowed distance\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65HRc3DS4ajY"
      },
      "outputs": [],
      "source": [
        "# Let's apply this to our dataset\n",
        "final_t_close_partitions = partition_dataset(df, feature_columns, sensitive_column, full_spans, lambda *args: is_k_anonymous(*args) and is_t_close(*args, global_freqs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVvrr-Oj4ajY"
      },
      "outputs": [],
      "source": [
        "len(final_t_close_partitions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWSnzkkz4ajZ"
      },
      "outputs": [],
      "source": [
        "dft = build_anonymized_dataset(df, final_t_close_partitions, feature_columns, sensitive_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfLJow3O4ajZ"
      },
      "outputs": [],
      "source": [
        "# Let's see how t-closeness fares\n",
        "dft.sort_values([column_x, column_y, sensitive_column])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mu-cmshw4ajZ"
      },
      "outputs": [],
      "source": [
        "column_x, column_y = feature_columns[:2]\n",
        "t_close_rects = get_partition_rects(df, final_t_close_partitions, column_x, column_y, indexes, offsets=[0.0, 0.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96gcFTAk4ajZ"
      },
      "outputs": [],
      "source": [
        "pl.figure(figsize=(20,20))\n",
        "ax = pl.subplot(111)\n",
        "plot_rects(df, ax, t_close_rects, column_x, column_y, edgecolor='b', facecolor='b')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2lh9odE4ajZ"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "**Brifely analyze the result we get. Compare with simple l-diversity and simple k-anonymity**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}