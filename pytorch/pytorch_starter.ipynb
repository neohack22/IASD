{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neohack22/IASD/blob/NLP/pytorch/pytorch_starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaRFGMtQgzac"
      },
      "source": [
        "This notebook is a simple introduction to pytorch, assuming you already know  python, numpy and the notebooks. \n",
        "\n",
        "\n",
        "To start with pytorch, here are some external websites: \n",
        "* http://pytorch.org/tutorials/ : official tutorials\n",
        "* http://pytorch.org/docs/master/ : official documentation\n",
        "\n",
        "Before, check the version of pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCgUbQCVgzai",
        "outputId": "5b7789ec-fb07-4f3c-f0e2-bb9e70c5f6fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.7.1\n"
          ]
        }
      ],
      "source": [
        "import torch as th\n",
        "print(th.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mntzM1ngzak"
      },
      "source": [
        "You should have a version of at least 1.0.0. \n",
        "\n",
        "\n",
        "## Tensor  overview\n",
        "For users who are familiar with numpy arrays, the PyTorch **Tensor** class is very similar. PyTorch is like NumPy, but with GPU acceleration and automatic computation of gradients. This  makes it suitable for deep learning: calculating backward pass data automatically starting from a forward expression.\n",
        "\n",
        "The forward pass is implemented as a computation graph. The **Tensor** is the basic piece of this computation graph, to encode the data (input/output) and the parameters of the model. \n",
        "A Tensor is both a tensor (like a numpy array or a matlab matrix) and a variable (or a node) of the computation graph. A Tensor can store data and the associated gradients.\n",
        "\n",
        "\n",
        "\n",
        "**IMPORTANT NOTE: ** Since torch 0.3, a **Tensor** is a **Variable** that wraps a tensor. Before these 2 concepts were separated. \n",
        "\n",
        "## Module overview\n",
        "\n",
        "A module is a part of a NNet. It may contains Tensors. The core PyTorch modules for building neural networks are located in *torch.nn*, which provides common neural network layers and other architectural components. Fully connected layers, convolutional layers, activation functions, and loss functions can all be found here. Modules can be seen as pre-built pieces of computation graph. \n",
        "\n",
        "A simple example of *module* is **Linear**: it's a fully connected layer, so a linear transformation of the input. It contains a matrix of parameters (a Tensor). Activation function are also *module*. You can therefore create a cascade of *Linear* module with a *Sigmoid*, for example. \n",
        "\n",
        "A special kind of module is a *container* : a module that contains other module. The most widely used is **Sequential**: it's a container to implement a feed-forward network. When you create a **Sequential** object you pass him an ordered list of modules to create the cascade of operation. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Tensor\n",
        "\n",
        "To start with  *Tensor*s, read this link first :\n",
        "http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html. \n",
        "and then look at the operations on tensors:  http://pytorch.org/docs/master/torch.html. \n",
        "\n",
        "## Basics\n",
        "You should know how to : \n",
        "* Build a tensor of dimensions (2,3) filled with integers from 1  to 6. \n",
        "* Convert this  Tensor in array numpy and back. \n",
        "* Compute the sum of its elements, the sum per rows and per columns. \n",
        "* Build a tensor of dimensions (3,2) filled with random numbers. Numbers are drawn from the uniform distribution on [0,1]\n",
        "* Same with a gaussian  (mean=0, variance=1). \n",
        "\n",
        "Your turn to code but remember: \n",
        "- you can use auto-completion\n",
        "- ask for help, like this : \n",
        "But in most of the case it is easier to use the online documentation of the function: https://pytorch.org/docs/stable/torch.html#torch.arange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cs57xf6pgzam"
      },
      "outputs": [],
      "source": [
        "# A hint \n",
        "help(th.arange)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcSshZ5Sgzan"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpTKOWn0gzao"
      },
      "source": [
        "## Operation and access\n",
        "\n",
        "* Extract the first row and the last row (do the same with columns)\n",
        "* Build a matrix  A of dimension (2,3), a matrix  B (2,1) et and  C (1,4) with random initialisation. \n",
        "* Concatenate A with B, and the results with avec C. \n",
        "* Create A (5,4), then B (3,4) which contains in this order: the second, the first and the fourth row of A. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peypBZfSgzao"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuY5fzV7gzap"
      },
      "source": [
        "Look at the following code and how  x2 is built from x. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lf5jteHGgzaq"
      },
      "outputs": [],
      "source": [
        "x = th.randn(5, 4)\n",
        "print(x)\n",
        "\n",
        "x2= th.stack((x,x) , dim=0)\n",
        "print (x2[0]) \n",
        "print (x2.size()) \n",
        "\n",
        "x2= th.stack((x,x) , dim=1)\n",
        "print (x2[0]) \n",
        "print (x2.size()) \n",
        "\n",
        "x2= th.stack((x,x) , dim=2)\n",
        "print (x2[0]) \n",
        "print (x2.size())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5hXbDc2gzaq"
      },
      "source": [
        "## Reshape \n",
        "\n",
        "The method ** .view() ** is similar to *reshape*. This is **important** since with neural net, you will often need to play with dimensions. \n",
        "\n",
        "* Build a tensor of size (2, 3, 4)\n",
        "* Convert it in a matrix of dimension (3,8) and (2,12)\n",
        "* What does  *view(2,-1)*  do ? \n",
        "\n",
        "Be careful: the view only change how the data viewed. Look at the following example and be sure to understand. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH91MQMdgzar",
        "outputId": "e3a96555-30fc-42a7-bd2f-0d2c4239fc08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11],\n",
            "        [12, 13, 14, 15],\n",
            "        [16, 17, 18, 19],\n",
            "        [20, 21, 22, 23],\n",
            "        [24, 25, 26, 27]])\n",
            "tensor([[ 0,  1,  2,  3,  4,  5,  6],\n",
            "        [ 7,  8,  9, 10, 11, 12, 13],\n",
            "        [14, 15, 16, 17, 18, 19, 20],\n",
            "        [21, 22, 23, 24, 25, 26, 27]])\n",
            "tensor([[ 0,  4,  8, 12, 16, 20, 24],\n",
            "        [ 1,  5,  9, 13, 17, 21, 25],\n",
            "        [ 2,  6, 10, 14, 18, 22, 26],\n",
            "        [ 3,  7, 11, 15, 19, 23, 27]])\n"
          ]
        }
      ],
      "source": [
        "a = th.arange(28).view(7,4)\n",
        "print(a)\n",
        "b = a.view(4,7)\n",
        "print(b)\n",
        "print(a.t())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBrFJKtFgzar"
      },
      "source": [
        "## Auto-grad\n",
        "\n",
        "`torch.autograd` provides classes and functions implementing automatic differentiation. \n",
        "When a tensor is created with `requires_grad=True`, the object will be able to store information about the gradient. \n",
        "In the following example, we build a computational graph. The \"end\" must be a scalar for automatic differentiation. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXG8PXTDgzas"
      },
      "outputs": [],
      "source": [
        "x = th.randn(2, 2, requires_grad=True)\n",
        "print(x)\n",
        "out = x.pow(2).sum() # out is a new variable (scalar)\n",
        "out.backward()       # back propagation in the graph\n",
        "print(x.grad)        # the gradient of out with respect to x \n",
        "print(x)             # A simple check "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfzrEmHMgzas"
      },
      "source": [
        "# A simple dataset\n",
        "Let start with an easy dataset for binary classification. The following subsections just provide a dummy dataset and function to visualize the data-set. \n",
        "\n",
        "\n",
        "\n",
        "## Create the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVlKoSQsgzas"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "ST1 = np.array([[17.0 ,12 ,13 ,15 ,15 ,20 ,20],[ 10 ,12 ,14 ,15 ,20 ,15 ,20]]) # class 1 \n",
        "ST2 = np.array([4, 7.5, 10 ,11, 5 ,5 ,6, 8, 5, 0, 5, 0, 10, 6]).reshape(2,7) # class 2 \n",
        "Xstudents = np.concatenate((ST1,ST2),axis=1)\n",
        "\n",
        "Ystudents = np.ones(14)\n",
        "Ystudents[7:] = 0\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRo01wG6gzat"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-aUNVdqgzat"
      },
      "source": [
        "# pyTorch\n",
        "\n",
        "\n",
        "Define a logistic regression model with pytorch, learn it and vizualise the result. \n",
        "The roadmap is: \n",
        "* A simple neural model can rely on  **Sequential**. A model handles  **Tensors**. The data for a model should be converted into Tensors. Start by this transformation. \n",
        "* Create a regression model  (a single neuron with the logistic activation function, or a linear layer with one single neuron with the logistic activation). \n",
        "* Define the '**optimizer** (Take the basic Stochastic Gradient Descent). \n",
        "* Define the objective function\n",
        "* Write the training loop and run it until convergence. It can be useful to play with learning rate. Run the gradient descent example by example. \n",
        "* Look at the solution \n",
        "* Start again in  **batch** mode (the gradient is estimated on the whole training set).\n",
        "\n",
        "\n",
        "\n",
        "## From data to tensors / variables \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4RFIUihgzat",
        "outputId": "7c75ddc7-b1ec-481e-8427-8583c880d2d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[17.0000, 10.0000],\n",
            "        [12.0000, 12.0000],\n",
            "        [13.0000, 14.0000],\n",
            "        [15.0000, 15.0000],\n",
            "        [15.0000, 20.0000],\n",
            "        [20.0000, 15.0000],\n",
            "        [20.0000, 20.0000],\n",
            "        [ 4.0000,  8.0000],\n",
            "        [ 7.5000,  5.0000],\n",
            "        [10.0000,  0.0000],\n",
            "        [11.0000,  5.0000],\n",
            "        [ 5.0000,  0.0000],\n",
            "        [ 5.0000, 10.0000],\n",
            "        [ 6.0000,  6.0000]])\n",
            "Y= tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.]) torch.Size([14])\n",
            "One i/O\n",
            "tensor([12., 12.]) torch.Size([2])\n",
            "tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "X = th.FloatTensor(Xstudents.T)\n",
        "print(X)\n",
        "\n",
        "Y=th.FloatTensor(Ystudents)\n",
        "print(\"Y=\",Y,Y.shape)\n",
        "print (\"One i/O\")\n",
        "print(X[1],X[1].shape)\n",
        "print(Y[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiV82sUcgzau"
      },
      "source": [
        "## Create the model, its loss and optimizer\n",
        "\n",
        "The model is a linear transformation followed by a Sigmoid function. This is equivalent to a logistic regression model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMIUxIBZgzau",
        "outputId": "563144fc-853e-4e38-844f-e39948930602"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
            "  (1): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# The model \n",
        "D_in=2  # input size : 2 \n",
        "D_out=1 # output size: one value \n",
        "model = th.nn.Sequential(\n",
        "    th.nn.Linear(D_in, D_out),\n",
        "    th.nn.Sigmoid()    \n",
        ")\n",
        "print(model)\n",
        "\n",
        "\n",
        "loss_fn = th.nn.BCELoss() # The binary cross entropy \n",
        "learning_rate = 1e-2\n",
        "optimizer = th.optim.SGD(model.parameters(), lr=learning_rate) # "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwOzOJg5gzau"
      },
      "source": [
        "A *Sequential* module is a container of modules. The inner modules are stored in a kind of array and the inference corresponds to a feed-forward (ordered) cascade of modules. Here an input given to model, first go through the *Linear* module and then *Sigmoid*. \n",
        "\n",
        "A *Linear* layer wraps a linear tranformation, along with the matrix. The inference (or *forward*) operation is simply the matrix multiplication with the input. \n",
        "\n",
        "The *Sigmoid* module is simply an activation function (component wise), without parameters. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H777H0ygzau",
        "outputId": "6b9be9d2-672c-4f4b-aa2a-fbc5e6e2facb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ -0.5260, -11.8948,   4.8465,  13.4865,  12.5495],\n",
            "       grad_fn=<AddBackward0>)\n",
            "Parameter containing:\n",
            "tensor([[-0.2616,  0.4014],\n",
            "        [-0.6992, -0.0686],\n",
            "        [ 0.2492,  0.1227],\n",
            "        [ 0.5410,  0.4781],\n",
            "        [ 0.5227,  0.3036]], requires_grad=True)\n",
            "torch.Size([5, 2])\n"
          ]
        }
      ],
      "source": [
        "lin_transform = th.nn.Linear(2,5) # a linear transformation from 2 to 5\n",
        "out = lin_transform(X[0])         # apply it to a vector from X\n",
        "print(out)                        # the output is of dimension 5\n",
        "print(lin_transform.weight)       # the parameters is a matrix of dimension \n",
        "print(lin_transform.weight.shape) # 5 rows (output dim), 2 columns (input dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vYeiSVFgzav",
        "outputId": "63d8740e-aee0-4bcc-b9d6-d475ec58c72a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([14, 1])\n",
            "Parameter containing:\n",
            "tensor([[-0.1814,  0.1146]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# In the sequential model you can access to each submodule:  \n",
        "lin_transform = model[0]\n",
        "A = lin_transform(X)\n",
        "print(A.shape)\n",
        "print(lin_transform.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhXwcB8Ggzav"
      },
      "source": [
        "## Testing the model with data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u351CNdggzaw"
      },
      "outputs": [],
      "source": [
        "# With a single input vector \n",
        "prediction = model(X[0]) # or prediction = model.forward(X[0]) both are equivalent\n",
        "\n",
        "print(\"For the first input: \",prediction)\n",
        "\n",
        "# With 3 input vectors \n",
        "prediction = model(X[0:3])\n",
        "print(\"For the 3 first inputs: \",prediction)\n",
        "\n",
        "# For the whole dataset\n",
        "prediction = model(X)\n",
        "print(\"For all: \",prediction)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YOOT1E-gzaw"
      },
      "outputs": [],
      "source": [
        "# With a single input vector \n",
        "prediction = model(X[0])\n",
        "print(\"The first prediction: \",prediction, prediction.shape)\n",
        "print(\"The reference: \",Y[0], Y[0].shape)\n",
        "\n",
        "loss_fn(prediction,Y[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR0akb9Cgzax"
      },
      "source": [
        "This code should generate a warning, since the label (or target value) and the prediction (considered as the input value of the loss) are of different dimensions. \n",
        "\n",
        "There is two ways to fix that. The first one is to reduce the input dimension using *squeeze*. The second one is to modify the target values. See the two cells below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szrAMCc0gzax"
      },
      "outputs": [],
      "source": [
        "prediction = model(X[0]).squeeze()\n",
        "print(\"The first prediction: \",prediction, prediction.shape)\n",
        "print(\"The reference: \",Y[0], Y[0].shape)\n",
        "\n",
        "loss_fn(prediction,Y[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhO7EDE-gzax"
      },
      "outputs": [],
      "source": [
        "prediction = model(X[0])\n",
        "Ymodified = Y.view(-1,1)\n",
        "print(\"The first prediction: \",prediction, prediction.shape)\n",
        "print(\"The reference: \",Ymodified[0], Ymodified[0].shape)\n",
        "\n",
        "loss_fn(prediction,Ymodified[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFENesNggzay"
      },
      "source": [
        "## Training loop\n",
        "\n",
        "Now we have everything to train the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nH9P8MdRgzay"
      },
      "outputs": [],
      "source": [
        "Nepochs = 1000 # number of epochs \n",
        "Nprint  = Nepochs/10  # frequence of print \n",
        "for epoch in range(Nepochs):\n",
        "    total=0.\n",
        "    for i in range(14):\n",
        "        prediction = model(X[i]).squeeze()    \n",
        "        loss = loss_fn(prediction, Y[i])\n",
        "        optimizer.zero_grad()  \n",
        "        loss.backward()        \n",
        "        optimizer.step()\n",
        "        total+=loss\n",
        "    if epoch%Nprint==0:\n",
        "        print(epoch,total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6msziDMgzay"
      },
      "source": [
        "## Explore the \"solution\" \n",
        "\n",
        "Here, we look at the different wrapping steps: \n",
        "- The model is a set of modules\n",
        "- A Linear module is a matrix of weights along with a bias vector. They are parameters.\n",
        "- A Parameter wrap a tensor\n",
        "- A tensor can be casted as a numpy array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47P1AsWpgzay"
      },
      "outputs": [],
      "source": [
        "mod = model[0]\n",
        "print(type(mod))\n",
        "print(type(mod.bias))\n",
        "print(type(mod.bias.data))\n",
        "print(type(mod.bias.data.numpy()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v_CLYNLgzaz"
      },
      "source": [
        "We can look at the parameters: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLm1H2xUgzaz"
      },
      "outputs": [],
      "source": [
        "print(mod.bias.data.view(1,1))\n",
        "print(mod.weight.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "00S9mYI6gzaz"
      },
      "source": [
        "## Impact of the learning rate \n",
        "\n",
        "Now, we will use the same model trained with a different learning rate. The training process restarts from scratch. We need to therefore to re-create the model and the associated optimizer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UHg8Vfkgzaz"
      },
      "outputs": [],
      "source": [
        "model = th.nn.Sequential(\n",
        "    th.nn.Linear(D_in, D_out),\n",
        "    th.nn.Sigmoid()    \n",
        ")\n",
        "learning_rate = 1e-1\n",
        "optimizer = th.optim.SGD(model.parameters(), lr=learning_rate) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2y3UZX_gza0"
      },
      "source": [
        "We now have the same model as before, randomly initialized. We train this same model with a different learning rate, a larger one. \n",
        "\n",
        "- Run the training with the same number of epochs and compare the loss value we get at the end\n",
        "- Do you think we can reach the same value with the learning rate of 1e-2, but with a longer training ? \n",
        "- Try the same thing with a learning rate of 0.5, what do you observe ? \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xHhYDTvgza0"
      },
      "outputs": [],
      "source": [
        "Nepochs = 1000 # number of epochs \n",
        "Nprint  = Nepochs/10  # frequence of print \n",
        "for epoch in range(Nepochs):\n",
        "    total=0.\n",
        "    for i in range(14):\n",
        "        prediction = model(X[i]).squeeze()    \n",
        "        loss = loss_fn(prediction, Y[i])\n",
        "        optimizer.zero_grad()  \n",
        "        loss.backward()        \n",
        "        optimizer.step()\n",
        "        total+=loss\n",
        "    if epoch%Nprint==0:\n",
        "        print(epoch,total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeSMvi9Fgza0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}