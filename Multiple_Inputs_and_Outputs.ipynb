{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multiple_Inputs_and_Outputs.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neohack22/IASD/blob/DeepLearning/Multiple_Inputs_and_Outputs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlwPqocaXRKz",
        "colab_type": "text"
      },
      "source": [
        "# Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17XDM1AaXUas",
        "colab_type": "text"
      },
      "source": [
        "## model in a few lines with the Functional API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7qji33jW2Ac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of unique issue tags\n",
        "num_tags = 12\n",
        "\n",
        "# Size of vocabulary obtained when preprocessing text data\n",
        "num_words = 10000\n",
        "\n",
        "# Number of departments for predictions\n",
        "num_departments = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN3u34L0Xv79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Variable_length sequence of ints\n",
        "title_input = keras.Input(shape=(None,), name='title')\n",
        "\n",
        "# Variable-length sequence of ints\n",
        "body_input = keras.Input(shape=(None,) name='body')\n",
        "\n",
        "# Binary vectors of size `num_tags`\n",
        "tags_input = keras.Input(shape=(num_tags,), name='tags')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4jSPyJVYcBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Embed each word in the title into a 64-dimensional vector\n",
        "title_features = layers.Embedding(num_words, 64)(title_input)\n",
        "\n",
        "# Embed each word in the text into a 64-dimensional vector\n",
        "body_features = layers.Embedding(num_words, 64)(body_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sodZslYRZazK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Reduce sequence of embedded words in the title into \n",
        "a single 128-dimensional vector\"\"\"\n",
        "title_features = layers.LSTM(128)(title_features)\n",
        "\n",
        "\"\"\" Reduce sequence of embedded words in the body \n",
        "into a single 32_dimensional vector \"\"\"\n",
        "body_features = layers.LSTM(32)(body_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIwtpJK2aFxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Merge all available features \n",
        "into a single large vector via concatenation \"\"\"\"\n",
        "x = layers.concatenate([title_features, body_features, tags_input])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35QfTXVtaYFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Stick a logistic regression for priority prediction \n",
        "on top of the features \"\"\"\"\n",
        "priority_pred = \\\n",
        "layers.Dense(1, activation='sigmoid', name='priority')(x)\n",
        "\n",
        "# Stick a department classifier on top of the features\n",
        "department_pred = \\\n",
        "layers.Dense(num_departments, activation='softmax', \\\n",
        "             name='department')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7vAfhEIbJ3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate an end-to-end model predicting both priority and department\n",
        "model = keras.Model(inputs=[title_input, body_input, tags_input],\n",
        "                    outputs=[priority_pred, department_pred])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeSm2E44blwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras.utils.plot_model(model, 'multi_input_and_output_model.png', \n",
        "                       show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mosGSOVpb9Eu",
        "colab_type": "text"
      },
      "source": [
        "## compiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm0xUSYxb-gj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "# iso\n",
        "model.compile(optimizer=keras.optyimizers.RMSprop(1e-3),\n",
        "              loss=['binary_crossentropy', 'categorical_crossentropy'],\n",
        "              loss_weights=[1., 0.2])\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy8_949tcyZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specify the loss\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "              loss={'priority': 'binary_crossentropy',\n",
        "                    'department': 'categorical_crossentropty'},\n",
        "              loss_weights=[1., 0.2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2Y63hvGdTV3",
        "colab_type": "text"
      },
      "source": [
        "## train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRQdy1kndUNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dummy input data\n",
        "title_data = np.random.randint(num_words, size=(1280,10))\n",
        "body_data = np.random.randint(num_words, size=(1280, 100))\n",
        "tags_data = np.random.randint(2, size=(1280, num_tags)).astype('float32')\n",
        "\n",
        "# Dummy target data\n",
        "priority_targets = np.random.random(size=(1280, 1))\n",
        "dept_targets = np.random.randint(2, size=(1280, num_departments))\n",
        "\n",
        "model.fit({'title': title_data, 'body': body_data, 'tags': tags_data},\n",
        "          {'priority': priority_targets, 'department': dept_targets},\n",
        "          epochs=2,\n",
        "          batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}