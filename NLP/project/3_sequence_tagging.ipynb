{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neohack22/IASD/blob/NLP/NLP/project/3_sequence_tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDXAKn0HaHlg"
      },
      "source": [
        "# Sequence models and recurrent networks \n",
        "\n",
        "## Preliminary remarks\n",
        "\n",
        "Recurrent networks in *pytorch* expects as input a Tensor in 3 dimensions (*3D tensor*). The axes carry an important semantic: \n",
        "- the first axis is \"the time\" \n",
        "- the second one corresponds to the mini-batch\n",
        "- the third corresponds to the dimension of input vectors (typically the embedding size)\n",
        "\n",
        "\n",
        "Therefore, a sequence of 5 vectors of 4 features (size 4) is represented as a Tensor of dimensions (5,1,4). If we have 7 sequences of 5 vectors, all of size 4, we get (5,7,4). \n",
        "\n",
        "Lets start with some simple code with synthetic data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uCwYoumraHll",
        "outputId": "1d8a799e-75a5-4ea5-e28b-af35d07445dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x108ee42d0>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title\n",
        "import pickle # for the real data \n",
        "import torch  # Torch + shortcuts\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(1) # To reproduce the experiments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle # for the real data\n",
        "import torch # Torch + shortcuts\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(1) # To reproduce the experiments"
      ],
      "metadata": {
        "id": "0YG0zsrTuiHh",
        "outputId": "984677c2-1017-418f-b35a-70bf0fc91857",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f7ec64012f0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JslvbGSoaHlo",
        "outputId": "825337c0-6a9d-48ec-9100-f0a2703ef09e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input sequence : tensor([[[-1.5256, -0.7502, -0.6540, -1.6095]],\n",
            "\n",
            "        [[ 0.8657,  0.2444, -0.6629,  0.8073]],\n",
            "\n",
            "        [[ 0.4391,  1.1712,  1.7674, -0.0954]],\n",
            "\n",
            "        [[ 0.0612, -0.6177, -0.7981, -0.1316]],\n",
            "\n",
            "        [[-0.7984,  0.3357,  0.2753,  1.7163]]])\n",
            "The shape :  torch.Size([5, 1, 4])\n"
          ]
        }
      ],
      "source": [
        "#@title\n",
        "inputs = torch.randn((5,1,4))\n",
        "print(\"input sequence :\", inputs)\n",
        "print(\"The shape : \", inputs.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.randn((5, 1, 4)) # = torch.randn((5, 1, 4))\n",
        "print(\"input sequence :\", inputs) #input sequence :\", inputs\")\n",
        "print(\"The shape : \", inputs.shape)"
      ],
      "metadata": {
        "id": "q8Wn5_kOvkUw",
        "outputId": "58ad70ca-727e-4ffd-9c76-9642bc096df5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input sequence : tensor([[[ 0.1991,  0.0457,  0.1530, -0.4757]],\n",
            "\n",
            "        [[ 0.2152, -0.5242, -1.8034, -1.3083]],\n",
            "\n",
            "        [[ 0.4533,  1.1422,  0.2486, -1.7754]],\n",
            "\n",
            "        [[ 1.1173,  0.2981,  0.1099, -0.6463]],\n",
            "\n",
            "        [[ 0.4285,  1.4761, -1.7869,  1.6103]]])\n",
            "The shape :  torch.Size([5, 1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw6DqLseaHlp"
      },
      "source": [
        "## A simple recurrent model and  LSTM\n",
        "\n",
        "A simple recurrent network is for instance of the thpe **nn.RNN**. \n",
        "To build it, we must specify: \n",
        "- the input size (this implies the size of the Linear Layer that will process input vectors);\n",
        "- the size of the hidden layer (this implies the size of the Linear Layer that will process the time transition). \n",
        "\n",
        "Other options are available and useful, like:\n",
        "- nonlinearity \n",
        "- bias\n",
        "- batch_first \n",
        "\n",
        "\n",
        "The forward function of a recurrent net can handle two types of input and therefore acts in two ways. \n",
        "\n",
        "### One step forward\n",
        "The first one corresponds to one time step: the neural networks reads one input symbol and update the hidden layer. The forward function therefore returns a tuple of two Tensors: the output and the updated hidden layer. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfG6Ok2NaHlq",
        "outputId": "948a40e8-7570-46a9-8777-63caa1f4f7f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "h0 :  tensor([[[-0.8737, -0.2693, -0.5124]]]) torch.Size([1, 1, 3])\n",
            "##################\n",
            "One step returns: \n",
            "  1/  output :  tensor([[[-0.6307, -0.0205,  0.0848]]], grad_fn=<StackBackward>) torch.Size([1, 1, 3])\n",
            "  2/  hidden :  tensor([[[-0.6307, -0.0205,  0.0848]]], grad_fn=<StackBackward>) torch.Size([1, 1, 3])\n",
            "##################\n"
          ]
        }
      ],
      "source": [
        "recNN = nn.RNN(input_size=4, hidden_size=3)  # Input dim is 4, hidden layer size  is 3\n",
        "\n",
        "# initialize the hidden state.\n",
        "h0 = torch.randn(1, 1, 3) # \n",
        "print(\"h0 : \",h0,h0.shape)\n",
        "\n",
        "# One step \n",
        "out, hn = recNN(inputs[0].view(1,1,-1), h0)\n",
        "print(\"##################\")\n",
        "print(\"One step returns: \")\n",
        "print(\"  1/  output : \", out, out.shape)\n",
        "print(\"  2/  hidden : \", hn, hn.shape)\n",
        "print(\"##################\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpufTZUBaHlr"
      },
      "source": [
        "We can observe that both vectors are the same. Indeed, in a simple recurrent network there is no distinction between the output and the hidden layers.  A prediction can be done by taking into account at each time step this hidden layer: \n",
        "\n",
        "$$ h_t = f_1(x_t,h_{t-1})$$\n",
        "$$ y_t = f_2(h_t)$$\n",
        "\n",
        "For one step forward, the recurrent net only needs to keep track of the hidden layer. Some more advanced architectures, like **LSTM** use  two kinds of hidden layers: one for the memory managment  named **cell state** (or $c_t$), and the other to make the prediction named  **hidden state** (or $h_t$). The API is generic for all the recurrent nets et returns a tuple at each time step. This tuple gathers the sufficient data to unfold the network. \n",
        "\n",
        "### Sequence forward (unfold)\n",
        "The second \"style\" of the forward function consists in taking as input a sequence and to unfold the network on this input sequence. It is equivalent to a for loop. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxlyuzJNaHls",
        "outputId": "06cbed63-da64-417b-da2b-d0402c25cdb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* outputs:\n",
            " tensor([[[ 0.6385, -0.3399, -0.2212]],\n",
            "\n",
            "        [[ 0.3047,  0.3983, -0.0730]],\n",
            "\n",
            "        [[ 0.3188, -0.4135, -0.8151]],\n",
            "\n",
            "        [[ 0.8521, -0.2961, -0.7123]],\n",
            "\n",
            "        [[ 0.8090,  0.9473, -0.8750]]], grad_fn=<StackBackward>) \n",
            "  shape: torch.Size([5, 1, 3]) \n",
            "\n",
            "* hn:\n",
            " tensor([[[ 0.8090,  0.9473, -0.8750]]], grad_fn=<StackBackward>) \n",
            "  shape: torch.Size([1, 1, 3])\n"
          ]
        }
      ],
      "source": [
        "# The whole the sequence in one call: unfolding the network \n",
        "outputs, hn = recNN(inputs, h0)\n",
        "print(\"* outputs:\\n\",outputs, \"\\n  shape:\",outputs.shape,\"\\n\")\n",
        "print(\"* hn:\\n\",hn, \"\\n  shape:\",hn.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-XUTWupaHlu"
      },
      "source": [
        "in this case, the forward function returns: \n",
        "- the sequence of the hidden layers associated to each input vector;\n",
        "- and the last hidden layer. \n",
        "The previous code is equivalent to this one: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRJ-5naIaHlu",
        "outputId": "0ecdc752-73fb-4552-e073-d8023c5ad4bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "at time  0  out =  tensor([[[ 0.6385, -0.3399, -0.2212]]], grad_fn=<StackBackward>)\n",
            "at time  1  out =  tensor([[[ 0.3047,  0.3983, -0.0730]]], grad_fn=<StackBackward>)\n",
            "at time  2  out =  tensor([[[ 0.3188, -0.4135, -0.8151]]], grad_fn=<StackBackward>)\n",
            "at time  3  out =  tensor([[[ 0.8521, -0.2961, -0.7123]]], grad_fn=<StackBackward>)\n",
            "at time  4  out =  tensor([[[ 0.8090,  0.9473, -0.8750]]], grad_fn=<StackBackward>)\n"
          ]
        }
      ],
      "source": [
        "hn=h0 # init \n",
        "for t in range(len(inputs)): \n",
        "    out, hn = recNN(inputs[t].view(1,1,-1), hn)\n",
        "    print(\"at time \",t, \" out = \", out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL3qQgmfaHlv"
      },
      "source": [
        "## Usage of LSTM\n",
        "\n",
        "To illustrate the previous section, the following code replace a simple recurrent network by a LSTM. Look at the differences ! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xuMLWN8aHlw",
        "outputId": "8e8ac883-431a-4509-cfc0-ee279cc021e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "##################\n",
            "One step returns: \n",
            "  1/  output :  tensor([[[-0.4350,  0.0462, -0.3104]]], grad_fn=<StackBackward>) torch.Size([1, 1, 3])\n",
            "  2/  hidden :  tensor([[[-0.4350,  0.0462, -0.3104]]], grad_fn=<StackBackward>) torch.Size([1, 1, 3])\n",
            "  3/  cell   :  tensor([[[-0.9659,  0.0889, -0.6284]]], grad_fn=<StackBackward>) torch.Size([1, 1, 3])\n",
            "##################\n"
          ]
        }
      ],
      "source": [
        "recNN = nn.LSTM(input_size=4, hidden_size=3)  # Input dim is 4, hidden layer size  is 3\n",
        "h0 =  torch.randn(1, 1, 3) # \n",
        "c0 =  torch.randn(1, 1, 3) # \n",
        "# One step \n",
        "\n",
        "# One step \n",
        "out, (hn,cn) = recNN(inputs[0].view(1,1,-1), (h0,c0))\n",
        "print(\"##################\")\n",
        "print(\"One step returns: \")\n",
        "print(\"  1/  output : \", out, out.shape)\n",
        "print(\"  2/  hidden : \", hn, hn.shape)\n",
        "print(\"  3/  cell   : \", cn, cn.shape)\n",
        "print(\"##################\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuLHVzeqaHlx"
      },
      "source": [
        "It is important to understand these examples and more specifically :\n",
        "* the parameters \"input dimension\" and \"output dimension\" set to 3 ? \n",
        "* why we initialize the hidden layer ?\n",
        "* the *-1* when we call *view* ? \n",
        "* ... \n",
        "If we unfold the LSTM along the sequence of inputs: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9Nt7oLraHlx",
        "outputId": "f7b89e4a-fb46-4080-e273-7e31e71c6825"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "##################\n",
            "Unfolding the net: \n",
            "  1/ out:\n",
            "  tensor([[[-0.4350,  0.0462, -0.3104]],\n",
            "\n",
            "        [[-0.0676,  0.0663, -0.2225]],\n",
            "\n",
            "        [[-0.0674, -0.1932,  0.0767]],\n",
            "\n",
            "        [[-0.1022, -0.3144,  0.0342]],\n",
            "\n",
            "        [[-0.0244, -0.1595, -0.0578]]], grad_fn=<StackBackward>) \n",
            " torch.Size([5, 1, 3])\n",
            "  2/ hn :\n",
            " tensor([[[-0.0244, -0.1595, -0.0578]]], grad_fn=<StackBackward>) \n",
            " torch.Size([1, 1, 3])\n",
            "  3/ cn :\n",
            " tensor([[[-0.0828, -0.3908, -0.1168]]], grad_fn=<StackBackward>) \n",
            " torch.Size([1, 1, 3])\n",
            "##################\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "out, (hn, cn) = recNN(inputs, (h0,c0))\n",
        "print(\"##################\")\n",
        "print(\"Unfolding the net: \")\n",
        "print(\"  1/ out:\\n \", out, \"\\n\",out.shape)\n",
        "print(\"  2/ hn :\\n\", hn, \"\\n\",hn.shape )\n",
        "print(\"  3/ cn :\\n\", cn,\"\\n\", cn.shape )\n",
        "print(\"##################\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyaQTowaaHly"
      },
      "source": [
        "# Sequence tagging  \n",
        "\n",
        "\n",
        "The task of *sequence tagging* consists in the attribution of a tag (or a class) to each element  (or words ) of a sequence (a sentence): \n",
        "* An observation is a sentence represented as a word sequence;\n",
        "* A tag sequence is associated to this sentence, one tag per word. \n",
        "\n",
        "If the input is sequence of symbols : \n",
        "$w_1, \\dots, w_M$, with $w_i \\in V$, the vocabulary or the finite set of the known words. Assume we have a tagset $T$ le *tagset* which is the set of all possible tags (the output space). At time $i$,  $y_i$ is the tag associated to the word  $w_i$.\n",
        "The prediction of the model is  $\\hat{y}_i$. \n",
        "Our goal is to predict the sequence $\\hat{y}_1, \\dots, \\hat{y}_M$, with $\\hat{y}_i \\in T$.\n",
        "\n",
        "## A recurrent tagger\n",
        "We can use a recurrent model to create a sequence tagger. The recurrent network \"reads\" the sentence and predict the tag sequence. We denote the hidden state of the recurrent network at time $i$ as  $h_i$. The prediction rule is to select   $\\hat{y}_i$ as : \n",
        "\n",
        "\\begin{align}\\hat{y}_i = \\text{argmax}_j \\  (\\log \\text{Softmax}(Ah_i + b))_j\\end{align}\n",
        "\n",
        "The softmax function gives us a probability distribution over the tagset ($\\in T$). The softmax is applied to a linear transformation of the hidden state $h_i$. In the following we can use the logsoftmax associated to the adapted loss. \n",
        "\n",
        "## A first (toy) dataset\n",
        "\n",
        "Let us build our first dataset and define some useful function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXym_OtkaHly",
        "outputId": "07aa7126-1df5-4c54-acdc-3b6130f775eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words dict:  {'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n",
            "Tags  dict:  {'DET': 0, 'NN': 1, 'V': 2}\n",
            "The sentence :  ['The', 'dog', 'ate', 'the', 'apple']\n",
            "The tag seq. :  ['DET', 'NN', 'V', 'DET', 'NN']\n",
            "#### in the prepared version\n",
            "The sentence :  tensor([0, 1, 2, 3, 4])\n",
            "The tag seq. :  tensor([0, 1, 2, 0, 1])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Convert the input sequence into an integer one.\n",
        "# The mapping is recorded in the dictionnary to_ix\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    tensor = torch.LongTensor(idxs)\n",
        "    return tensor\n",
        "\n",
        "# Toy dataset\n",
        "training_data = [\n",
        "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
        "]\n",
        "\n",
        "# The dictionnary : word -> index\n",
        "word_to_ix = {}\n",
        "# The other : tag -> index\n",
        "tag_to_ix = {}\n",
        "# Build them \n",
        "for sent, tags in training_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "    for tag in tags:\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "##\n",
        "print(\"Words dict: \", word_to_ix)\n",
        "print(\"Tags  dict: \",tag_to_ix)\n",
        "\n",
        "print(\"The sentence : \", training_data[0][0])\n",
        "print(\"The tag seq. : \", training_data[0][1])\n",
        "print(\"#### in the prepared version\")\n",
        "print(\"The sentence : \", prepare_sequence(training_data[0][0],word_to_ix))\n",
        "print(\"The tag seq. : \", prepare_sequence(training_data[0][1],tag_to_ix))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rf9Q5jGaHlz"
      },
      "source": [
        "## Build our first model\n",
        "Fill the following class. We can use a LSTM our tagger, with 3 components: \n",
        "- a LSTM us unfolded on the word sequence to be processed\n",
        "- an Embedding layer to project words\n",
        "- A linear layer to feed the log-softmax for prediction purpose. \n",
        "\n",
        "These three modules must be created in the constructor of the class. The forward function requires your full attention: \n",
        "- the model takes an input sequence: a tensor of word idx\n",
        "- the embedding layers will generate a new tensor, what is the dimensions ? \n",
        "- what is expected by the LSTM module ? \n",
        "- what is the dimensions of the LSTM ? \n",
        "- what is expected by the final Linear module ? \n",
        "\n",
        "Try to write it : \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nvPHTrQYaHl0"
      },
      "outputs": [],
      "source": [
        "\n",
        "class RecurrentTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(RecurrentTagger, self).__init__()\n",
        "        # TODO: \n",
        "\n",
        "    def init_hidden(self):\n",
        "        # This function is given: understand it. \n",
        "        return (torch.zeros(1, 1, self.hidden_dim),\n",
        "                torch.zeros(1, 1, self.hidden_dim))\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        # Your work\n",
        "        return None # of course it is not None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivWNVkr-aHl0"
      },
      "source": [
        "# Training \n",
        "Now write the code to train this model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZZ4QkgzCaHl0"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6\n",
        "\n",
        "\n",
        "model = RecurrentTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Look at the scores \n",
        "# The output element i,j concerns the  tag j pour le mot i.\n",
        "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "tag_scores = model(inputs)\n",
        "print(tag_scores)\n",
        "\n",
        "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    for sentence, tags in training_data:\n",
        "        # Get our inputs ready for the network\n",
        " \n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "\n",
        "        # Step2: Also, we need to clear out the hidden state of the recurrent net,\n",
        "        # detaching it from its history on the last instance.\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        #  calling optimizer.step()\n",
        "        \n",
        "# Les mêmes scores à la fin\n",
        "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "tag_scores = model(inputs)\n",
        "print(tag_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CDF_s2ZaHl1"
      },
      "source": [
        "# A real task\n",
        "Load the following dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tTPb-TRQaHl2"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "mydata = pickle.load( open( \"/Le/chemin/vers/brown.save.p\", \"rb\" ) )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2_b2q_3aHl2"
      },
      "source": [
        "Look at the data, process it as required and then: \n",
        "* Spit the dataset in 3 sets:  train / validation / test (80%,10%,10%)\n",
        "* Learn the model and test it \n",
        "* Tune the hyperparameters. What is the best score you can obtain ? \n",
        "* Start again with a bi-lstm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bEAuttnnaHl2"
      },
      "outputs": [],
      "source": [
        "0 0 x1 x2 x3 0 0 / ks = 5\n",
        "-> z1 , z2 ,z3 "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}