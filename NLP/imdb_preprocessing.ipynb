{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neohack22/IASD/blob/NLP/NLP/imdb_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGgznjDGc19L"
      },
      "source": [
        "The goal of this lab session is to implement the model proposed by  Yoon Kim, published in 2014. The original paper can be found [here](https://www.aclweb.org/anthology/D14-1181).\n",
        "Of course, there exists pytorch and tensorflow implementations on the web. They are more or less correct and efficient. However, here it is important to do it yourself. The goal is to better understand pytorch and the convolution. \n",
        "\n",
        "The road-map is to: \n",
        "- Implement the convolution and pooling \n",
        "- Add dropout on the last layer\n",
        "\n",
        "To start, it is useful to discover the convolution layers. In this lab, we consider the convolution operation in 1-dimension, followed by the adapted max pooling. \n",
        "\n",
        "\n",
        "We use the same dataset as before: imdb. The first following cells are the same as the previous lab session on this dataset (load the data, build the vocabulary, and prepare data for the model). \n",
        "\n",
        "\n",
        "# Data loading \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhmmsZLuc19P"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import torch.autograd as ag\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "th.manual_seed(1) # set the seed \n",
        "\n",
        "\n",
        "def clean_str(string, tolower=True):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning.\n",
        "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" \", string) ## remove \n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \", string) ## remove \n",
        "    string = re.sub(r\"\\)\", \" \", string)## remove \n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    if tolower:\n",
        "        string = string.lower()\n",
        "    return string.strip()\n",
        "\n",
        "\n",
        "def loadTexts(filename, limit=-1):\n",
        "    \"\"\"\n",
        "    Texts loader for imdb.\n",
        "    If limit is set to -1, the whole dataset is loaded, otherwise limit is the number of lines\n",
        "    \"\"\"\n",
        "    f = open(filename)\n",
        "    dataset=[]\n",
        "    line =  f.readline()\n",
        "    cpt=1\n",
        "    skip=0\n",
        "    while line :\n",
        "        cleanline = clean_str(line).split()\n",
        "        if cleanline: \n",
        "            dataset.append(cleanline)\n",
        "        else: \n",
        "            line = f.readline()\n",
        "            skip+=1\n",
        "            continue\n",
        "        if limit > 0 and cpt >= limit: \n",
        "            break\n",
        "        line = f.readline()\n",
        "        cpt+=1        \n",
        "        \n",
        "    f.close()\n",
        "    print(\"Load \", cpt, \" lines from \", filename , \" / \", skip ,\" lines discarded\")\n",
        "    return dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxGJXdCtc19R"
      },
      "source": [
        "Load the data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfRYf5Qbc19S",
        "outputId": "38f28c2b-448b-4f44-ffa7-70efa3dc20f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load  299966  lines from  /home/allauzen/cours/nlp-iasd/labs/imdb.pos  /  35  lines discarded\n",
            "[['excellent'], ['do', \"n't\", 'miss', 'it', 'if', 'you', 'can'], ['a', 'great', 'parody'], ['dreams', 'of', 'a', 'young', 'girl'], ['tromendous', 'piece', 'of', 'art'], ['funny', 'funny', 'movie', '!'], ['need', 'more', 'scifi', 'like', 'this'], ['pride', 'and', 'prejudice', 'is', 'absolutely', 'amazing', '!', '!'], ['scott', 'pilgrim', 'vs', 'the', 'world'], ['quirky', 'and', 'effective']]\n",
            "299965  pos sentences\n",
            "Load  299949  lines from  /home/allauzen/cours/nlp-iasd/labs/imdb.neg  /  52  lines discarded\n",
            "[['typical', 'movie', 'where', 'best', 'parts', 'are', 'in', 'the', 'preview'], ['not', 'for', 'the', 'squeamish'], ['cool', 'when', 'i', 'was', 'kid'], ['i', 'appreciate', 'the', 'effort', 'but'], ['pretty', 'bad'], ['much', 'ado', 'about', 'nothing'], ['series', 'of', 'unlikely', 'events'], ['april', 'is', 'the', 'cruelest', 'month'], ['great', 'idea', 'but'], ['and', 'people', 'thought', 'this', 'was', 'good', '\\\\?']]\n",
            "299948  neg sentences\n"
          ]
        }
      ],
      "source": [
        "LIM=-1\n",
        "pathd = \"/home/allauzen/cours/nlp-iasd/labs/\"\n",
        "txtfile=pathd+\"imdb.pos\"\n",
        "postxt = loadTexts(txtfile,limit=LIM)\n",
        "print(postxt[0:10])\n",
        "print (len(postxt), \" pos sentences\")\n",
        "\n",
        "txtfile=pathd+\"imdb.neg\"\n",
        "negtxt = loadTexts(txtfile,limit=LIM)\n",
        "print(negtxt[0:10])\n",
        "\n",
        "print (len(negtxt), \" neg sentences\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoZvtYM8c19T"
      },
      "outputs": [],
      "source": [
        "wfreq = {}\n",
        "maxlength = 0 \n",
        "for sent in postxt+negtxt: \n",
        "    isent = []\n",
        "    maxlength = max(maxlength,len(sent))\n",
        "    for w in sent: \n",
        "        if w in wfreq:\n",
        "            wfreq[w] = wfreq[w]+1\n",
        "        else :\n",
        "            wfreq[w]=1\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtKDn5Ukc19T",
        "outputId": "60a0d54b-658c-411d-ca0f-d1ce4b817734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63699\n"
          ]
        }
      ],
      "source": [
        "print(len(wfreq))\n",
        "orderedvocab = []\n",
        "for w in sorted(wfreq, key=wfreq.get, reverse=True):\n",
        "    orderedvocab.append((w, wfreq[w]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDwnOPXKc19U",
        "outputId": "561d61e1-1fc2-4f21-cbcd-15851d5fb5f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('!', 153714), ('the', 146409), ('a', 131821), ('of', 94543), ('movie', 80115), ('and', 63910), ('this', 53299), ('to', 46991), ('it', 46431), ('i', 44902)]\n"
          ]
        }
      ],
      "source": [
        "print(orderedvocab[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gU3HhMi-c19U",
        "outputId": "e1a6d1c6-1db9-480c-830c-7a55c3dce434"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10002  ==  10002\n",
            "1 <unk> 1\n",
            "2 ! 2\n",
            "3 the 3\n",
            "4 a 4\n",
            "5 of 5\n"
          ]
        }
      ],
      "source": [
        "VOCSIZE = 10000\n",
        "w2idx = {}\n",
        "idx2w = {}\n",
        "w2idx[\"<pad>\"]  = 0\n",
        "w2idx[\"<unk>\"] = 1\n",
        "idx2w[1]=\"<unk>\"\n",
        "idx2w[0]=\"<pad>\"\n",
        "\n",
        "for i in range(VOCSIZE): \n",
        "    w, _ = orderedvocab[i]\n",
        "    w2idx[w] = i+2\n",
        "    idx2w[i+2] = w\n",
        "    \n",
        "print(len(w2idx), \" == \",len(idx2w))\n",
        "for i in range(1,6):\n",
        "    print(i, idx2w[i], w2idx[idx2w[i]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5p9n43DWc19V",
        "outputId": "36d28dc5-47de-4691-a17d-b9d934159b16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10002  words in the vocab\n",
            "200000  sentences\n",
            "48  is maximum sentence length\n",
            "tensor([ 36,  25, 381,  10,  58,  21,  83])\n"
          ]
        }
      ],
      "source": [
        "NB_SENTENCES = 100000 # for each class\n",
        "txtidx = []\n",
        "maxlength = 0 \n",
        "for sent in postxt[1:NB_SENTENCES+1]+negtxt[:NB_SENTENCES]:\n",
        "    maxlength = max(maxlength,len(sent))\n",
        "    isent=[]\n",
        "    for w in sent: \n",
        "        widx=1\n",
        "        if w in w2idx:\n",
        "            widx=w2idx[w]\n",
        "        isent.append(widx)\n",
        "    txtidx.append(th.LongTensor((isent)))\n",
        "    \n",
        "print(len(w2idx), \" words in the vocab\")\n",
        "print(len(txtidx), \" sentences\")\n",
        "print(maxlength, \" is maximum sentence length\")\n",
        "print(txtidx[0])\n",
        "\n",
        "\n",
        "### For the labels\n",
        "labels = th.ones([2*NB_SENTENCES])\n",
        "labels[0:NB_SENTENCES] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl15Y5TLc19V",
        "outputId": "73e2f2b6-a121-4ab2-abfc-a0096ab06f2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 36,  25, 381,  10,  58,  21,  83]) torch.Size([7])\n",
            "['do', \"n't\", 'miss', 'it', 'if', 'you', 'can']\n",
            "['do', \"n't\", 'miss', 'it', 'if', 'you', 'can']\n"
          ]
        }
      ],
      "source": [
        "def idx2wordlist(idx_array): \n",
        "    l = []\n",
        "    for i in idx_array: \n",
        "        l.append(idx2w[i.item()])\n",
        "    return l\n",
        "print(txtidx[0], txtidx[0].shape)\n",
        "\n",
        "print(idx2wordlist(txtidx[0]))\n",
        "print(postxt[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOTdRRANc19W"
      },
      "outputs": [],
      "source": [
        "pack = (txtidx, labels, idx2w)\n",
        "import pickle\n",
        "\n",
        "if True : \n",
        "     pickle.dump(pack, open('imdb-200k', 'wb'))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}