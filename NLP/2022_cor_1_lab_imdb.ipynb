{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neohack22/IASD/blob/NLP/NLP/2022_cor_1_lab_imdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl9nmloVpySE"
      },
      "source": [
        "The goal is to set up a simple classifier for text and sentiment analysis. The task is the binary classification of movie reviews. The dataset is a part of the *imdb* dataset. You can find the original dataset on the [imdb website](https://www.imdb.com/interfaces/) or a version on the [kaggle website](https://www.kaggle.com/utathya/imdb-review-dataset). For this lab session, we will use a preprocessed version. \n",
        "\n",
        "\n",
        "The roadmap is:\n",
        "- Load, clean and setup the data (in practice this a very important step, for this lab we skip it). \n",
        "- Make it suitable for pytorch models\n",
        "- Define your own model\n",
        "- Experiments\n",
        "\n",
        "\n",
        "# The data \n",
        "\n",
        "Datasets are available in the cloud repository. There are 2 files, one for positive reviews (imdb.pos) and one for the negative ones (imdb.neg). There are  300000 examples of each class. \n",
        "\n",
        "Here two functions to load and clean the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aLVRPoUpySK",
        "outputId": "c5a27f1c-3e71-496e-b322-32c359f89ce9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f57a2a39350>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import torch.autograd as ag\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import math\n",
        "import pickle\n",
        "import gzip\n",
        "\n",
        "th.manual_seed(1) # set the seed "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ucjfrVbpySN"
      },
      "source": [
        "# Data loading \n",
        "\n",
        "\n",
        "Load the data : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K59vHLBpySN",
        "outputId": "745ce76b-1536-4f21-fc3a-b3db55efd4d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-04-06 11:30:46--  https://drive.google.com/uc?export=download&id=1199vdCPh5jCMBTWTSM9th0wqMNv2KSvA\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.18.206, 2a00:1450:4007:812::200e\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.18.206|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0k-0o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/n2hk1hevtcvg0o0qrmu5u6kngpg8htco/1649237400000/16692574002775380562/*/1199vdCPh5jCMBTWTSM9th0wqMNv2KSvA?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-04-06 11:30:50--  https://doc-0k-0o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/n2hk1hevtcvg0o0qrmu5u6kngpg8htco/1649237400000/16692574002775380562/*/1199vdCPh5jCMBTWTSM9th0wqMNv2KSvA?e=download\n",
            "Resolving doc-0k-0o-docs.googleusercontent.com (doc-0k-0o-docs.googleusercontent.com)... 216.58.213.129, 2a00:1450:4007:811::2001\n",
            "Connecting to doc-0k-0o-docs.googleusercontent.com (doc-0k-0o-docs.googleusercontent.com)|216.58.213.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1552309 (1,5M) [application/x-gzip]\n",
            "Saving to: ‘imdb.pck.gz’\n",
            "\n",
            "imdb.pck.gz         100%[===================>]   1,48M  5,08MB/s    in 0,3s    \n",
            "\n",
            "2022-04-06 11:30:51 (5,08 MB/s) - ‘imdb.pck.gz’ saved [1552309/1552309]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# find the file imdb.pck.gz, and set the next variable accordingly\n",
        "filename = 'imdb.pck.gz'\n",
        "\n",
        "# You can download the file with the following line: \n",
        "! wget \"https://drive.google.com/uc?export=download&id=1199vdCPh5jCMBTWTSM9th0wqMNv2KSvA\" -O imdb.pck.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "hVFv-0u_pySO"
      },
      "source": [
        "Open the data with python and you will get 3 objects : \n",
        "- *texts*  : a list of tensors, each tensor represent a word sequence to classify. \n",
        "- *labels* : the class, positive or negative, of the corresponding text\n",
        "- *lexicon*: a dictionnary to map integers to real words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlMFMQ08pySO",
        "outputId": "d52cb691-861d-4f36-c49f-269a8c45afed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'> <class 'torch.Tensor'> <class 'dict'>\n",
            "tensor([ 36,  25, 381,  10,  58,  21,  83])\n",
            "nb examples :  30000\n",
            "Vocab size:  5002\n"
          ]
        }
      ],
      "source": [
        "fp = gzip.open(filename,'rb')\n",
        "texts , labels, lexicon  = pickle.load(fp) \n",
        "\n",
        "print(type(texts), type(labels), type(lexicon))\n",
        "print(texts[0])\n",
        "print(\"nb examples : \", len(texts))\n",
        "VOCAB_SIZE = len(lexicon)\n",
        "print(\"Vocab size: \", VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBHrK2okpySP"
      },
      "source": [
        "Note that a reduced number of words are selected to build the vocabulary. The less frequent words are discarded are replaced by a specific form (*unk* for unknown).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SlDfDuVpySQ",
        "outputId": "92b5a3ea-7909-44db-d2ec-4e8b3913cc98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word of index 0  :  <pad>\n",
            "word of index 1  :  <unk>\n",
            "word of index 2  :  !\n",
            "word of index 3  :  the\n",
            "word of index 4  :  a\n",
            "word of index 5  :  of\n",
            "word of index 6  :  movie\n",
            "word of index 7  :  and\n",
            "word of index 8  :  this\n",
            "word of index 9  :  to\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    print(\"word of index\", i , \" : \", lexicon[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08EvO2IfpySR"
      },
      "source": [
        "To read the text you can use for example the following code: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUNU4tIdpySR",
        "outputId": "26aaa4f1-ed60-496f-f151-baedc638f5b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([7])\n",
            "Some positive reviews\n",
            "------------\n",
            "['strong', 'drama']\n",
            "['please', 'remake', 'this', 'movie']\n",
            "['very', 'funny', '!']\n",
            "['great', 'series']\n",
            "['fun', 'movie']\n",
            "Some negative reviews\n",
            "------------\n",
            "['absolute', 'waste', 'of', 'time']\n",
            "['the', 'worst', 'movie', 'ever', 'made']\n",
            "['slow', 'motion', 'picture', 'that', 'did', \"n't\", 'get', 'to', 'the', 'point']\n",
            "['there', 'are', 'good', 'bad', 'movies', 'and', 'there', 'are', 'bad', 'bad', 'movies', 'this', 'one', 'is', 'a', 'real', 'stinker']\n",
            "['<unk>', 'so', 'bad', 'its', 'funny']\n",
            "-----------\n",
            "A random sentence: \n",
            "['you', 'definitely', 'need', 'to', 'see', 'this', 'movie']\n"
          ]
        }
      ],
      "source": [
        "def idx2wordlist(idx_array,lexicon): \n",
        "    l = []\n",
        "    for i in idx_array: \n",
        "        l.append(lexicon[i.item()])\n",
        "    return l\n",
        "print(texts[0].shape)\n",
        "print(\"Some positive reviews\")\n",
        "print(\"------------\")\n",
        "for i in range(5): \n",
        "    print(idx2wordlist(texts[i+50],lexicon))\n",
        "print(\"Some negative reviews\")\n",
        "print(\"------------\")\n",
        "for i in range(5): \n",
        "    print(idx2wordlist(texts[-i-2000],lexicon))\n",
        "    \n",
        "print(\"-----------\\nA random sentence: \")\n",
        "print(idx2wordlist(texts[104],lexicon))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtlcwqfQpySS"
      },
      "source": [
        "# Interface data/model\n",
        "\n",
        "In pratice, we start from raw texts and we need to convert them into word indices. At this step, we can perform text pre-processing, tokenization and data cleaning. In the present case, it is already done. But in real life it is a very important step. \n",
        "\n",
        "\n",
        "The goal is to implement a CBOW (Continuous Bag of Words, or a bag of word embeddings) classifier.  This means that the first layer of the model deals with word embeddings. \n",
        "\n",
        "The **Embedding** module in pytorch is designed for that purpose. This module expects as input an array or a list of word indices. For this session, the goal is to quickly develop a model. The data interface is therefore rather simple. \n",
        "We end this section by creating a labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBwbWCEwpyST"
      },
      "source": [
        "# A first model\n",
        "The first model is a CBOW (Continuous Bag of Words). A text is represented as set of words (a bag of binary features):   \n",
        "- Each word is associated to its embedding. \n",
        "- The text is  represented as the sum of the word embeddings involved. \n",
        "- This sum of embeddings is then feed to linear layer with one output unit, \n",
        "- followed by the sigmoid activation. The model output is similar to a logistic regression. \n",
        "\n",
        "Now we want to code this in pytorch. One way is to first try to build such model (or a toy but similar example) **step by step**, then to create a nice class to wrap everything in a **model**. \n",
        "\n",
        "## Building the model, step by step\n",
        "\n",
        "The input layer of the model is an Embedding layer. This is already implemented in pytorch. Look at the following toy example: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q25KzwFlpySU",
        "outputId": "cd77d9d1-f5d7-45e7-d60c-980e4d6ce137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The input:  tensor([ 21, 316, 320,   9,  59,   8,   6])\n",
            "length:  7\n",
            "Embs shape :  torch.Size([7, 4])\n",
            "tensor([[ 0.6971, -0.9576, -1.0220,  1.3295],\n",
            "        [ 1.0256,  1.7889, -1.2001,  0.8268],\n",
            "        [-1.1081,  0.4350, -0.5725, -1.6943],\n",
            "        [-0.9530, -1.2833, -0.6837,  1.3832],\n",
            "        [ 0.2081, -0.4403,  1.3717,  0.9725],\n",
            "        [-0.5415, -1.4216, -0.0367, -1.9919],\n",
            "        [-1.3417,  0.0124, -1.3485, -0.5739]], grad_fn=<EmbeddingBackward>)\n"
          ]
        }
      ],
      "source": [
        "# build an Embedding layer\n",
        "# it is important to understand the parameters given to the constructor ! \n",
        "D = 4\n",
        "embLayer = th.nn.Embedding(num_embeddings=len(lexicon), embedding_dim=D)\n",
        "# The dim of 4 is a toy example. \n",
        "# run forward on some input\n",
        "inp = texts[104]\n",
        "embs = embLayer(inp) # embLayer.forward(inp)\n",
        "# Look at the dimension of i/o\n",
        "print(\"The input: \",inp)\n",
        "print(\"length: \",len(inp))\n",
        "print(\"Embs shape : \",embs.shape)\n",
        "print(embs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAo2OntepySV"
      },
      "source": [
        "Now, we want to compress the resulting tensor along the time dimension. This dimension depends on the input texts, while we want to build a fixed-size representation of the sentence. The sum is a first idea. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9aqQ9aJpySV",
        "outputId": "26197125-7c58-40e5-9456-ba7270543fcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([7])\n",
            "torch.Size([4])\n"
          ]
        }
      ],
      "source": [
        "## compute the sum of out to create a vector of size \"embedding_dim\".\n",
        "## Of course it will be a tensor with one dimension set to \"embedding_dim\".\n",
        "sumOfEmbs = embs.sum(dim=1)\n",
        "print(sumOfEmbs.shape) # check the shape \n",
        "sumOfEmbs = embs.sum(dim=0)\n",
        "print(sumOfEmbs.shape) # check the shape "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fray2Zj_pySW"
      },
      "source": [
        "The final layer is a linear transformation: as input we have a vector of size *embedding_dim* and 1 in output. \n",
        "Code this transformation and check the shape of the final result. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA4Ekg8ypySW",
        "outputId": "8ac98391-4be8-466d-98ef-8190c74e61d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1]) tensor([0.6601], grad_fn=<SigmoidBackward>)\n",
            "torch.Size([1]) tensor([0.8424], grad_fn=<SigmoidBackward>)\n"
          ]
        }
      ],
      "source": [
        "# Compute out, after you created the Linear layer\n",
        "th.manual_seed(12) # set the seed \n",
        "W  = th.nn.Linear(in_features=D, out_features=1) \n",
        "out_activation = th.nn.Sigmoid()\n",
        "out= out_activation(W(sumOfEmbs))\n",
        "print(out.shape,out)\n",
        "\n",
        "W  = th.nn.Linear(in_features=D, out_features=1) \n",
        "out_activation = th.nn.Sigmoid()\n",
        "out= out_activation(W(sumOfEmbs))\n",
        "print(out.shape,out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIOT0Mm3pySW"
      },
      "source": [
        "##  Wrap everything in a nice module/model\n",
        "\n",
        "To implement the model, we propose to fill the following class. To write your own module, inherit from the *Module* class. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLWaKdIEpySX"
      },
      "outputs": [],
      "source": [
        "class CBOW_classifier(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW_classifier, self).__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, embedding_dim) \n",
        "        self.lin = nn.Linear(embedding_dim, 1)\n",
        "        \n",
        "    def forward(self, inp):\n",
        "        return th.sigmoid(self.lin(self.emb(inp).sum(dim=0)))\n",
        "        \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "cs5wXpWwpySX"
      },
      "source": [
        "This class inherits of *Module*. These two methods are mandatory. The constructor build a model with its initialized parameters. The *forward* is for inference.  Reminder: in pytorch, a model takes *Tensors (variables) * and returns *Tensors (variables)*. The output is compared with the gold standard by the loss function. \n",
        "\n",
        "Fill this class and make a simple test: take a training example and see if the forward pass is correct. The result should be a *FloatTensor* with one value: the score between 0 and 1 assigned by the model to the example. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIlrnlyMpySX",
        "outputId": "9fd6769d-002a-4cad-875e-ffef4b306831"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.6236], grad_fn=<SigmoidBackward>)\n",
            "tensor(0.)\n"
          ]
        }
      ],
      "source": [
        "classifier = CBOW_classifier(vocab_size=len(lexicon),embedding_dim=10)\n",
        "print(classifier(texts[0]))\n",
        "print(labels[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFWSldy6pySY",
        "outputId": "bb7b2097-d081-4f63-b030-ffa7884b8395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.6236], grad_fn=<SigmoidBackward>)\n"
          ]
        }
      ],
      "source": [
        "print(classifier.forward(texts[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEqE-4ippySY"
      },
      "source": [
        "## Objective function\n",
        "The loss (or objective) function is tailored to the model and the task.  \n",
        "\n",
        "- Read the doc of module **nn** : http://pytorch.org/docs/master/nn.html. \n",
        "- In our case, two loss functions can be used:  *BCELoss* and *BCEWithLogitsLoss*. Compare them and make your choice. \n",
        "- Given this choice, you may want to modify the class *CBOW_Classifier*. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOg8oBhTpySY"
      },
      "outputs": [],
      "source": [
        "## TODO : define de training function\n",
        "loss_fn = nn.BCELoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "pDdDsFBmpySZ"
      },
      "source": [
        "## Training \n",
        "\n",
        "Write the code to train your model and monitor the training process and to evaluate the model using test data. Starts with a SGD optimizer with 0.1 as learning rate. \n",
        "\n",
        "### Random order\n",
        "In many cases, in can be important to iter on the data in a random order and not in the order we built the corpus. This initial order can introduce bias in the evaluation process.  A simple method to shuffle the data is to shuffle the indices we use. Assume we have 10 training samples, we can do something like \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yPZ4J8ypySZ",
        "outputId": "207db18a-a6c1-461d-d9f3-508e0e18256d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n",
            "0\n",
            "2\n",
            "3\n",
            "8\n",
            "7\n",
            "9\n",
            "6\n",
            "1\n",
            "5\n"
          ]
        }
      ],
      "source": [
        "ids = list(range(10))\n",
        "import random \n",
        "random.shuffle(ids)\n",
        "for i in ids: \n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e1N9HuIpySZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkLbvcefpySZ"
      },
      "source": [
        "##### Now we have everything to run the training loop and test this model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FN8eCwkpySa",
        "outputId": "a7be379c-0a06-408e-c669-c3cb0efd7906"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 tensor(0.7279) 54.54333333333334 tensor(15049.)\n",
            "1 tensor(0.6946) 58.906666666666666 tensor(15068.)\n",
            "2 tensor(0.6643) 62.86333333333334 tensor(14997.)\n",
            "3 tensor(0.6356) 65.49333333333334 tensor(15064.)\n",
            "4 tensor(0.6110) 67.36 tensor(14950.)\n",
            "5 tensor(0.5916) 68.86666666666666 tensor(14988.)\n",
            "6 tensor(0.5753) 69.91333333333333 tensor(14934.)\n",
            "7 tensor(0.5615) 71.37 tensor(14945.)\n",
            "8 tensor(0.5477) 72.33666666666667 tensor(14995.)\n",
            "9 tensor(0.5371) 72.68 tensor(14916.)\n"
          ]
        }
      ],
      "source": [
        "total = len(texts)\n",
        "randomidx = list(range(total))\n",
        "preds = th.zeros(total)\n",
        "optimizer = th.optim.SGD(classifier.parameters(),lr=1e-2)\n",
        "Nepochs = 10\n",
        "losses = th.zeros(Nepochs)\n",
        "for epoch in range(Nepochs):\n",
        "    total_loss = th.Tensor([0])\n",
        "    correct=0 \n",
        "    random.shuffle(randomidx)\n",
        "    for i in randomidx:\n",
        "        classifier.zero_grad()\n",
        "        x = texts[i]\n",
        "        probs = classifier(x)[0]    \n",
        "        loss = loss_fn(probs, labels[i])\n",
        "        pred= probs>0.5\n",
        "        preds[i] = pred\n",
        "        if pred.item() == labels[i].item() : \n",
        "            correct +=1\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.data\n",
        "    losses[epoch] = total_loss/total\n",
        "    print(epoch, losses[epoch], 100.0*correct/total, preds.sum())\n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xvduf49XpySa",
        "outputId": "1435f946-064f-47ef-c8f8-f1fc951d87ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f57963942d0>]"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lejz2by5pySa"
      },
      "source": [
        "# First Experiments\n",
        "\n",
        "- Make experiments  with  2000 to start then all the data for training (equally distributed between positive and negative examples). \n",
        "- You should create a development and test sets. \n",
        "- Test different parametrization of the model (here the embedding size) and the hyper-parameter (the learning rate) for each setups. \n",
        "- Compare these different setups (loss function on the train and also the classification accuracy). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdNAgkrLpySa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WkBH35wpySa"
      },
      "source": [
        "# A deeper model\n",
        "\n",
        "We can add a hidden layer to the previous classifier. \n",
        "- Do the same as before with the different setups\n",
        "- Find the good choice of hyper-parameters.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf5frvuhpySb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}