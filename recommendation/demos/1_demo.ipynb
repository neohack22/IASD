{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neohack22/IASD/blob/recommandation/recommendation/demos/1_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZE4FUsZTDbA"
      },
      "source": [
        "# Recommender Systems Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0Pn6ov0TDbB"
      },
      "source": [
        "## 0. Quick Start\n",
        "To run this notebook you just need to have [pipenv](https://github.com/pypa/pipenv) installed.\n",
        "Then run these 3 commands:\n",
        "- First install the dependencies with: `pipenv install`\n",
        "- Launch the virtual env: `pipenv shell`\n",
        "- Finally start jupyter and open the notebook: `jupyter-lab`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MODeQU0Tf6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfce1626-298f-440d-be56-3e57f4554074"
      },
      "source": [
        "%%bash\n",
        "pip install surprise"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting surprise\n",
            "  Downloading https://files.pythonhosted.org/packages/61/de/e5cba8682201fcf9c3719a6fdda95693468ed061945493dea2dd37c5618b/surprise-0.1-py2.py3-none-any.whl\n",
            "Collecting scikit-surprise\n",
            "  Downloading https://files.pythonhosted.org/packages/97/37/5d334adaf5ddd65da99fc65f6507e0e4599d092ba048f4302fe8775619e8/scikit-surprise-1.1.1.tar.gz (11.8MB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.15.0)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py): started\n",
            "  Building wheel for scikit-surprise (setup.py): finished with status 'done'\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1617646 sha256=3ad4d785ae4ef38801e71c3ebde5b9ae956542673331da78aec15b975883c972\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/9c/3d/41b419c9d2aff5b6e2b4c0fc8d25c538202834058f9ed110d0\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.1 surprise-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install surprise"
      ],
      "metadata": {
        "id": "MTy7PoMMiGct",
        "outputId": "8754582b-1c04-479d-8873-f4fa7d289bd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Collecting scikit-surprise\n",
            "  Downloading scikit-surprise-1.1.1.tar.gz (11.8 MB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.7.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.15.0)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py): started\n",
            "  Building wheel for scikit-surprise (setup.py): finished with status 'done'\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1633986 sha256=72e7fd852666b7bd4e934f8a14e21954c084becdb0a2e3309d592235b853eae8\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/44/74/b498c42be47b2406bd27994e16c5188e337c657025ab400c1c\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.1 surprise-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NooTDdZOTDbC"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel \n",
        "\n",
        "from surprise import NormalPredictor, SVD, KNNBasic, NMF\n",
        "from surprise import Dataset, Reader\n",
        "from surprise import accuracy\n",
        "from surprise.model_selection import cross_validate, KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "from surprise import NormalPredictor, SVD, KNNBasic, NMF\n",
        "from surprise import Dataset, Reader\n",
        "from surprise import accuracy\n",
        "from surprise.model_selection import cross_validate, KFold"
      ],
      "metadata": {
        "id": "11CF_AqkiL2-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjDT2A50TDbG"
      },
      "source": [
        "## 1. Introduction\n",
        "Recommender systems goal is to push *relevant* items to a given user. Understanding and modelling the user's preferences is required to reach this goal. In this project you will learn how to model the user's preferences with the [Surprise library](http://surpriselib.com/) to build different recommender systems. The first one will be a pure *collaborative filtering* approach, and the second one will rely on item attributes in a *content-based* way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36Z7GcLfTDbG"
      },
      "source": [
        "## 2. Loading Data\n",
        "We use here the [MovieLens dataset](https://grouplens.org/datasets/movielens/). It contains 25 millions of users ratings. the data are in the `./data/raw` folder. We could load directly the .csv file with [a built-in Surprise function](https://github.com/NicolasHug/Surprise/blob/ef3ed6e98304dbf8d033c8eee741294b05b5ba07/surprise/dataset.py#L105), but it's more convenient to load it through a Pandas dataframe for later flexibility purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmPTqb_FTDbH"
      },
      "source": [
        "RATINGS_DATA_FILE = './data/raw/ratings.csv'\n",
        "MOVIES_DATA_FILE = './data/raw/movies.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnB855KRTDbJ"
      },
      "source": [
        "# load the raw csv into a data_frame\n",
        "df_ratings = pd.read_csv(RATINGS_DATA_FILE)\n",
        "\n",
        "# drop the timestamp column since we dont need it now\n",
        "df_ratings = df_ratings.drop(columns=\"timestamp\")\n",
        "\n",
        "# movies dataframe\n",
        "df_movies = pd.read_csv(MOVIES_DATA_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVJsMrEuTDbM",
        "outputId": "918bab06-8b34-4378-ed40-b85a516a92de"
      },
      "source": [
        "# check we have 25M users' ratings\n",
        "df_ratings.userId.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000095"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwjp-MU-TDbQ"
      },
      "source": [
        "def get_subset(df, number):\n",
        "    \"\"\"\n",
        "        just get a subset of a large dataset for debug purpose\n",
        "    \"\"\"\n",
        "    rids = np.arange(df.shape[0])\n",
        "    np.random.shuffle(rids)\n",
        "    df_subset = df.iloc[rids[:number], :].copy()\n",
        "    return df_subset\n",
        "df_ratings_100k = get_subset(df_ratings, 100000)\n",
        "df_movies_100 = get_subset(df_movies, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAWC1dx9TDbS"
      },
      "source": [
        "# Surprise reader\n",
        "reader = Reader(rating_scale=(0, 5))\n",
        "\n",
        "# Finally load all ratings\n",
        "ratings = Dataset.load_from_df(df_ratings_100k, reader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FraqKt7hTDbU"
      },
      "source": [
        "## 3. Collaborative Filtering\n",
        "We can test first any of the [Surprise algorithms](https://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QQASBfcTDbV"
      },
      "source": [
        "# define a cross-validation iterator\n",
        "kf = KFold(n_splits=3)\n",
        "\n",
        "algos = [SVD(), NMF(), KNNBasic()]    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkUTsvHgTDbX",
        "outputId": "1b26bff2-a89a-423e-f5a8-6170e2c557eb"
      },
      "source": [
        "def get_rmse(algo, testset):\n",
        "        predictions = algo.test(testset)\n",
        "        accuracy.rmse(predictions, verbose=True)\n",
        "        \n",
        "for trainset, testset in tqdm(kf.split(ratings)): \n",
        "    \"\"\"\n",
        "        get an evaluation with cross-validation for different algorithms\n",
        "    \"\"\"  \n",
        "    for algo in algos:\n",
        "        algo.fit(trainset)\n",
        "        get_rmse(algo, testset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RMSE: 1.0408\n",
            "RMSE: 1.0949\n",
            "Computing the msd similarity matrix...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:01,  1.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done computing similarity matrix.\n",
            "RMSE: 1.0608\n",
            "RMSE: 1.0489\n",
            "RMSE: 1.1087\n",
            "Computing the msd similarity matrix...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2it [00:03,  1.56s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done computing similarity matrix.\n",
            "RMSE: 1.0745\n",
            "RMSE: 1.0303\n",
            "RMSE: 1.0855\n",
            "Computing the msd similarity matrix...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "3it [00:04,  1.54s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done computing similarity matrix.\n",
            "RMSE: 1.0559\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Dom6XI9TDbZ"
      },
      "source": [
        "## 4. Content-based Filtering\n",
        "Here we will rely directly on items attributes. First we have to describe a user profile with an attributes vector. Then we will use these vectors to generate recommendations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qinBp3urTDba",
        "outputId": "9ca29271-3a19-4884-ec98-45c9ef7e1a15"
      },
      "source": [
        "# computing similarities requires too much ressources on the whole dataset, so we take the subset with 100 items\n",
        "df_movies_100 = df_movies_100.reset_index(drop=True)\n",
        "df_movies_100.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movieId</th>\n",
              "      <th>title</th>\n",
              "      <th>genres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>162126</td>\n",
              "      <td>Autobiography of a Princess (1975)</td>\n",
              "      <td>(no genres listed)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>194666</td>\n",
              "      <td>Roads in February (2018)</td>\n",
              "      <td>Drama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>157679</td>\n",
              "      <td>Alley Cats Strike (2000)</td>\n",
              "      <td>Children|Comedy|Drama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>169196</td>\n",
              "      <td>Once Upon a Time Veronica (2012)</td>\n",
              "      <td>Drama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>191777</td>\n",
              "      <td>Revenge: A Love Story (2010)</td>\n",
              "      <td>Thriller</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   movieId                               title                 genres\n",
              "0   162126  Autobiography of a Princess (1975)     (no genres listed)\n",
              "1   194666            Roads in February (2018)                  Drama\n",
              "2   157679            Alley Cats Strike (2000)  Children|Comedy|Drama\n",
              "3   169196    Once Upon a Time Veronica (2012)                  Drama\n",
              "4   191777        Revenge: A Love Story (2010)               Thriller"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erSkCS9dTDbc"
      },
      "source": [
        "# we compute a TFIDF on the titles of the movies\n",
        "tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\n",
        "tfidf_matrix = tf.fit_transform(df_movies_100['title'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89bo1QRJTDbe"
      },
      "source": [
        "# we get cosine similarities: this takes a lot of time on the real dataset\n",
        "cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBfSq_MXTDbh"
      },
      "source": [
        "# we generate in 'results' the most similar movies for each movie: we put a pair (score, movie_id)\n",
        "results = {}\n",
        "for idx, row in df_movies_100.iterrows():\n",
        "    similar_indices = cosine_similarities[idx].argsort()[:-100:-1] \n",
        "    similar_items = [(cosine_similarities[idx][i], df_movies_100['movieId'].loc[[i]].tolist()[0]) for i in similar_indices] \n",
        "    results[idx] = similar_items[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL-uCJIXTDbj",
        "outputId": "27b63444-08b2-4631-cbbd-e691144c5ac1"
      },
      "source": [
        "len(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx1NXKXDTDbl"
      },
      "source": [
        "# transform a 'movieId' into its corresponding movie title\n",
        "def item(id):  \n",
        "    return df_movies_100.loc[df_movies_100['movieId'] == id]['title'].tolist()[0].split(' - ')[0] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9-fOls1TDbn"
      },
      "source": [
        "# transform a 'movieId' into the index id\n",
        "def get_idx(id):\n",
        "    return df_movies_100[df_movies_100['movieId'] == id].index.tolist()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_WRVRXUTDbp"
      },
      "source": [
        "# Finally we put everything together here:\n",
        "def recommend(item_id, num):\n",
        "    print(\"Recommending \" + str(num) + \" products similar to \" + item(item_id) + \"...\")   \n",
        "    print(\"-------\")    \n",
        "    recs = results[get_idx(item_id)][:num]   \n",
        "    for rec in recs: \n",
        "        print(\"\\tRecommended: \" + item(rec[1]) + \" (score:\" +      str(rec[0]) + \")\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruDiCPsDTDbr"
      },
      "source": [
        "Suppose a user wants the 10 most 'similar' (from a CBF point of view) movies from the movie 'Alley Cats Strike':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oH_oVvmTDbr",
        "outputId": "bf915e8b-ae84-49bc-f80c-e09b64aa953d"
      },
      "source": [
        "recommend(item_id=157679, num=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recommending 10 products similar to Alley Cats Strike (2000)...\n",
            "-------\n",
            "\tRecommended: Ringu 0: Bâsudei (2000) (score:0.10424703060511913)\n",
            "\tRecommended: 6th Day, The (2000) (score:0.10424703060511913)\n",
            "\tRecommended: Room 205 of Fear (2011) (score:0.0)\n",
            "\tRecommended: Legend (2015) (score:0.0)\n",
            "\tRecommended: Hardcore (2001) (score:0.0)\n",
            "\tRecommended: The Huntress: Rune of the Dead (2019) (score:0.0)\n",
            "\tRecommended: House of Dracula (1945) (score:0.0)\n",
            "\tRecommended: Schramm (1993) (score:0.0)\n",
            "\tRecommended: The Coed and the Zombie Stoner (2014) (score:0.0)\n",
            "\tRecommended: Honor Among Lovers (1931) (score:0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}