{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neohack22/IASD/blob/IA/IA/projects/privacy/Basic_Approaches_kAnonymity_lDiversity_tCloseness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouohtpTAZ-XK"
      },
      "source": [
        "# K-Anonymity + L-Diversity + T-Closeness\n",
        "\n",
        "In the late 1990s Sweeney published several articles in which she proposed the concept of k-anonymity. Among them we cite the most famous one [1] published in 2002. At the time when anonymization was referring to the concept now known as depseudonymization. As we already discusses pseudonymization is the replacement of all directly identifying data (such as the social security number) with a random value (pseudonym). Sweeney showed in [1] that it was possible to re-identify a pseudonymized relational database (SQL or more generally a tabular file) and proposed a simple notion to prevent these kind of leak from hapenning.\n",
        "\n",
        "## K-Anonymity\n",
        "\n",
        "Let us suppose that we have a dataset that contains $N$ entries.\n",
        "Each entry consists of a list of $D$ attributes $X_i$ ($i \\in [0,D]$) that contain (non-sensitive) information about a person, such as age, gender, zip code of residence, etc. These attributes are called \"quasi-identifiers\", as combining several of them into a \"super-identifier\" can often uniquely identify a person even in large datasets (e.g. the combination of gender, age and zip code might be so specific that only a single person in a dataset has a given combination see [1] for more details).\n",
        "\n",
        "k-anonymity protects the privacy of individual persons by pooling their attributes into groups of at least $k$ people. In addition, the model assumes that the dataset contains a single sensitive attribute that contains e.g. information about a person's income and that we want to protect. The method can also be generalized to datasets with more than one sensitive attribute or datasets where there's no clear distinction between quasi-identifiers and sensitive attributes. For this case study we will look at the simple case though. Now, k-anonymity demands that we group individual rows/persons of our dataset into group of at least $k$ rows/persons and replace the quasi-identifier attributes of these rows with aggregate quantities, such that it is no longer possible to read the individual values. This protects people by ensuring that an adversary who knows all values of a person's quasi-identifier attributes can only find out which group a person might belong to but not know if the person is really in the dataset.\n",
        "\n",
        "The main advantage of k-anonymity is that it is easy to understand. However, the model is not robust when the sensitive values associated with a given quasi-identifier value are all identical. In this case, we can deduce that all people with this quasi-identifier value have the same sensitive data, which we are able to deduce. It is therefore impossible to ensure that the risk of re-dentification is limited, which means that a use of k-anonymity alone does not offer reasonable guarantees of anonymity. This problem can be fixed by using an extension of k-anonymity called \"l-diversity\".\n",
        "\n",
        "## L-Diversity \n",
        "\n",
        "l-diversity, first introduced in 2007 in [2], ensures that each k-anonymous group contains at least l different values of the sensitive attribute. Therefore, even if an adversary can identify the group of a person he/she still would not be able to find out the value of that person's sensitive attribute with certainty. However, even when using l-diversity an adversary could still learn some information about a person's sensitive attribute using probabilistic reasoning: If, for example, 4 out of 5 people in a 5-anonymous group possess a given value of the sensitive attribute, an attacker can reason that a given person who he/she knows is part of the group will -with high probability- possess that value.\n",
        "Again, this problem can be fixed by extending k-anonymity using a so-called \"t-closeness\" criterion. \n",
        "\n",
        "## T-Closeness\n",
        "\n",
        "t-closeness, also introduced in 2007 in [3], demands that the statistical distribution of the sensitive attribute values in each k-anonymous group is \"close\" to the overall distribution of that attribute in the entire dataset. Typically, the closeness between two probability vectors $p=(p_1,...,p_l)$ and $q=(q_1,...,q_l)$ can be measured using e.g. the Kullback-Leibler (KL) divergence defined as follows: $$ D_{KL}(p,q) = \\sum_{i=1}^{l}p_i \\log(\\frac{p_i}{q_i}).$$\n",
        "An adversary could then only learn a limited amount of information from comparing the distribution of the values in the group to the distribution in the entire dataset.\n",
        "\n",
        "Then begins to ask the question of the usefulness of the data. Due to proximity constraints, the data does not necessarily seem directly usable. However, it is still possible to identify trends, or perform general calculations or correlations on the whole table.\n",
        "Of course, k-anonymity, l-diversity and t-closeness all limit the amount of information that a legitimate user can learn from the data as well, so typically we need to balance the degree of privacy against the utility of the resulting data. Eventhough the data does not necessarily seem directly usable. However, it is still possible to identify trends, or perform general calculations or correlations on the whole table.\n",
        "\n",
        "## Implementing privacy conditions\n",
        "\n",
        "Turning a dataset into a k-anonymous (and possibly l-diverse or t-close) dataset is a complex problem. Meyerson and Williams have shown in [4] that finding the optimal transformation of the database NP-difficult. Fortunately, several practical algorithms exists that often produce \"good enough\" results by employing greedy search techniques.\n",
        "\n",
        "In this notebook, we will explore the so-called \"Mondrian\" (see [5] for more details) algorithm, which uses a greedy search method to partition the original data into smaller and smaller groups. The algorithm assumes that we have converted all attributes into numerical or categorical values and that we're able to measure the \"span\" of a given data attribute $X_i$ (\"span\" will be detailed later).\n",
        "\n",
        "### Partitioning\n",
        "\n",
        "The algorithm proceeds then as follows to partition the data into k groups:\n",
        "\n",
        "1. Initialize the final set of partitions to an empty set $P_{final} = \\{\\}$.\n",
        "2. Initialize the temporary set of paritions to a set containing a partition with the entire dataset $P_{temp} = \\{\\{1, 2,\\dots ,N\\}\\}$.\n",
        "4. While there are partitions in the temporary set, pop one partition from it.\n",
        "  * Calculate the relative spans of all columns in the partition.\n",
        "  * Sort the resulting columns by their span (in descending order) and iterate over them. For each column,\n",
        "      * Try to split the partition along that column using the median of the column values as the split point.\n",
        "      * Check if the resulting partitions are valid according to k-anonymity (and possibly additional) criteria.\n",
        "      * If yes, add the two new partitions to the temporary set and break out of the loop.\n",
        "  * If no column produced a valid split, add the original partition to the set of final partitions.\n",
        "5. Return the final set of partitions\n",
        "\n",
        "### Data Aggregation\n",
        "\n",
        "After obtaining the partitions we still need to aggregate the values of the quasi identifiers and the sensitive attributes in each k-anonymous group. For this, we can e.g. replace numerical attributes with their range (e.g. \"age: 24-28\") and categorical attributes with their union (e.g. \"employment-group: [self-employed, employee, worker]\"), though other aggregations are possible. An open source tool developed by the Technological University of Munich, ARX (see [6]) allows anonymisation according to many models from original data in tabular format. The tool also makes it possible to estimate the risks of de-anonymization according to the models presented above, and even in a finer way by calculating the distribution of the probabilities of de-anonymization and not simply the maximum values.\n",
        "\n",
        "## Biliography\n",
        "\n",
        "- [1]Â [k-Anonymity: A Model For Protecting Privacy](https://epic.org/privacy/reidentification/Sweeney_Article.pdf)\n",
        "- [2] [l-Diversity: Privacy Beyond k-Anonymity](https://personal.utdallas.edu/~muratk/courses/privacy08f_files/ldiversity.pdf)\n",
        "- [3] [t-Closeness: Privacy Beyond k-Anonymity and l-Diversity](https://www.cs.purdue.edu/homes/ninghui/papers/t_closeness_icde07.pdf)\n",
        "- [4] [On the Complexity of Optimal K-Anonymity](http://www.aladdin.cs.cmu.edu/papers/pdfs/y2004/kanonim.pdf)\n",
        "- [5] [Mondrian - Multidimensional k-Anonymity](https://www.utdallas.edu/~muratk/courses/privacy08f_files/MultiDim.pdf)\n",
        "- [6] [Putting statistical disclosure control into practice: The ARX data anonymization tool](https://link.springer.com/chapter/10.1007/978-3-319-23633-9_6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ng7DWXF_Z-XW"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# we use Pandas to work with the data as it makes working with categorical data very easy\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we use Pandas to work with the data as it makes working with categorical data very easy\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "7IVyLGd7dWow"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "fidUZramZ-XY",
        "outputId": "6b34ee30-7b09-40b3-b562-b42119defa96"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-91b1a852a89a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34m'income'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m ))\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/adult.all.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;31m# We load the data using Pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/python_parser.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"readline\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/adult.all.txt'"
          ]
        }
      ],
      "source": [
        "# this is a list of the column names in our dataset (as the file doesn't contain any headers)\n",
        "names = (\n",
        "    'age',\n",
        "    'workclass', #Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
        "    'fnlwgt', # \"weight\" of that person in the dataset (i.e. how many people does that person represent) -> https://www.kansascityfed.org/research/datamuseum/cps/coreinfo/keyconcepts/weights\n",
        "    'education',\n",
        "    'education-num',\n",
        "    'marital-status',\n",
        "    'occupation',\n",
        "    'relationship',\n",
        "    'race',\n",
        "    'sex',\n",
        "    'capital-gain',\n",
        "    'capital-loss',\n",
        "    'hours-per-week',\n",
        "    'native-country',\n",
        "    'income',\n",
        ")\n",
        "\n",
        "# some fields are categorical and will require special treatment\n",
        "categorical = set((\n",
        "    'workclass',\n",
        "    'education',\n",
        "    'marital-status',\n",
        "    'occupation',\n",
        "    'relationship',\n",
        "    'sex',\n",
        "    'native-country',\n",
        "    'race',\n",
        "    'income',\n",
        "))\n",
        "df = pd.read_csv(\"data/adult.all.txt\", sep=\", \", header=None, names=names, index_col=False, engine='python');# We load the data using Pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is a list of the column names in our dataset \n",
        "# (as the file doesn't contain any headers)\n",
        "names = (\n",
        "    'age',\n",
        "    'workclass', #Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
        "    'fnlwgt', # \"weight\" of that person in the dataset\n",
        "    # (i.e. how many people does that person represent)\n",
        "    # -> https://www.kansascityfed.org/research/datamuseum/cps/coreinfo/keyconcepts/weights\n",
        "    'education',\n",
        "    'education-num',\n",
        "    'marital-status',\n",
        "    'occupation',\n",
        "    'relationship',\n",
        "    'race',\n",
        "    'sex',\n",
        "    'capital-gain',\n",
        "    'capital-loss',\n",
        "    'hours-per-week',\n",
        "    'native-country',\n",
        "    'income',\n",
        ")\n",
        "\n",
        "# some fields are categorical and will require special treatment\n",
        "categorical = set((\n",
        "    'workclass',\n",
        "    'education',\n",
        "    'marital-status',\n",
        "    'occupation',\n",
        "    'relationship',\n",
        "    'sex',\n",
        "    'native-country',\n",
        "    'race',\n",
        "    'income',\n",
        "))\n",
        "df = pd.read_csv(\"/content/adult.all.txt\", sep=\", \", header=None, names=names, index_col=False) #, engine'python');\n",
        "\n",
        "# We load the data using Pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_546wHSdwrY",
        "outputId": "4e1b8b7e-01b9-4027-de66-95f02f020acd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Eif2bSX1Z-Xa"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "pXyhOQncgT-o",
        "outputId": "15d4b850-90fc-4928-ceac-1567e4f73f43"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   age         workclass  fnlwgt  education  education-num  \\\n",
              "0   39         State-gov   77516  Bachelors             13   \n",
              "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
              "2   38           Private  215646    HS-grad              9   \n",
              "3   53           Private  234721       11th              7   \n",
              "4   28           Private  338409  Bachelors             13   \n",
              "\n",
              "       marital-status         occupation   relationship   race     sex  \\\n",
              "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
              "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
              "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
              "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
              "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
              "\n",
              "   capital-gain  capital-loss  hours-per-week native-country income  \n",
              "0          2174             0              40  United-States  <=50k  \n",
              "1             0             0              13  United-States  <=50k  \n",
              "2             0             0              40  United-States  <=50k  \n",
              "3             0             0              40  United-States  <=50k  \n",
              "4             0             0              40           Cuba  <=50k  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-12a96d87-5ae9-45e3-b145-ca2c407e719c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>marital-status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>native-country</th>\n",
              "      <th>income</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39</td>\n",
              "      <td>State-gov</td>\n",
              "      <td>77516</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Adm-clerical</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>2174</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50k</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>Self-emp-not-inc</td>\n",
              "      <td>83311</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Husband</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50k</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38</td>\n",
              "      <td>Private</td>\n",
              "      <td>215646</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>9</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50k</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53</td>\n",
              "      <td>Private</td>\n",
              "      <td>234721</td>\n",
              "      <td>11th</td>\n",
              "      <td>7</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Handlers-cleaners</td>\n",
              "      <td>Husband</td>\n",
              "      <td>Black</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>&lt;=50k</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>Private</td>\n",
              "      <td>338409</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Prof-specialty</td>\n",
              "      <td>Wife</td>\n",
              "      <td>Black</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>Cuba</td>\n",
              "      <td>&lt;=50k</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-12a96d87-5ae9-45e3-b145-ca2c407e719c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-12a96d87-5ae9-45e3-b145-ca2c407e719c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-12a96d87-5ae9-45e3-b145-ca2c407e719c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "li52_Lc6Z-Xa"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "for name in categorical:\n",
        "    df[name] = df[name].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name in categorical:\n",
        "  df[name] = df[name].astype('category')"
      ],
      "metadata": {
        "id": "M_SfPJOwgZKH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMwI7gn4Z-Xb"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "**Implement a function that returns the spans (max-min for numerical columns, number of different values for categorical columns) of all columns for a partition of a dataframe.** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PljiXbWJZ-Xb"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def get_spans(df, partition, scale=None):\n",
        "    \"\"\"\n",
        "    :param        df: the dataframe of reference\n",
        "    :param partition: the partition for which to calculate the spans\n",
        "    :param     scale: if given, the spans of each column will be divided\n",
        "                      by the value in `scale` for that column\n",
        "    :        returns: The spans of all columns in the partition\n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_spans(df, partition, scale=None):\n",
        "  \"\"\"\n",
        "  :param        df: the dataframe of reference\n",
        "  :param partition: the partition for which to calculate the spans\n",
        "  :param     scale: if given, the spans of each column will be divided\n",
        "                    by the value in `scale` for that column\n",
        "  :        returns: The spans of all columns in the partition\n",
        "  \"\"\"\n",
        "\n",
        "  spans = {}\n",
        "  for name in df.columns:\n",
        "    if df[name].dtypes == \"category\":\n",
        "      spans[name] = df[name][partition].nunique()\n",
        "    else:\n",
        "      spans[name] = df[name][partition].max() - df[name][partition].min()\n",
        "    if scale != None:\n",
        "      spans[name] = spans[name] / scale[name]\n",
        "      # si on fait une sous partie on aura normalisÃ© avec le span classique\n",
        "\n",
        "  return spans"
      ],
      "metadata": {
        "id": "W2bOUKCLhj4r"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lstmhYJZ-Xc"
      },
      "outputs": [],
      "source": [
        "full_spans = get_spans(df, df.index)\n",
        "full_spans"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spans (count) of the whole dataset\n",
        "full_spans = get_spans(df, df.index) # filter indexes\n",
        "full_spans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5uY3EyWm2mI",
        "outputId": "7b5bafac-aae8-48b4-8c81-91de8939ca05"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 73,\n",
              " 'workclass': 9,\n",
              " 'fnlwgt': 1478115,\n",
              " 'education': 16,\n",
              " 'education-num': 15,\n",
              " 'marital-status': 7,\n",
              " 'occupation': 15,\n",
              " 'relationship': 6,\n",
              " 'race': 5,\n",
              " 'sex': 2,\n",
              " 'capital-gain': 99999,\n",
              " 'capital-loss': 4356,\n",
              " 'hours-per-week': 98,\n",
              " 'native-country': 42,\n",
              " 'income': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t_7B9-vZ-Xd"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "**Implement a `split` function that takes a dataframe, a current partition and a column as inputs and returns two ensembles that split the given partition such that all rows with values of the column `column` below the median are in one ensemble and all rows with values above or equal to the median are in the other.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkdYKEHgZ-Xe"
      },
      "outputs": [],
      "source": [
        "def split(df, partition, column):\n",
        "    \"\"\"\n",
        "    :param        df: The dataframe of reference\n",
        "    :param partition: The partition to split\n",
        "    :param    column: The column along which to split\n",
        "    :        returns: A tuple containing a split of the original partition\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def split(df, partition, column):\n",
        "  \"\"\"\n",
        "  :param        df: The dataframe of reference\n",
        "  :param partition: The partition to split\n",
        "  :param    column: The column along which to split\n",
        "  :        returns: A tuple containing a split of the original partition\n",
        "  \"\"\"\n",
        "  index_tuple = ()\n",
        "\n",
        "  sorted_df = df.sort_values(by=column, ascending=True)\n",
        "\n",
        "  if sorted_df[column][partition].dtypes == \"int64\":\n",
        "    fs_index = sorted_df.iloc[\n",
        "        0:int(sorted_df[column][partition].median())].index\n",
        "    first_split = sorted_df.iloc[\n",
        "        0:int(sorted_df[column][partition].median() + 1):]\n",
        "    ss_index = sorted_df.iloc[\n",
        "        int(sorted_df[column][partition].median() +1):].index\n",
        "    second_split = sorted_df.iloc[sorted_df[column][partition].count() + 1:]\n",
        "\n",
        "  else:\n",
        "    fs_index = sorted_df.iloc[\n",
        "        0: sorted_df[column][partition].count() // 2].index\n",
        "    first_split = sorted_df.iloc[0: sorted_df[column][partition].count() // 2]\n",
        "    ss_index = sorted_df.iloc[\n",
        "        sorted_df[column][partition].count() // 2+1:].index\n",
        "    second_split = sorted_df.iloc[sorted_df[column][partition].count() // 2+1:]\n",
        "\n",
        "  index_tuple = (fs_index, ss_index)\n",
        "\n",
        "  return index_tuple, first_split, second_split"
      ],
      "metadata": {
        "id": "-Be4CKVNnu6p"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "split(df, df.index, \"age\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MxdoywAW7Dgv",
        "outputId": "48e70359-c6a7-4245-ed87-a624be720f23"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((Int64Index([28092, 46962, 42922,  1389, 32963, 48407, 27759, 11234,   572,\n",
              "              46961, 44542, 47729, 41634, 31864, 12838, 42361, 34335, 40248,\n",
              "              11925,  7779, 29130, 24391, 42334, 14720, 12787, 31758, 18389,\n",
              "              22479, 19206, 18747, 10449, 19211, 16994, 11194, 12802, 45357,\n",
              "              24404],\n",
              "             dtype='int64'),\n",
              "  Int64Index([43878, 41184, 30402, 44560, 23660, 36933, 19236, 25541, 35827,\n",
              "               7755,\n",
              "              ...\n",
              "              47649, 12975, 40988, 15356,  1935,  5104, 18725,   222, 31696,\n",
              "               6624],\n",
              "             dtype='int64', length=48804)),\n",
              "        age         workclass  fnlwgt education  education-num marital-status  \\\n",
              " 28092   17           Private   36877      10th              6  Never-married   \n",
              " 46962   17           Private   73820      12th              8  Never-married   \n",
              " 42922   17           Private  165457      10th              6  Never-married   \n",
              " 1389    17           Private   46496      11th              7  Never-married   \n",
              " 32963   17           Private   40299      11th              7  Never-married   \n",
              " 48407   17                 ?  246974      12th              8  Never-married   \n",
              " 27759   17           Private  317702      10th              6  Never-married   \n",
              " 11234   17                 ?  297117      11th              7  Never-married   \n",
              " 572     17           Private  242718      11th              7  Never-married   \n",
              " 46961   17           Private   94492      11th              7  Never-married   \n",
              " 44542   17           Private   58037      10th              6  Never-married   \n",
              " 47729   17           Private  208046   HS-grad              9  Never-married   \n",
              " 41634   17           Private  141445      10th              6  Never-married   \n",
              " 31864   17           Private  214787      12th              8  Never-married   \n",
              " 12838   17           Private  126771      12th              8  Never-married   \n",
              " 42361   17           Private  147069      11th              7  Never-married   \n",
              " 34335   17                 ?  347248      10th              6  Never-married   \n",
              " 40248   17           Private   98005      11th              7  Never-married   \n",
              " 11925   17           Private  114798      10th              6  Never-married   \n",
              " 7779    17           Private  287160      11th              7  Never-married   \n",
              " 29130   17           Private   28031       9th              5  Never-married   \n",
              " 24391   17           Private   41865      10th              6  Never-married   \n",
              " 42334   17           Private  213354      11th              7  Never-married   \n",
              " 14720   17           Private   99462      11th              7  Never-married   \n",
              " 12787   17         Local-gov  308901      11th              7  Never-married   \n",
              " 31758   17                 ?  171461      10th              6  Never-married   \n",
              " 18389   17         Local-gov  244856      11th              7  Never-married   \n",
              " 22479   17           Private  184198      11th              7  Never-married   \n",
              " 19206   17           Private  183066      10th              6  Never-married   \n",
              " 18747   17      Self-emp-inc  183784      10th              6  Never-married   \n",
              " 10449   17           Private  191910      10th              6  Never-married   \n",
              " 19211   17           Private  148522      11th              7  Never-married   \n",
              " 16994   17           Private  142457      11th              7  Never-married   \n",
              " 11194   17           Private  244589      11th              7  Never-married   \n",
              " 12802   17           Private  117549      10th              6  Never-married   \n",
              " 45357   17           Private  121470      12th              8  Never-married   \n",
              " 24404   17           Private   25690      10th              6  Never-married   \n",
              " 10443   17  Self-emp-not-inc  228786      10th              6  Never-married   \n",
              " \n",
              "               occupation    relationship                race     sex  \\\n",
              " 28092              Sales       Own-child               White  Female   \n",
              " 46962              Sales       Own-child               White  Female   \n",
              " 42922      Other-service       Own-child               White    Male   \n",
              " 1389       Other-service       Own-child               White    Male   \n",
              " 32963              Sales       Own-child               White  Female   \n",
              " 48407                  ?       Own-child               White    Male   \n",
              " 27759              Sales       Own-child               Black  Female   \n",
              " 11234                  ?       Own-child               White  Female   \n",
              " 572                Sales       Own-child               White    Male   \n",
              " 46961              Sales       Own-child               White  Female   \n",
              " 44542      Other-service       Own-child               White    Male   \n",
              " 47729              Sales       Own-child               Black  Female   \n",
              " 41634      Other-service       Own-child               White    Male   \n",
              " 31864       Adm-clerical       Own-child               White  Female   \n",
              " 12838     Prof-specialty       Own-child               White    Male   \n",
              " 42361      Other-service       Own-child               White  Female   \n",
              " 34335                  ?       Own-child               White  Female   \n",
              " 40248              Sales       Own-child               White  Female   \n",
              " 11925      Other-service       Own-child               White  Female   \n",
              " 7779       Other-service       Own-child               White  Female   \n",
              " 29130      Other-service       Own-child               White    Male   \n",
              " 24391      Other-service       Own-child               White  Female   \n",
              " 42334  Handlers-cleaners       Own-child               White    Male   \n",
              " 14720      Other-service       Own-child  Amer-Indian-Eskimo  Female   \n",
              " 12787       Adm-clerical       Own-child               White  Female   \n",
              " 31758                  ?       Own-child               White  Female   \n",
              " 18389     Prof-specialty       Own-child               White  Female   \n",
              " 22479              Sales       Own-child               White  Female   \n",
              " 19206      Other-service       Own-child               White  Female   \n",
              " 18747              Sales       Own-child               White    Male   \n",
              " 10449      Other-service       Own-child               White    Male   \n",
              " 19211      Other-service       Own-child               White    Male   \n",
              " 16994      Other-service       Own-child               Black    Male   \n",
              " 11194              Sales       Own-child               White  Female   \n",
              " 12802              Sales  Other-relative               Black  Female   \n",
              " 45357   Transport-moving       Own-child               White    Male   \n",
              " 24404      Other-service       Own-child               White  Female   \n",
              " 10443      Other-service       Own-child               White  Female   \n",
              " \n",
              "        capital-gain  capital-loss  hours-per-week native-country income  \n",
              " 28092             0             0              10  United-States  <=50k  \n",
              " 46962             0             0               8  United-States  <=50k  \n",
              " 42922             0             0              16  United-States  <=50k  \n",
              " 1389              0             0               5  United-States  <=50k  \n",
              " 32963             0             0              25  United-States  <=50k  \n",
              " 48407             0             0              30  United-States  <=50k  \n",
              " 27759             0             0              15  United-States  <=50k  \n",
              " 11234             0             0              40  United-States  <=50k  \n",
              " 572               0             0              12  United-States  <=50k  \n",
              " 46961             0             0              19  United-States  <=50k  \n",
              " 44542             0             0              40  United-States  <=50k  \n",
              " 47729             0             0              16  United-States  <=50k  \n",
              " 41634             0             0              30  United-States  <=50k  \n",
              " 31864             0             0              25  United-States  <=50k  \n",
              " 12838             0             0               7  United-States  <=50k  \n",
              " 42361             0             0              16  United-States  <=50k  \n",
              " 34335             0             0              20  United-States  <=50k  \n",
              " 40248             0             0              20  United-States  <=50k  \n",
              " 11925             0             0              20  United-States  <=50k  \n",
              " 7779              0             0              15  United-States  <=50k  \n",
              " 29130             0             0              16  United-States  <=50k  \n",
              " 24391             0             0              25  United-States  <=50k  \n",
              " 42334             0             0               6  United-States  <=50k  \n",
              " 14720             0             0              20  United-States  <=50k  \n",
              " 12787             0             0              15  United-States  <=50k  \n",
              " 31758             0             0              20  United-States  <=50k  \n",
              " 18389             0             0              40  United-States  <=50k  \n",
              " 22479             0             0              13  United-States  <=50k  \n",
              " 19206             0             0              25  United-States  <=50k  \n",
              " 18747             0             0              15  United-States  <=50k  \n",
              " 10449             0             0              25  United-States  <=50k  \n",
              " 19211             0          1721              15  United-States  <=50k  \n",
              " 16994             0             0              20  United-States  <=50k  \n",
              " 11194             0             0              40  United-States  <=50k  \n",
              " 12802             0             0              12  United-States  <=50k  \n",
              " 45357             0             0              10              ?  <=50k  \n",
              " 24404             0             0              10  United-States  <=50k  \n",
              " 10443             0             0              24  United-States  <=50k  ,\n",
              " Empty DataFrame\n",
              " Columns: [age, workclass, fnlwgt, education, education-num, marital-status, occupation, relationship, race, sex, capital-gain, capital-loss, hours-per-week, native-country, income]\n",
              " Index: [])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split(df, partition, column):\n",
        "  partitioned_df = df[column][partition]\n",
        "  if column in categorical:\n",
        "    values_list = partitioned_df.unique()\n",
        "    first_partition = set(values[:len(values)//2])\n",
        "    second_partition = set(values[len(values)//2:])\n",
        "    return partitioned_df.index[\n",
        "        partitioned_df.isin(\n",
        "            first_partition), partitioned_df.index[\n",
        "                partitioned_df.isin(second_partition)]]\n",
        "  else:\n",
        "    partition_median = partitioned_df.median()\n",
        "    first_split = partitioned_df.index[partitioned_df < partition_median]\n",
        "    second_split = partitioned_df.index[\n",
        "        partitioned_df >= partition_median]\n",
        "    return (first_split, second_split)"
      ],
      "metadata": {
        "id": "qShhsccH7fG1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split(df, df.index, \"age\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OScROAib9bI4",
        "outputId": "5cd7a5fc-3ec7-4484-928c-771cb34e58bc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Int64Index([    4,     8,    11,    12,    13,    15,    16,    17,    22,\n",
              "                26,\n",
              "             ...\n",
              "             48810, 48817, 48819, 48821, 48824, 48830, 48833, 48834, 48836,\n",
              "             48841],\n",
              "            dtype='int64', length=23694),\n",
              " Int64Index([    0,     1,     2,     3,     5,     6,     7,     9,    10,\n",
              "                14,\n",
              "             ...\n",
              "             48827, 48828, 48829, 48831, 48832, 48835, 48837, 48838, 48839,\n",
              "             48840],\n",
              "            dtype='int64', length=25148))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAmDV3n3Z-Xe"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Now that we have all helper functions in place, we can implement the partition algorithm discussed above:\n",
        "\n",
        "**Implement the partitioning algorithm discussed above, using a k-anonymous criterion for the partitions you create.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rTKjUTZZ-Xf"
      },
      "outputs": [],
      "source": [
        "def is_k_anonymous(df, partition, sensitive_column, k=3):\n",
        "    \"\"\"\n",
        "    :param               df: The dataframe on which to check the partition.\n",
        "    :param        partition: The partition of the dataframe to check.\n",
        "    :param sensitive_column: The name of the sensitive column\n",
        "    :param                k: The desired k\n",
        "    :returns               : True if the partition is valid according to our k-anonymity criteria, False otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "def partition_dataset(df, feature_columns, sensitive_column, scale, is_valid):\n",
        "    \"\"\"\n",
        "    :param               df: The dataframe to be partitioned.\n",
        "    :param  feature_columns: A list of column names along which to partition the dataset.\n",
        "    :param sensitive_column: The name of the sensitive column (to be passed on to the `is_valid` function)\n",
        "    :param            scale: The column spans as generated before.\n",
        "    :param         is_valid: A function that takes a dataframe and a partition and returns True if the partition is valid.\n",
        "    :returns               : A list of valid partitions that cover the entire dataframe.\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uWe02TiZ-Xf"
      },
      "source": [
        "Now let's try this on our dataset! To keep things simple, we will at first select only two columns from the dataset that we apply the partitioning to. This makes it easier to check/visualize the result and speed up the execution (the naive algorithms can take several minutes when running on the entire dataset) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGgryw_cZ-Xg"
      },
      "outputs": [],
      "source": [
        "# we apply our partitioning method to two columns of our dataset, using \"income\" as the sensitive attribute\n",
        "feature_columns = ['age', 'hours-per-week']\n",
        "sensitive_column = 'income'\n",
        "final_partitions = partition_dataset(df, feature_columns, sensitive_column, full_spans, is_k_anonymous)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqFEvtzFZ-Xg"
      },
      "outputs": [],
      "source": [
        "# we get the number of partitions that were created\n",
        "len(final_partitions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN3maPt1Z-Xh"
      },
      "source": [
        "Let's visualize the created partitions! To do that, we will write functions to get the rectangular bounds of a partition along two columns. We can then plot these rectangles to see how our partitioning function divides the dataset. If we perform the partition only along the two columns selected for plotting then the resulting rectangles should not overlap and cover the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx3JLvmBZ-Xi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pylab as pl\n",
        "import matplotlib.patches as patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDvK1SXiZ-Xi"
      },
      "outputs": [],
      "source": [
        "def build_indexes(df):\n",
        "    indexes = {}\n",
        "    for column in categorical:\n",
        "        values = sorted(df[column].unique())\n",
        "        indexes[column] = { x : y for x, y in zip(values, range(len(values)))}\n",
        "    return indexes\n",
        "\n",
        "def get_coords(df, column, partition, indexes, offset=0.1):\n",
        "    if column in categorical:\n",
        "        sv = df[column][partition].sort_values()\n",
        "        l, r = indexes[column][sv[sv.index[0]]], indexes[column][sv[sv.index[-1]]]+1.0\n",
        "    else:\n",
        "        sv = df[column][partition].sort_values()\n",
        "        next_value = sv[sv.index[-1]]\n",
        "        larger_values = df[df[column] > next_value][column]\n",
        "        if len(larger_values) > 0:\n",
        "            next_value = larger_values.min()\n",
        "        l = sv[sv.index[0]]\n",
        "        r = next_value\n",
        "    # we add some offset to make the partitions more easily visible\n",
        "    l -= offset\n",
        "    r += offset\n",
        "    return l, r\n",
        "\n",
        "def get_partition_rects(df, partitions, column_x, column_y, indexes, offsets=[0.1, 0.1]):\n",
        "    rects = []\n",
        "    for partition in partitions:\n",
        "        xl, xr = get_coords(df, column_x, partition, indexes, offset=offsets[0])\n",
        "        yl, yr = get_coords(df, column_y, partition, indexes, offset=offsets[1])\n",
        "        rects.append(((xl, yl),(xr, yr)))\n",
        "    return rects\n",
        "\n",
        "def get_bounds(df, column, indexes, offset=1.0):\n",
        "    if column in categorical:\n",
        "        return 0-offset, len(indexes[column])+offset\n",
        "    return df[column].min()-offset, df[column].max()+offset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuN5ZL9cZ-Xi"
      },
      "outputs": [],
      "source": [
        "feature_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0QUa3ehZ-Xj"
      },
      "outputs": [],
      "source": [
        "# we calculate the bounding rects of all partitions that we created\n",
        "indexes = build_indexes(df)\n",
        "column_x, column_y = feature_columns[:2]\n",
        "rects = get_partition_rects(df, final_partitions, column_x, column_y, indexes, offsets=[0.0, 0.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dv5392ikZ-Xj"
      },
      "outputs": [],
      "source": [
        "# let's see how our rectangles look like\n",
        "rects[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcsDXDIiZ-Xk"
      },
      "outputs": [],
      "source": [
        "# we plot the rectangles\n",
        "def plot_rects(df, ax, rects, column_x, column_y, edgecolor='black', facecolor='none'):\n",
        "    for (xl, yl),(xr, yr) in rects:\n",
        "        ax.add_patch(patches.Rectangle((xl,yl),xr-xl,yr-yl,linewidth=1,edgecolor=edgecolor,facecolor=facecolor, alpha=0.5))\n",
        "    ax.set_xlim(*get_bounds(df, column_x, indexes))\n",
        "    ax.set_ylim(*get_bounds(df, column_y, indexes))\n",
        "    ax.set_xlabel(column_x)\n",
        "    ax.set_ylabel(column_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmzF0b7GZ-Xk"
      },
      "outputs": [],
      "source": [
        "pl.figure(figsize=(20,20))\n",
        "ax = pl.subplot(111)\n",
        "plot_rects(df, ax, rects, column_x, column_y, facecolor='r')\n",
        "#pl.scatter(df[column_x], df[column_y])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7ZTgXeFZ-Xl"
      },
      "source": [
        "**Brifely analyze the result we get.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90UWj61OZ-Xl"
      },
      "source": [
        "# Generating an k-Anonymous Dataset\n",
        "\n",
        "Of course, to use the data we want to produce a new dataset that contains one row for each partition and value of the sensitive attribute. To do this, we need to aggregate the columns in each partition.  Let's do this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMUU7snBZ-Xl"
      },
      "outputs": [],
      "source": [
        "def agg_categorical_column(series):\n",
        "    return [','.join(set(series))]\n",
        "\n",
        "def agg_numerical_column(series):\n",
        "    return [series.mean()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ao5_NwWeZ-Xm"
      },
      "outputs": [],
      "source": [
        "def build_anonymized_dataset(df, partitions, feature_columns, sensitive_column, max_partitions=None):\n",
        "    aggregations = {}\n",
        "    for column in feature_columns:\n",
        "        if column in categorical:\n",
        "            aggregations[column] = agg_categorical_column\n",
        "        else:\n",
        "            aggregations[column] = agg_numerical_column\n",
        "    rows = []\n",
        "    for i, partition in enumerate(partitions):\n",
        "        if i % 100 == 1:\n",
        "            print(\"Final {} partitions...\".format(i))\n",
        "        if max_partitions is not None and i > max_partitions:\n",
        "            break\n",
        "        grouped_columns = df.loc[partition].agg(aggregations, squeeze=False)\n",
        "        sensitive_counts = df.loc[partition].groupby(sensitive_column).agg({sensitive_column : 'count'})\n",
        "        values = grouped_columns.iloc[0].to_dict()\n",
        "        for sensitive_value, count in sensitive_counts[sensitive_column].items():\n",
        "            if count == 0:\n",
        "                continue\n",
        "            values.update({\n",
        "                sensitive_column : sensitive_value,\n",
        "                'count' : count,\n",
        "\n",
        "            })\n",
        "            rows.append(values.copy())\n",
        "    return pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySFSaNihZ-Xm"
      },
      "outputs": [],
      "source": [
        "dfn = build_anonymized_dataset(df, final_partitions, feature_columns, sensitive_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZI-bQBFrZ-Xm"
      },
      "outputs": [],
      "source": [
        "# we sort the resulting dataframe using the feature columns and the sensitive attribute\n",
        "dfn.sort_values(feature_columns+[sensitive_column])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlaMAgu-Z-Xm"
      },
      "source": [
        "# Implementing l-diversity (the naive way)\n",
        "\n",
        "Now let's see how we can implement l-diversity in order to protect the privacy of the persons in the dataset even better. To implement l-diversity, we can do the following things:\n",
        "\n",
        "* Modify our `is_valid` function to not only check for the size of a given partition but also ensure that the values of the sensitive attribute in the partition are diverse enough.\n",
        "* Modify the `split` function to produce splits that are diverse (if possible)\n",
        "\n",
        "Here we will only implement the first point to keep things simple, please keep in mind that this is not the smartest way to implement l-diversity, as our \"naive\" splitting function might produce invalid splits even when it would actually be possible to produce a valid one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBSCSNpXZ-Xm"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "**Implement a validator function that returns `True` if a given partition contains at least `l` different values of the sensitive attribute, `False` otherwise.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26lNJZspZ-Xn"
      },
      "outputs": [],
      "source": [
        "def is_l_diverse(df, partition, sensitive_column, l=2):\n",
        "    \"\"\"\n",
        "    :param               df: The dataframe for which to check l-diversity\n",
        "    :param        partition: The partition of the dataframe on which to check l-diversity\n",
        "    :param sensitive_column: The name of the sensitive column\n",
        "    :param                l: The minimum required diversity of sensitive attribute values in the partition\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CdYRJFFZ-Xn"
      },
      "outputs": [],
      "source": [
        "# now let's apply this method to our data and see how the result changes\n",
        "final_l_diverse_partitions = partition_dataset(df, feature_columns, sensitive_column, full_spans, lambda *args: is_k_anonymous(*args) and is_l_diverse(*args))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvLnhx1qZ-Xn"
      },
      "outputs": [],
      "source": [
        "len(final_l_diverse_partitions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLATG_LcZ-Xo"
      },
      "outputs": [],
      "source": [
        "column_x, column_y = feature_columns[:2]\n",
        "l_diverse_rects = get_partition_rects(df, final_l_diverse_partitions, column_x, column_y, indexes, offsets=[0.0, 0.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umzVBUK4Z-Xo"
      },
      "outputs": [],
      "source": [
        "pl.figure(figsize=(20,20))\n",
        "ax = pl.subplot(111)\n",
        "plot_rects(df, ax, l_diverse_rects, column_x, column_y, edgecolor='b', facecolor='b')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YgPS4pjZ-Xo"
      },
      "source": [
        "**Brifely analyze the result we get. Compare with simple k-anonymity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqP3MkASZ-Xo"
      },
      "outputs": [],
      "source": [
        "# again we build an anonymized dataset from the l-diverse partitions\n",
        "dfl = build_anonymized_dataset(df, final_l_diverse_partitions, feature_columns, sensitive_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veUeVUA2Z-Xp"
      },
      "outputs": [],
      "source": [
        "# Let's see how l-diversity improves the anonymity of our dataset\n",
        "dfl.sort_values([column_x, column_y, sensitive_column])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C257QcVZ-Xp"
      },
      "source": [
        "# Implementing t-closeness\n",
        "\n",
        "As we can see, for regions where the value diversity is low, our l-diverse method produces partitions that contain a very large number of entries for one value of the sensitive attribute and only one entry for the other value. This is not ideal as while there is \"plausible deniability\" for a person in the dataset (after all the person could be the one \"outlier\") but an adversary can still be very certain about the person's attribute value in that case.\n",
        "\n",
        "t-closeness solves this problem by making sure that the distribution of sensitive attribute values in a given partition is similar to the distribution of the values in the overall dataset. We'll implement a naive (and not efficient / correct) version of t-closeness below. As with the l-diversity case, it would be better to tailor the `split` function to produce partitions that are t-close, which would increase the efficiency of the method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9b1k3LtZ-Xp"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "**Implement a version of the `is_valid` function that returns `True` if the partition is diverse enough and `False` otherwise. To measure diversity, calculate the total variation distance (easier to implement compared to KL) between the empirical probability distribution of the sensitive attribute over the entire dataset vs. the distribution over\n",
        "the partition. Hint: the total variation distance is the maximum pointwise absolute difference between the two distributions. You can assume that the sensitive attribute is a categorical value.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUyQ4qNMZ-Xp"
      },
      "outputs": [],
      "source": [
        "# here we generate the global frequencies for the sensitive column \n",
        "global_freqs = {}\n",
        "total_count = float(len(df))\n",
        "group_counts = df.groupby(sensitive_column)[sensitive_column].agg('count')\n",
        "for value, count in group_counts.to_dict().items():\n",
        "    p = count/total_count\n",
        "    global_freqs[value] = p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Iy65CZHZ-Xq"
      },
      "outputs": [],
      "source": [
        "global_freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ensQTNkfZ-Xq"
      },
      "outputs": [],
      "source": [
        "def is_t_close(df, partition, sensitive_column, global_freqs, p=0.2):\n",
        "    \"\"\"\n",
        "    :param               df: The dataframe for which to check l-diversity\n",
        "    :param        partition: The partition of the dataframe on which to check l-diversity\n",
        "    :param sensitive_column: The name of the sensitive column\n",
        "    :param     global_freqs: The global frequencies of the sensitive attribute values\n",
        "    :param                p: The maximum allowed distance\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zacsj5krZ-Xq"
      },
      "outputs": [],
      "source": [
        "# Let's apply this to our dataset\n",
        "final_t_close_partitions = partition_dataset(df, feature_columns, sensitive_column, full_spans, lambda *args: is_k_anonymous(*args) and is_t_close(*args, global_freqs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWvmJLrTZ-Xq"
      },
      "outputs": [],
      "source": [
        "len(final_t_close_partitions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3jOgzaJZ-Xq"
      },
      "outputs": [],
      "source": [
        "dft = build_anonymized_dataset(df, final_t_close_partitions, feature_columns, sensitive_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSZ1xR-IZ-Xr"
      },
      "outputs": [],
      "source": [
        "# Let's see how t-closeness fares\n",
        "dft.sort_values([column_x, column_y, sensitive_column])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIxS5LmCZ-Xr"
      },
      "outputs": [],
      "source": [
        "column_x, column_y = feature_columns[:2]\n",
        "t_close_rects = get_partition_rects(df, final_t_close_partitions, column_x, column_y, indexes, offsets=[0.0, 0.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxzFAgE4Z-Xr"
      },
      "outputs": [],
      "source": [
        "pl.figure(figsize=(20,20))\n",
        "ax = pl.subplot(111)\n",
        "plot_rects(df, ax, t_close_rects, column_x, column_y, edgecolor='b', facecolor='b')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RWv_3HIZ-Xr"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "**Brifely analyze the result we get. Compare with simple l-diversity and simple k-anonymity**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}