{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neohack22/IASD/blob/graphs/WS_22_03_IASD_Train_Test_Split.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoaqhgJQC_fC"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Install the necessary libraries in your Colab notebook environment and connect to your hosted Neo4J Sandbox."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmHymBmsVYqx",
        "outputId": "b6ddb0a6-8c7c-4cbd-c209-9d97f13f2e43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.7/dist-packages (5.0.0)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from neo4j) (2022.2.1)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install neo4j pyspark matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhmJ1QEdC-4e"
      },
      "outputs": [],
      "source": [
        "ip = \"54.174.38.179\"\n",
        "bolt_port = \"7687\"\n",
        "username = \"neo4j\"\n",
        "password = \"spots-carrier-wires\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI-e7X2UuOgR",
        "outputId": "5afdcc74-61b1-4fd8-a890-fd9829140fe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54.174.38.179:7687\n"
          ]
        }
      ],
      "source": [
        "from neo4j import GraphDatabase\n",
        "\n",
        "driver = GraphDatabase.driver(\"bolt://\" + ip + \":\" + bolt_port, auth=(username, password))\n",
        "\n",
        "print(driver.address) # your-sandbox-ip:your-sandbox-bolt-port"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_3fld9VKPs4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from neo4j import unit_of_work\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5aOcDSFldr-"
      },
      "source": [
        "# Train / test split\n",
        "\n",
        "We need to produce a train and test datasets on which we can build, and then evaluate a model. This binary classifier will predict wether the two nodes composing the pair shoulde be linked (1) or not (0).\n",
        "\n",
        "This classifier will be trained on a dataset composed of nodes pairs and tested on an isolated dataset of nodes pairs. We need to be careful when splitting the data, because pairs of nodes in our training set should not be connected to those in the test set.\n",
        "\n",
        "In order to build the features for each pair of nodes in the training set, we will compute measures that should not contain information from the test set (and vice versa). **The solution is to split our co-authorship graph into two sub graphs of similar structure**. \n",
        "\n",
        "We will use the year of first collaboration to split our initial graph in two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrynYYXMldsA"
      },
      "source": [
        "## Year split\n",
        "\n",
        "We can create train and test graphs by splitting the data on a particular year. Now we need to figure out what year that should be. Let's have a look at the distribution of the first year that co-authors collaborated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNzeR6rCKpxO"
      },
      "source": [
        "- Count the number of CO_AUTHOR relationships by year of first collaboration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BOhn-NZMuvs"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "MATCH ()-[r:CO_AUTHOR]->()\n",
        "WITH r.year AS year, COUNT(*) AS count\n",
        "ORDER BY year\n",
        "RETURN toString(year) AS year, count\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtLlbAjWM7SA"
      },
      "source": [
        "- Save the results of your counting in a Spark DataFrame by running the cell below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYOuTwT_P3nW"
      },
      "outputs": [],
      "source": [
        "# Run to execute the query in Colab\n",
        "\n",
        "with driver.session() as session:\n",
        "    result = session.run(query)\n",
        "    # Convert the result to a list of dictionaries\n",
        "    result_dict = [dict(record) for record in result]\n",
        "    # Create DataFrame from this list\n",
        "    co_authorships_by_year = spark.createDataFrame(result_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbCuRh7zPtdZ"
      },
      "source": [
        "- Plot the distribution as a bar graph by running the cell below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azLvZUikldsA"
      },
      "source": [
        "It looks like 2006 would act as a good year on which to split the data. We'll take all the co-authorships from 2005 and earlier as our train graph, and everything from 2006 onwards as the test graph.\n",
        "\n",
        "Let's create explicit *CO_AUTHOR_EARLY* and *CO_AUTHOR_LATE* relationships in our graph based on that year. We will then use them to distinguish train and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRdOYFrQQnxl"
      },
      "source": [
        " - Create the *CO_AUTHOR_EARLY* and *CO_AUTHOR_LATE* relationships"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtN6mMEdQnFT"
      },
      "outputs": [],
      "source": [
        "# Test the queries in your Neo4J Browser first\n",
        "\n",
        "query_early = \"\"\"\n",
        "  MATCH (...)-[...]->(...)\n",
        "  WHERE ...\n",
        "  MERGE (...)-[:CO_AUTHOR_EARLY {year: r.year}]-(...)\n",
        "\"\"\"\n",
        "\n",
        "query_late = \"\"\"\n",
        "  MATCH (...)-[...]->(...)\n",
        "  WHERE ...\n",
        "  MERGE (...)-[:CO_AUTHOR_LATE {year: r.year}]-(...)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYv5OWcQldsA"
      },
      "outputs": [],
      "source": [
        "# Run to execute the queries in Colab\n",
        "\n",
        "with driver.session() as session:\n",
        "    session.run(query_early)\n",
        "\n",
        "with driver.session() as session:\n",
        "    session.run(query_late)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNkhvKdcldsA"
      },
      "source": [
        "- Run the following cell to count how many *CO_AUTHOR* relationships we have in each of these sub graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCyjg06VldsA"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "  MATCH ()-[r]->()\n",
        "  WHERE TYPE(r) = \"CO_AUTHOR_EARLY\" OR TYPE(r) = \"CO_AUTHOR_LATE\"\n",
        "  RETURN TYPE(r), COUNT(*) AS count\n",
        "\"\"\"\n",
        "\n",
        "with driver.session() as session:\n",
        "    result = session.run(query)\n",
        "    early_late_count = spark.createDataFrame([dict(record) for record in result])\n",
        "\n",
        "early_late_count.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7kxaXsfldsD"
      },
      "source": [
        "## Class imbalance considerations\n",
        "\n",
        "Using 2006 as the split year, we have a split of 52% - 48% split of *CO_AUTHOR* relationships. We will use this split to separate our **positive examples** (i.e. pairs of nodes that we will label with 1) in train and test set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRwmrikioJHK"
      },
      "source": [
        "## Create the training set pairs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3i_H5haYLGl"
      },
      "source": [
        "- Complete the following cell to extract the **positive examples** for the training set in a Spark DataFrame. That is the pairs of nodes that are linked by a *CO_AUTHOR_EARLY* relationship."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL3FGUb3Y6DQ"
      },
      "outputs": [],
      "source": [
        "# Test the query in your Neo4J Browser\n",
        "\n",
        "query = \"\"\"\n",
        "  MATCH (...)-[...]->(...)\n",
        "  RETURN ID(author) AS node1, ID(other) AS node2, 1 AS label\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvFiu397YhRE"
      },
      "outputs": [],
      "source": [
        "# Run to execute the query in Colab\n",
        "\n",
        "with driver.session() as session:\n",
        "    result = session.run(query)\n",
        "    train_existing_links = spark.createDataFrame([dict(record) for record in result])\n",
        "    train_existing_links = train_existing_links.dropDuplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e34Vd-Eiv98"
      },
      "source": [
        "We also need to split our **negative examples** (i.e. pairs of nodes that we will label with 0, because no *CO_AUTHOR* link exist between them) between train and test sets.\n",
        "\n",
        "The simplest approach would be to use all pair of nodes within each sub graph that don’t have a relationship. The problem with this approach is that there are significantly more examples of pairs of nodes that don’t have a relationship than there are pairs of nodes that do. If we use all of these negative examples in our training set we will have a massive class imbalance.\n",
        "\n",
        "We need to reduce the number of negative examples to produce a training dataset with balanced classes. \n",
        "\n",
        "To do so, we will:\n",
        "- use pairs of nodes that are two to three of hops away from each other\n",
        "- further down sample the negative examples if necessary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql3Q6pkQZ8F6"
      },
      "source": [
        "- Extract the **negative examples** for the training set in a Spark DataFrame. That is the pairs of *Author* nodes in the *EARLY* sub graph that are two to three hops away and that did not collaborate together yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MdISIUoacUp"
      },
      "outputs": [],
      "source": [
        "# Test the query in your Neo4J Browser\n",
        "\n",
        "query = \"\"\"\n",
        "  MATCH (author)-[...*...]->(...) // Two to three hops away\n",
        "  WHERE NOT((author)-[...]-(...)) // Did not collaborate together\n",
        "  RETURN ID(author) AS node1, ID(other) AS node2, 0 AS label\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmXsm64fldsD"
      },
      "outputs": [],
      "source": [
        "# Run to execute the query in Colab\n",
        "\n",
        "with driver.session() as session:\n",
        "    result = session.run(query)\n",
        "    train_missing_links = spark.createDataFrame([dict(record) for record in result])    \n",
        "    train_missing_links = train_missing_links.drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isVVPHeIRquI"
      },
      "source": [
        "We add the positive pairs to the negative ones in order to create our training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcCP2d9ZRwZ9",
        "outputId": "6deebade-f95b-48ed-eb0b-c160b358302e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observations in training set:  188505\n"
          ]
        }
      ],
      "source": [
        "training_df = train_missing_links.union(train_existing_links)\n",
        "print('Observations in training set: ', training_df.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWBhn30vbWhl"
      },
      "source": [
        "- Complete the following cell to count the number of positive and negative pairs in the training set (use a groupBy on the Spark DataFrame). Do we have class imbalance?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "peAYj2zildsD",
        "outputId": "27bb1b93-86ef-476c-aaaa-997261cb3764"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-120bd8436be9>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    training_df. ... .show()\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "print('Train set class imbalance:')\n",
        "training_df. ... .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDYN8TH0bqkx"
      },
      "source": [
        "- Randomly downsample the **negative examples** to get a balanced training dataset (50% label 0 and 50% label 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrjvuD22TnUK"
      },
      "outputs": [],
      "source": [
        "### Isolate and count the negative examples\n",
        "\n",
        "train_df_class_0 = training_df.filter(...)\n",
        "train_count_class_0 = train_df_class_0.count()\n",
        "\n",
        "### Isolate and count the positive examples\n",
        "\n",
        "train_df_class_1 = training_df.filter(...)\n",
        "train_count_class_1 = train_df_class_1.count()\n",
        "\n",
        "### Compute the proportion of positive examples\n",
        "fraction = train_count_class_1/train_count_class_0\n",
        "\n",
        "### Sample the negative examples to reflect the proportion of positive examples\n",
        "### Hint use pyspark.sql.DataFrame.sample function \n",
        "\n",
        "train_df_class_0_under = train_df_class_0.sample(...)\n",
        "\n",
        "### Add these sample to the positive examples to get a balanced training set\n",
        "df_train_under = train_df_class_0_under.union(train_df_class_1)\n",
        "\n",
        "# Show class imbalance now\n",
        "print('Train set class imbalance after downsampling:')\n",
        "df_train_under.groupBy(F.col('label')).count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2c6VxURoNLk"
      },
      "source": [
        "## Create the test set pairs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVdiDHpoldsD"
      },
      "source": [
        "Let's now do the same thing for the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWMmbbzQU-7n"
      },
      "source": [
        "Run the following cell to:\n",
        "\n",
        "- Extract the **positive examples** for the test set in a Spark DataFrame. That is the pairs of nodes that are linked by a *CO_AUTHOR_LATE* relationship.\n",
        "- Extract the **negative examples** for the test set in a Spark DataFrame. That is the pairs of *Author* nodes in the *LATE* sub graph that are two to three hops away and that did not collaborate together yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrxzqoSuldsD"
      },
      "outputs": [],
      "source": [
        "with driver.session() as session:\n",
        "    result = session.run(\n",
        "        \"\"\"\n",
        "          MATCH (author:Author)-[:CO_AUTHOR_LATE]->(other:Author)\n",
        "          RETURN ID(author) AS node1, ID(other) AS node2, 1 AS label\n",
        "        \"\"\")\n",
        "    test_existing_links = spark.createDataFrame([dict(record) for record in result])\n",
        "    test_existing_links = test_existing_links.dropDuplicates()\n",
        "\n",
        "    result = session.run(\n",
        "        \"\"\"\n",
        "          MATCH (author)-[:CO_AUTHOR_LATE*2..3]->(other)\n",
        "          WHERE NOT((author)-[:CO_AUTHOR_LATE]-(other))\n",
        "          RETURN ID(author) AS node1, ID(other) AS node2, 0 AS label\n",
        "        \"\"\")\n",
        "    test_missing_links = spark.createDataFrame([dict(record) for record in result])    \n",
        "    test_missing_links = test_missing_links.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with driver.session() as session:\n",
        "\n",
        "    result = session.run(\n",
        "        \"\"\"\n",
        "          MATCH (author)-[:CO_AUTHOR_LATE*2..3]->(other)\n",
        "          WHERE NOT((author)-[:CO_AUTHOR_LATE]-(other))\n",
        "          RETURN ID(author) AS node1, ID(other) AS node2, 0 AS label\n",
        "        \"\"\")\n",
        "    test_missing_links = spark.createDataFrame([dict(record) for record in result])    \n",
        "    test_missing_links = test_missing_links.drop_duplicates()"
      ],
      "metadata": {
        "id": "MEDQCi9UgAeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_YWtdLxXHcg"
      },
      "source": [
        "We add the positive pairs to the negative ones in order to create our testing set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Go1LOmlgldsD"
      },
      "outputs": [],
      "source": [
        "testing_df = test_missing_links.union(test_existing_links)\n",
        "print('Observations in test set: ', testing_df.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jcoWTrUYQnK"
      },
      "source": [
        "- Run the following cell to count the number of positive and negative pairs in the test set to evaluate class imbalance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHHI6qPhYb1C"
      },
      "outputs": [],
      "source": [
        "print('Test set class imbalance:')\n",
        "testing_df.groupBy(F.col('label')).count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcF6uqtqdDOY"
      },
      "source": [
        "In the test set, we do not want a balanced dataset because we need to model the real structure of observations on which this link prediction model could be tested.\n",
        "\n",
        "This is why we want a density close to: \n",
        "\n",
        "*(# CO_AUTHOR relationships) / (# pairs of Authors without CO_AUTHOR relationship)*.\n",
        "\n",
        "We know that:\n",
        "\n",
        "*# pairs of Authors without CO_AUTHOR relationship = (# Authors)² - (# CO_AUTHOR relationships) - (# Authors)*.\n",
        "\n",
        "- Run the folowing cell to compute the target density needed in our test set (using the equation above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILYZhkeYhPkD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5648c63-2b2e-45c4-ea4a-c4b29bf27c6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target density in test set:  2.4074343933981664e-05\n"
          ]
        }
      ],
      "source": [
        "with driver.session() as session:\n",
        "\n",
        "    # Compute the number of Authors\n",
        "    result = session.run(\n",
        "        \"\"\"\n",
        "          MATCH (a1:Author)\n",
        "          RETURN COUNT(DISTINCT a1) as nb_authors\n",
        "       \"\"\")\n",
        "    nb_authors = [dict(record) for record in result][0]['nb_authors']\n",
        "\n",
        "    # Compute the number of CO_AUTHOR relationships\n",
        "    result = session.run(\n",
        "        \"\"\"\n",
        "          MATCH (a1:Author)-[r:CO_AUTHOR]-(a2:Author)\n",
        "          RETURN COUNT(DISTINCT r) as nb_rels\n",
        "       \"\"\")\n",
        "    nb_co_author_relationships = [dict(record) for record in result][0]['nb_rels']\n",
        "\n",
        "    # Compute the target density for the test set\n",
        "    nb_negative_pairs = (nb_authors*nb_authors) - nb_co_author_relationships - nb_authors\n",
        "    density = nb_co_author_relationships / nb_negative_pairs\n",
        "    print(\"Target density in test set: \", density)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGKDFswsqSR7"
      },
      "source": [
        "Considering the number of negative pairs that we have extracted, respecting this density would produce only a couple of positive examples in the test set. This is a bit extreme for our future classifier evaluation. \n",
        "\n",
        "This is why we will use **an arbitrary density of 0.01** for the sake of the illustration.\n",
        "\n",
        "- Run the following cell to downsample our positive class for the test set to reflect this density of 0.01."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIjQQfbBXdPW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdd98b95-18a7-4d07-e957-5854844e7c5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set class imbalance after downsampling:\n",
            "+-----+------+\n",
            "|label| count|\n",
            "+-----+------+\n",
            "|    1|   801|\n",
            "|    0|128915|\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "approx_density = 0.01\n",
        "\n",
        "### Isolate and count the negative examples\n",
        "\n",
        "test_df_class_0 = testing_df.filter(F.col('label') == 0)\n",
        "test_count_class_0 = test_df_class_0.count()\n",
        "\n",
        "### Isolate and count the positive examples\n",
        "\n",
        "test_df_class_1 = testing_df.filter(F.col('label') == 1)\n",
        "test_count_class_1 = test_df_class_1.count()\n",
        "\n",
        "### Sample the positive examples\n",
        "test_df_class_1_under = test_df_class_1.sample(withReplacement=False, fraction=approx_density, seed=42) # Note: Spark does not guarantee the fraction to be exactly respected\n",
        "\n",
        "### Add these sample to the negative examples to get a realistic test set\n",
        "df_test_under = test_df_class_1_under.union(test_df_class_0)\n",
        "\n",
        "# Show class imblance now\n",
        "print('Test set class imbalance after downsampling:')\n",
        "df_test_under.groupBy(F.col('label')).count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF8no09gooKF"
      },
      "source": [
        "# Save our train and test pairs DataFrames to CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rXpnmmpldsD"
      },
      "source": [
        "Let's have a look at the contents of our train and test DataFrames before saving them on Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2pq1BXCldsD"
      },
      "outputs": [],
      "source": [
        "df_train_under.filter(F.col('label') == 0).show(5)\n",
        "df_train_under.filter(F.col('label') == 1).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X440h4_NldsD"
      },
      "outputs": [],
      "source": [
        "df_test_under.filter(F.col('label') == 0).show(5)\n",
        "df_test_under.filter(F.col('label') == 1).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koBpOO2ios-F"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdMDyva_ldsD"
      },
      "outputs": [],
      "source": [
        "# Save our DataFrames to CSV files for use in the next notebook\n",
        "\n",
        "save_folder = '/content/gdrive/My Drive/IASD04/IASD_link_prediction/link-prediction/notebooks/data/'\n",
        "\n",
        "df_train_under.write.csv(save_folder + 'df_train_under.csv', mode='overwrite', header=True)\n",
        "df_test_under.write.csv(save_folder + 'df_test_under.csv', mode='overwrite', header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP7xBbvTvv6j"
      },
      "source": [
        "Please check that both datasets have been written to your Drive at the desired location because we are going to need them later for features engineering."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ZoaqhgJQC_fC",
        "JF8no09gooKF"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "py:light,ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}