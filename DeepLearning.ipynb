{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neohack22/IASD/blob/DeepLearning/DeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjADYmSoMmvt"
      },
      "source": [
        "*   Implement a two layers XOR network (one hidden layer)\n",
        "*   Make it learn the XOR function with two inputs and one output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nouvelle section"
      ],
      "metadata": {
        "id": "LamvTX01ebQj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SP80eKUtNe5D"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz8T47CMPOXB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhuKdTwtMjxF"
      },
      "outputs": [],
      "source": [
        "X=np.array([[0.,0.],[0.,1.],[1.,0.],[1.,1.]])\n",
        "y=np.array([[0.],[1.],[1.],[0.]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNoR5XncPZC1"
      },
      "outputs": [],
      "source": [
        "def train(self):\n",
        "  \"\"\"\n",
        "  Train a single layer perceptron.\n",
        "  \"\"\"\n",
        "  correct_counter = 0\n",
        "\n",
        "  for train, target in cycle(zip(self.X, self.y)):\n",
        "    # end if all points are correctly classified\n",
        "    if correct_counter = len(self.train_data):\n",
        "      break\n",
        "\n",
        "    output = self.classify(train)\n",
        "    self.node_val = train\n",
        "\n",
        "    if output == target:\n",
        "      correct_counter += 1\n",
        "    else:\n",
        "      # if incorrectly classified, update and reset correct_counter\n",
        "      self.update_weights(target, output)\n",
        "      correct_counter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX8L1O06SrTb"
      },
      "outputs": [],
      "source": [
        "class Perceptron:\n",
        "  \"\"\"\n",
        "  Create a perceptron.\n",
        "  train_data: a 4x2 matrix with the input data.\n",
        "  target: a 4x1 matrix w/ the perceptron's expected outputs\n",
        "  lr: the learning rate. Defaults to 0.01\n",
        "  input_nodes: the number of nodes in the input layer of the perceptron.\n",
        "  Should be equal to the second dimension of train_data.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, train_data, target, lr=0.01, input_nodes=2):\n",
        "    self.train_data = train_data\n",
        "    self.target = target\n",
        "    self.lr = lr\n",
        "    self.input_nodes = input_nodes\n",
        "\n",
        "    # randomly initialize the weights and set the biais to -1.\n",
        "    self.w = np.random.uniform(size=self.input_nodes)\n",
        "    self.b = -1\n",
        "\n",
        "    # node_val hold the values of each node at a given point of time.\n",
        "    self.node_val = np.zeroes(self.input_nodes)\n",
        "\n",
        "  def _gradient(self, node, exp, output):\n",
        "    \"\"\"\n",
        "    Return the gradient for a weight.\n",
        "    This is the value of delta-w.\n",
        "    \"\"\"\n",
        "    return node * (exp - output)\n",
        "\n",
        "  def update_weights(self, exp, output):\n",
        "    \"\"\"\n",
        "    Update weights and bias based on their respective gradients\n",
        "    \"\"\"\n",
        "    for i in range(self.input_nodes):\n",
        "      self.w[i] += self.lr * self._gradient(self.node_val[i], exp, output)\n",
        "\n",
        "    # the value of the bias node ca be considered as being 1 and the weight\n",
        "    # between this node and the output node being self.b\n",
        "    self.b += self.lr * self._gradient(1, exp, output)\n",
        "\n",
        "  def forward(self, datapoint):\n",
        "    \"\"\"\n",
        "    One forward pass through the perceptron.\n",
        "    Implementation of \"wX + b\".\n",
        "    \"\"\"\n",
        "    return self.b + np.dot(self.w, datapoint)\n",
        "\n",
        "  def classify(self, datapoint):\n",
        "    \"\"\"\n",
        "    Return the class to which a datapoint belongs based on the perceptron's \n",
        "    output for that point.\n",
        "    \"\"\"\n",
        "    if.self.forward(datapoint) >= 0:\n",
        "      return 1\n",
        "\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eu_a872NQXCV"
      },
      "outputs": [],
      "source": [
        "def _gradient(self, node, exp, output):\n",
        "  \"\"\"\n",
        "  Return the gradient for a weight.\n",
        "  This is the value of delta-w.\n",
        "  \"\"\"\n",
        "  return node * (exp - output)\n",
        "\n",
        "def update_weights(self, exp, output):\n",
        "  \"\"\"\n",
        "  Update weights and bias based on their respective gradients\n",
        "  \"\"\"\n",
        "  for i in range(self.input_nodes):\n",
        "    self.w[i] += self.lr * self._gradient(self.node_val[i], exp, output)\n",
        "\n",
        "  # the value of the bias node can be considered as being 1 and the weight\n",
        "  # between this node and the output node being self.b\n",
        "  self.b += self.lr * self._gradient(1, exp, output)\n",
        "\n",
        "def forward(self, datapoint):\n",
        "  \"\"\"\n",
        "  One forward pass throught the perceptron.\n",
        "  Implementation of \"wX + b\".\n",
        "  \"\"\"\n",
        "  return self.b + np.dot(self.w, datapoint)\n",
        "\n",
        "def classify(self, datapoint):\n",
        "  \"\"\"\n",
        "  Return the class to which a datapoint belongs based on\n",
        "  the perceptron's output for that point.\n",
        "  \"\"\"\n",
        "  if self.forward(datapoint) >= 0:\n",
        "    return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHM31dChSbyk"
      },
      "outputs": [],
      "source": [
        "p_xor = Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CHPYYPyWG4L"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkjUGxcXWIRt"
      },
      "outputs": [],
      "source": [
        "# correction\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        " \n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        " \n",
        "\n",
        "import numpy as np\n",
        "X = np.array([[0.,0.],[0.,1.],[1.,0.],[1.,1.]])\n",
        "y = np.array([[0.],[1.],[1.],[0.]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmLke7_OWN3P",
        "outputId": "3effe20a-b067-4e3a-9650-9f55cabe5024"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "4/4 [==============================] - 1s 3ms/step - loss: 0.2935\n",
            "Epoch 2/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2879\n",
            "Epoch 3/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2841\n",
            "Epoch 4/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2812\n",
            "Epoch 5/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2785\n",
            "Epoch 6/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2774\n",
            "Epoch 7/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2764\n",
            "Epoch 8/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2749\n",
            "Epoch 9/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2748\n",
            "Epoch 10/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2741\n",
            "Epoch 11/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2723\n",
            "Epoch 12/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2725\n",
            "Epoch 13/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2720\n",
            "Epoch 14/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2715\n",
            "Epoch 15/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2711\n",
            "Epoch 16/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2705\n",
            "Epoch 17/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2700\n",
            "Epoch 18/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2697\n",
            "Epoch 19/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2692\n",
            "Epoch 20/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2688\n",
            "Epoch 21/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2679\n",
            "Epoch 22/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2678\n",
            "Epoch 23/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2675\n",
            "Epoch 24/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2667\n",
            "Epoch 25/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2669\n",
            "Epoch 26/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2665\n",
            "Epoch 27/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2662\n",
            "Epoch 28/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2657\n",
            "Epoch 29/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2654\n",
            "Epoch 30/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2651\n",
            "Epoch 31/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2643\n",
            "Epoch 32/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2643\n",
            "Epoch 33/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2640\n",
            "Epoch 34/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2633\n",
            "Epoch 35/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2634\n",
            "Epoch 36/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2632\n",
            "Epoch 37/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2627\n",
            "Epoch 38/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2621\n",
            "Epoch 39/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2620\n",
            "Epoch 40/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2613\n",
            "Epoch 41/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2614\n",
            "Epoch 42/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2609\n",
            "Epoch 43/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2605\n",
            "Epoch 44/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2604\n",
            "Epoch 45/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2594\n",
            "Epoch 46/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2596\n",
            "Epoch 47/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2593\n",
            "Epoch 48/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2591\n",
            "Epoch 49/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2581\n",
            "Epoch 50/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2578\n",
            "Epoch 51/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2582\n",
            "Epoch 52/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2577\n",
            "Epoch 53/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2568\n",
            "Epoch 54/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2571\n",
            "Epoch 55/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2562\n",
            "Epoch 56/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2564\n",
            "Epoch 57/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2556\n",
            "Epoch 58/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2558\n",
            "Epoch 59/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2551\n",
            "Epoch 60/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2545\n",
            "Epoch 61/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2546\n",
            "Epoch 62/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2542\n",
            "Epoch 63/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2534\n",
            "Epoch 64/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2534\n",
            "Epoch 65/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2532\n",
            "Epoch 66/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2523\n",
            "Epoch 67/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2524\n",
            "Epoch 68/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2515\n",
            "Epoch 69/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2516\n",
            "Epoch 70/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
            "Epoch 71/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2508\n",
            "Epoch 72/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2502\n",
            "Epoch 73/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2494\n",
            "Epoch 74/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2495\n",
            "Epoch 75/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2490\n",
            "Epoch 76/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2481\n",
            "Epoch 77/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2481\n",
            "Epoch 78/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2475\n",
            "Epoch 79/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2471\n",
            "Epoch 80/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2466\n",
            "Epoch 81/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2460\n",
            "Epoch 82/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2454\n",
            "Epoch 83/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2449\n",
            "Epoch 84/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2440\n",
            "Epoch 85/1000\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.2434\n",
            "Epoch 86/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2432\n",
            "Epoch 87/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2428\n",
            "Epoch 88/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2421\n",
            "Epoch 89/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2416\n",
            "Epoch 90/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2409\n",
            "Epoch 91/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2399\n",
            "Epoch 92/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2397\n",
            "Epoch 93/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2390\n",
            "Epoch 94/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2377\n",
            "Epoch 95/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2376\n",
            "Epoch 96/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2363\n",
            "Epoch 97/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2362\n",
            "Epoch 98/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2353\n",
            "Epoch 99/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2346\n",
            "Epoch 100/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2338\n",
            "Epoch 101/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2325\n",
            "Epoch 102/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2323\n",
            "Epoch 103/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2314\n",
            "Epoch 104/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2299\n",
            "Epoch 105/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2296\n",
            "Epoch 106/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2287\n",
            "Epoch 107/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2273\n",
            "Epoch 108/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2269\n",
            "Epoch 109/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2259\n",
            "Epoch 110/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2250\n",
            "Epoch 111/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2234\n",
            "Epoch 112/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2226\n",
            "Epoch 113/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2215\n",
            "Epoch 114/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2210\n",
            "Epoch 115/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2195\n",
            "Epoch 116/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2187\n",
            "Epoch 117/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2176\n",
            "Epoch 118/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2165\n",
            "Epoch 119/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2153\n",
            "Epoch 120/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2140\n",
            "Epoch 121/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2128\n",
            "Epoch 122/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2115\n",
            "Epoch 123/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2104\n",
            "Epoch 124/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2084\n",
            "Epoch 125/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2076\n",
            "Epoch 126/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2062\n",
            "Epoch 127/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2049\n",
            "Epoch 128/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2029\n",
            "Epoch 129/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2020\n",
            "Epoch 130/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2006\n",
            "Epoch 131/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1987\n",
            "Epoch 132/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1972\n",
            "Epoch 133/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1962\n",
            "Epoch 134/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1942\n",
            "Epoch 135/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1932\n",
            "Epoch 136/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1913\n",
            "Epoch 137/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1900\n",
            "Epoch 138/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1885\n",
            "Epoch 139/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1867\n",
            "Epoch 140/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1846\n",
            "Epoch 141/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1829\n",
            "Epoch 142/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1816\n",
            "Epoch 143/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1799\n",
            "Epoch 144/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1777\n",
            "Epoch 145/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1765\n",
            "Epoch 146/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1748\n",
            "Epoch 147/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1730\n",
            "Epoch 148/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1709\n",
            "Epoch 149/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1697\n",
            "Epoch 150/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1677\n",
            "Epoch 151/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1658\n",
            "Epoch 152/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1641\n",
            "Epoch 153/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1621\n",
            "Epoch 154/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1597\n",
            "Epoch 155/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1586\n",
            "Epoch 156/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1561\n",
            "Epoch 157/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1547\n",
            "Epoch 158/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1525\n",
            "Epoch 159/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1511\n",
            "Epoch 160/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1485\n",
            "Epoch 161/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1468\n",
            "Epoch 162/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1454\n",
            "Epoch 163/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1434\n",
            "Epoch 164/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1417\n",
            "Epoch 165/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1393\n",
            "Epoch 166/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1380\n",
            "Epoch 167/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1361\n",
            "Epoch 168/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1342\n",
            "Epoch 169/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1318\n",
            "Epoch 170/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1304\n",
            "Epoch 171/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1287\n",
            "Epoch 172/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1268\n",
            "Epoch 173/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1249\n",
            "Epoch 174/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1231\n",
            "Epoch 175/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1210\n",
            "Epoch 176/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1196\n",
            "Epoch 177/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1177\n",
            "Epoch 178/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1161\n",
            "Epoch 179/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1139\n",
            "Epoch 180/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1123\n",
            "Epoch 181/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1108\n",
            "Epoch 182/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1092\n",
            "Epoch 183/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1071\n",
            "Epoch 184/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1059\n",
            "Epoch 185/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1039\n",
            "Epoch 186/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1026\n",
            "Epoch 187/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1009\n",
            "Epoch 188/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0990\n",
            "Epoch 189/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0979\n",
            "Epoch 190/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0963\n",
            "Epoch 191/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0948\n",
            "Epoch 192/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0929\n",
            "Epoch 193/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0917\n",
            "Epoch 194/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0903\n",
            "Epoch 195/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0886\n",
            "Epoch 196/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0875\n",
            "Epoch 197/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0861\n",
            "Epoch 198/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0847\n",
            "Epoch 199/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0833\n",
            "Epoch 200/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0820\n",
            "Epoch 201/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0805\n",
            "Epoch 202/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0794\n",
            "Epoch 203/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0779\n",
            "Epoch 204/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0769\n",
            "Epoch 205/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0757\n",
            "Epoch 206/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0745\n",
            "Epoch 207/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0733\n",
            "Epoch 208/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0721\n",
            "Epoch 209/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0710\n",
            "Epoch 210/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0699\n",
            "Epoch 211/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0688\n",
            "Epoch 212/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0677\n",
            "Epoch 213/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0665\n",
            "Epoch 214/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0656\n",
            "Epoch 215/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0645\n",
            "Epoch 216/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0636\n",
            "Epoch 217/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0626\n",
            "Epoch 218/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0617\n",
            "Epoch 219/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0607\n",
            "Epoch 220/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0598\n",
            "Epoch 221/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0588\n",
            "Epoch 222/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0579\n",
            "Epoch 223/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0571\n",
            "Epoch 224/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0562\n",
            "Epoch 225/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0554\n",
            "Epoch 226/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0545\n",
            "Epoch 227/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0537\n",
            "Epoch 228/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0530\n",
            "Epoch 229/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0521\n",
            "Epoch 230/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0514\n",
            "Epoch 231/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0507\n",
            "Epoch 232/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0500\n",
            "Epoch 233/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0493\n",
            "Epoch 234/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0486\n",
            "Epoch 235/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0479\n",
            "Epoch 236/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0472\n",
            "Epoch 237/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0465\n",
            "Epoch 238/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0458\n",
            "Epoch 239/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0453\n",
            "Epoch 240/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0446\n",
            "Epoch 241/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0440\n",
            "Epoch 242/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0434\n",
            "Epoch 243/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0428\n",
            "Epoch 244/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0422\n",
            "Epoch 245/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0417\n",
            "Epoch 246/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0411\n",
            "Epoch 247/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0406\n",
            "Epoch 248/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0401\n",
            "Epoch 249/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0395\n",
            "Epoch 250/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0390\n",
            "Epoch 251/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0385\n",
            "Epoch 252/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0380\n",
            "Epoch 253/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0375\n",
            "Epoch 254/1000\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0371\n",
            "Epoch 255/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0366\n",
            "Epoch 256/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0361\n",
            "Epoch 257/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0357\n",
            "Epoch 258/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0352\n",
            "Epoch 259/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0348\n",
            "Epoch 260/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0343\n",
            "Epoch 261/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0340\n",
            "Epoch 262/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0336\n",
            "Epoch 263/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0331\n",
            "Epoch 264/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0327\n",
            "Epoch 265/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0324\n",
            "Epoch 266/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0320\n",
            "Epoch 267/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0316\n",
            "Epoch 268/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0312\n",
            "Epoch 269/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0309\n",
            "Epoch 270/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0305\n",
            "Epoch 271/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0302\n",
            "Epoch 272/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0298\n",
            "Epoch 273/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0295\n",
            "Epoch 274/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0291\n",
            "Epoch 275/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0288\n",
            "Epoch 276/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0285\n",
            "Epoch 277/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0282\n",
            "Epoch 278/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0279\n",
            "Epoch 279/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0276\n",
            "Epoch 280/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0273\n",
            "Epoch 281/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0270\n",
            "Epoch 282/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0267\n",
            "Epoch 283/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0264\n",
            "Epoch 284/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0262\n",
            "Epoch 285/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0259\n",
            "Epoch 286/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0256\n",
            "Epoch 287/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0254\n",
            "Epoch 288/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0251\n",
            "Epoch 289/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0248\n",
            "Epoch 290/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0246\n",
            "Epoch 291/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0243\n",
            "Epoch 292/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0241\n",
            "Epoch 293/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0238\n",
            "Epoch 294/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0236\n",
            "Epoch 295/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0234\n",
            "Epoch 296/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0232\n",
            "Epoch 297/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0229\n",
            "Epoch 298/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0227\n",
            "Epoch 299/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0225\n",
            "Epoch 300/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0223\n",
            "Epoch 301/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0221\n",
            "Epoch 302/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0219\n",
            "Epoch 303/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0217\n",
            "Epoch 304/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0215\n",
            "Epoch 305/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0213\n",
            "Epoch 306/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0211\n",
            "Epoch 307/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0209\n",
            "Epoch 308/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0207\n",
            "Epoch 309/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0205\n",
            "Epoch 310/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0203\n",
            "Epoch 311/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0201\n",
            "Epoch 312/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0200\n",
            "Epoch 313/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0198\n",
            "Epoch 314/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0196\n",
            "Epoch 315/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0194\n",
            "Epoch 316/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0193\n",
            "Epoch 317/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0191\n",
            "Epoch 318/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0189\n",
            "Epoch 319/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0188\n",
            "Epoch 320/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0186\n",
            "Epoch 321/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0185\n",
            "Epoch 322/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0183\n",
            "Epoch 323/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0182\n",
            "Epoch 324/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0180\n",
            "Epoch 325/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0179\n",
            "Epoch 326/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0177\n",
            "Epoch 327/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0176\n",
            "Epoch 328/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0174\n",
            "Epoch 329/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0173\n",
            "Epoch 330/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0171\n",
            "Epoch 331/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0170\n",
            "Epoch 332/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0169\n",
            "Epoch 333/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0167\n",
            "Epoch 334/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0166\n",
            "Epoch 335/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0165\n",
            "Epoch 336/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0164\n",
            "Epoch 337/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0162\n",
            "Epoch 338/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0161\n",
            "Epoch 339/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0160\n",
            "Epoch 340/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0159\n",
            "Epoch 341/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0157\n",
            "Epoch 342/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0156\n",
            "Epoch 343/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0155\n",
            "Epoch 344/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0154\n",
            "Epoch 345/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0153\n",
            "Epoch 346/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0152\n",
            "Epoch 347/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0151\n",
            "Epoch 348/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0149\n",
            "Epoch 349/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0148\n",
            "Epoch 350/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0147\n",
            "Epoch 351/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0146\n",
            "Epoch 352/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0145\n",
            "Epoch 353/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0144\n",
            "Epoch 354/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0143\n",
            "Epoch 355/1000\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0142\n",
            "Epoch 356/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0141\n",
            "Epoch 357/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0140\n",
            "Epoch 358/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0139\n",
            "Epoch 359/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0138\n",
            "Epoch 360/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0137\n",
            "Epoch 361/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0136\n",
            "Epoch 362/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0135\n",
            "Epoch 363/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0135\n",
            "Epoch 364/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0134\n",
            "Epoch 365/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0133\n",
            "Epoch 366/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0132\n",
            "Epoch 367/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0131\n",
            "Epoch 368/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0130\n",
            "Epoch 369/1000\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0129\n",
            "Epoch 370/1000\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 371/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0128\n",
            "Epoch 372/1000\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 373/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0126\n",
            "Epoch 374/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0125\n",
            "Epoch 375/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0124\n",
            "Epoch 376/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0124\n",
            "Epoch 377/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0123\n",
            "Epoch 378/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0122\n",
            "Epoch 379/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0121\n",
            "Epoch 380/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0120\n",
            "Epoch 381/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0120\n",
            "Epoch 382/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0119\n",
            "Epoch 383/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0118\n",
            "Epoch 384/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0118\n",
            "Epoch 385/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0117\n",
            "Epoch 386/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0116\n",
            "Epoch 387/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0115\n",
            "Epoch 388/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0115\n",
            "Epoch 389/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0114\n",
            "Epoch 390/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0113\n",
            "Epoch 391/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0113\n",
            "Epoch 392/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0112\n",
            "Epoch 393/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0111\n",
            "Epoch 394/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0111\n",
            "Epoch 395/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0110\n",
            "Epoch 396/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0109\n",
            "Epoch 397/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0109\n",
            "Epoch 398/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0108\n",
            "Epoch 399/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0108\n",
            "Epoch 400/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0107\n",
            "Epoch 401/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0106\n",
            "Epoch 402/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0106\n",
            "Epoch 403/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0105\n",
            "Epoch 404/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0105\n",
            "Epoch 405/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0104\n",
            "Epoch 406/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0103\n",
            "Epoch 407/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0103\n",
            "Epoch 408/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0102\n",
            "Epoch 409/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0102\n",
            "Epoch 410/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0101\n",
            "Epoch 411/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0101\n",
            "Epoch 412/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0100\n",
            "Epoch 413/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0099\n",
            "Epoch 414/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0099\n",
            "Epoch 415/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0098\n",
            "Epoch 416/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0098\n",
            "Epoch 417/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0097\n",
            "Epoch 418/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0097\n",
            "Epoch 419/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0096\n",
            "Epoch 420/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0096\n",
            "Epoch 421/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0095\n",
            "Epoch 422/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0095\n",
            "Epoch 423/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0094\n",
            "Epoch 424/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0094\n",
            "Epoch 425/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0093\n",
            "Epoch 426/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0093\n",
            "Epoch 427/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0092\n",
            "Epoch 428/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0092\n",
            "Epoch 429/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0092\n",
            "Epoch 430/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0091\n",
            "Epoch 431/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0091\n",
            "Epoch 432/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0090\n",
            "Epoch 433/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0090\n",
            "Epoch 434/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0089\n",
            "Epoch 435/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0089\n",
            "Epoch 436/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0088\n",
            "Epoch 437/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0088\n",
            "Epoch 438/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0088\n",
            "Epoch 439/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0087\n",
            "Epoch 440/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0087\n",
            "Epoch 441/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0086\n",
            "Epoch 442/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0086\n",
            "Epoch 443/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0085\n",
            "Epoch 444/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0085\n",
            "Epoch 445/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0085\n",
            "Epoch 446/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0084\n",
            "Epoch 447/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0084\n",
            "Epoch 448/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0083\n",
            "Epoch 449/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0083\n",
            "Epoch 450/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0083\n",
            "Epoch 451/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0082\n",
            "Epoch 452/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0082\n",
            "Epoch 453/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0081\n",
            "Epoch 454/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0081\n",
            "Epoch 455/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0081\n",
            "Epoch 456/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0080\n",
            "Epoch 457/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0080\n",
            "Epoch 458/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0080\n",
            "Epoch 459/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0079\n",
            "Epoch 460/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0079\n",
            "Epoch 461/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0079\n",
            "Epoch 462/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0078\n",
            "Epoch 463/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0078\n",
            "Epoch 464/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0078\n",
            "Epoch 465/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0077\n",
            "Epoch 466/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0077\n",
            "Epoch 467/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0076\n",
            "Epoch 468/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0076\n",
            "Epoch 469/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0076\n",
            "Epoch 470/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0075\n",
            "Epoch 471/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0075\n",
            "Epoch 472/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0075\n",
            "Epoch 473/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0075\n",
            "Epoch 474/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0074\n",
            "Epoch 475/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0074\n",
            "Epoch 476/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0074\n",
            "Epoch 477/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0073\n",
            "Epoch 478/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0073\n",
            "Epoch 479/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0073\n",
            "Epoch 480/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0072\n",
            "Epoch 481/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0072\n",
            "Epoch 482/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0072\n",
            "Epoch 483/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0071\n",
            "Epoch 484/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0071\n",
            "Epoch 485/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0071\n",
            "Epoch 486/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0071\n",
            "Epoch 487/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0070\n",
            "Epoch 488/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0070\n",
            "Epoch 489/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0070\n",
            "Epoch 490/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0069\n",
            "Epoch 491/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0069\n",
            "Epoch 492/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0069\n",
            "Epoch 493/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0069\n",
            "Epoch 494/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0068\n",
            "Epoch 495/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0068\n",
            "Epoch 496/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0068\n",
            "Epoch 497/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0067\n",
            "Epoch 498/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0067\n",
            "Epoch 499/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0067\n",
            "Epoch 500/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0067\n",
            "Epoch 501/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0066\n",
            "Epoch 502/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0066\n",
            "Epoch 503/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0066\n",
            "Epoch 504/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0066\n",
            "Epoch 505/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0065\n",
            "Epoch 506/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0065\n",
            "Epoch 507/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0065\n",
            "Epoch 508/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0065\n",
            "Epoch 509/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0064\n",
            "Epoch 510/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0064\n",
            "Epoch 511/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0064\n",
            "Epoch 512/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0064\n",
            "Epoch 513/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0063\n",
            "Epoch 514/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0063\n",
            "Epoch 515/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0063\n",
            "Epoch 516/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0063\n",
            "Epoch 517/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0062\n",
            "Epoch 518/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0062\n",
            "Epoch 519/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0062\n",
            "Epoch 520/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0062\n",
            "Epoch 521/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0062\n",
            "Epoch 522/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0061\n",
            "Epoch 523/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0061\n",
            "Epoch 524/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0061\n",
            "Epoch 525/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0061\n",
            "Epoch 526/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0060\n",
            "Epoch 527/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0060\n",
            "Epoch 528/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0060\n",
            "Epoch 529/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0060\n",
            "Epoch 530/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0060\n",
            "Epoch 531/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0059\n",
            "Epoch 532/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0059\n",
            "Epoch 533/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0059\n",
            "Epoch 534/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0059\n",
            "Epoch 535/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0058\n",
            "Epoch 536/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0058\n",
            "Epoch 537/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0058\n",
            "Epoch 538/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0058\n",
            "Epoch 539/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0058\n",
            "Epoch 540/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0057\n",
            "Epoch 541/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0057\n",
            "Epoch 542/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0057\n",
            "Epoch 543/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0057\n",
            "Epoch 544/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0057\n",
            "Epoch 545/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0056\n",
            "Epoch 546/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0056\n",
            "Epoch 547/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0056\n",
            "Epoch 548/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0056\n",
            "Epoch 549/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0056\n",
            "Epoch 550/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0056\n",
            "Epoch 551/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0055\n",
            "Epoch 552/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0055\n",
            "Epoch 553/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0055\n",
            "Epoch 554/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0055\n",
            "Epoch 555/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0055\n",
            "Epoch 556/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0054\n",
            "Epoch 557/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0054\n",
            "Epoch 558/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0054\n",
            "Epoch 559/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0054\n",
            "Epoch 560/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0054\n",
            "Epoch 561/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0054\n",
            "Epoch 562/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0053\n",
            "Epoch 563/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0053\n",
            "Epoch 564/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0053\n",
            "Epoch 565/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0053\n",
            "Epoch 566/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0053\n",
            "Epoch 567/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0052\n",
            "Epoch 568/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0052\n",
            "Epoch 569/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0052\n",
            "Epoch 570/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0052\n",
            "Epoch 571/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0052\n",
            "Epoch 572/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0052\n",
            "Epoch 573/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0051\n",
            "Epoch 574/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0051\n",
            "Epoch 575/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0051\n",
            "Epoch 576/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0051\n",
            "Epoch 577/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0051\n",
            "Epoch 578/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0051\n",
            "Epoch 579/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0050\n",
            "Epoch 580/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0050\n",
            "Epoch 581/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0050\n",
            "Epoch 582/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0050\n",
            "Epoch 583/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0050\n",
            "Epoch 584/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0050\n",
            "Epoch 585/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0050\n",
            "Epoch 586/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0049\n",
            "Epoch 587/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0049\n",
            "Epoch 588/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0049\n",
            "Epoch 589/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0049\n",
            "Epoch 590/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0049\n",
            "Epoch 591/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0049\n",
            "Epoch 592/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0048\n",
            "Epoch 593/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0048\n",
            "Epoch 594/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0048\n",
            "Epoch 595/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0048\n",
            "Epoch 596/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0048\n",
            "Epoch 597/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0048\n",
            "Epoch 598/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0048\n",
            "Epoch 599/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0047\n",
            "Epoch 600/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0047\n",
            "Epoch 601/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0047\n",
            "Epoch 602/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0047\n",
            "Epoch 603/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0047\n",
            "Epoch 604/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0047\n",
            "Epoch 605/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0047\n",
            "Epoch 606/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0047\n",
            "Epoch 607/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0046\n",
            "Epoch 608/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0046\n",
            "Epoch 609/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0046\n",
            "Epoch 610/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0046\n",
            "Epoch 611/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0046\n",
            "Epoch 612/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0046\n",
            "Epoch 613/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0046\n",
            "Epoch 614/1000\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.0045\n",
            "Epoch 615/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0045\n",
            "Epoch 616/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0045\n",
            "Epoch 617/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0045\n",
            "Epoch 618/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0045\n",
            "Epoch 619/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0045\n",
            "Epoch 620/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0045\n",
            "Epoch 621/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0045\n",
            "Epoch 622/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0044\n",
            "Epoch 623/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0044\n",
            "Epoch 624/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0044\n",
            "Epoch 625/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0044\n",
            "Epoch 626/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0044\n",
            "Epoch 627/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0044\n",
            "Epoch 628/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0044\n",
            "Epoch 629/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0044\n",
            "Epoch 630/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0043\n",
            "Epoch 631/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0043\n",
            "Epoch 632/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0043\n",
            "Epoch 633/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0043\n",
            "Epoch 634/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0043\n",
            "Epoch 635/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0043\n",
            "Epoch 636/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0043\n",
            "Epoch 637/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0043\n",
            "Epoch 638/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0042\n",
            "Epoch 639/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0042\n",
            "Epoch 640/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0042\n",
            "Epoch 641/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0042\n",
            "Epoch 642/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0042\n",
            "Epoch 643/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0042\n",
            "Epoch 644/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0042\n",
            "Epoch 645/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0042\n",
            "Epoch 646/1000\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.0042\n",
            "Epoch 647/1000\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.0041\n",
            "Epoch 648/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0041\n",
            "Epoch 649/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0041\n",
            "Epoch 650/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0041\n",
            "Epoch 651/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0041\n",
            "Epoch 652/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0041\n",
            "Epoch 653/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0041\n",
            "Epoch 654/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0041\n",
            "Epoch 655/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0041\n",
            "Epoch 656/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0040\n",
            "Epoch 657/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0040\n",
            "Epoch 658/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0040\n",
            "Epoch 659/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0040\n",
            "Epoch 660/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0040\n",
            "Epoch 661/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0040\n",
            "Epoch 662/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0040\n",
            "Epoch 663/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0040\n",
            "Epoch 664/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0040\n",
            "Epoch 665/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0040\n",
            "Epoch 666/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0039\n",
            "Epoch 667/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0039\n",
            "Epoch 668/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0039\n",
            "Epoch 669/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0039\n",
            "Epoch 670/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0039\n",
            "Epoch 671/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0039\n",
            "Epoch 672/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0039\n",
            "Epoch 673/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0039\n",
            "Epoch 674/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0039\n",
            "Epoch 675/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0039\n",
            "Epoch 676/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0038\n",
            "Epoch 677/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0038\n",
            "Epoch 678/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0038\n",
            "Epoch 679/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0038\n",
            "Epoch 680/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0038\n",
            "Epoch 681/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0038\n",
            "Epoch 682/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0038\n",
            "Epoch 683/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0038\n",
            "Epoch 684/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0038\n",
            "Epoch 685/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0038\n",
            "Epoch 686/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0038\n",
            "Epoch 687/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 688/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 689/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 690/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 691/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 692/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 693/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 694/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 695/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 696/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 697/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 698/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0036\n",
            "Epoch 699/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 700/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 701/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 702/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 703/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 704/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0036\n",
            "Epoch 705/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 706/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0036\n",
            "Epoch 707/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 708/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 709/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 710/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 711/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 712/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0035\n",
            "Epoch 713/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 714/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 715/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 716/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 717/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 718/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 719/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 720/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 721/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 722/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 723/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 724/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0034\n",
            "Epoch 725/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 726/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 727/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 728/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 729/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 730/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 731/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 732/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 733/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 734/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 735/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0033\n",
            "Epoch 736/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0033\n",
            "Epoch 737/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0033\n",
            "Epoch 738/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0033\n",
            "Epoch 739/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0033\n",
            "Epoch 740/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0033\n",
            "Epoch 741/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0033\n",
            "Epoch 742/1000\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0033\n",
            "Epoch 743/1000\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.0033\n",
            "Epoch 744/1000\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0033\n",
            "Epoch 745/1000\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0033\n",
            "Epoch 746/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0033\n",
            "Epoch 747/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0033\n",
            "Epoch 748/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0033\n",
            "Epoch 749/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0032\n",
            "Epoch 750/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0032\n",
            "Epoch 751/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0032\n",
            "Epoch 752/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0032\n",
            "Epoch 753/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0032\n",
            "Epoch 754/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0032\n",
            "Epoch 755/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0032\n",
            "Epoch 756/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0032\n",
            "Epoch 757/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0032\n",
            "Epoch 758/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0032\n",
            "Epoch 759/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0032\n",
            "Epoch 760/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0032\n",
            "Epoch 761/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0032\n",
            "Epoch 762/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0032\n",
            "Epoch 763/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 764/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 765/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 766/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 767/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 768/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0031\n",
            "Epoch 769/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0031\n",
            "Epoch 770/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 771/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 772/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 773/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 774/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 775/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 776/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0031\n",
            "Epoch 777/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0031\n",
            "Epoch 778/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 779/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0030\n",
            "Epoch 780/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 781/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0030\n",
            "Epoch 782/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0030\n",
            "Epoch 783/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0030\n",
            "Epoch 784/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 785/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 786/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 787/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 788/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 789/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 790/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 791/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 792/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 793/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 794/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 795/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 796/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0029\n",
            "Epoch 797/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 798/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0029\n",
            "Epoch 799/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0029\n",
            "Epoch 800/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 801/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0029\n",
            "Epoch 802/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0029\n",
            "Epoch 803/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0029\n",
            "Epoch 804/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0029\n",
            "Epoch 805/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 806/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 807/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0029\n",
            "Epoch 808/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0029\n",
            "Epoch 809/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 810/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0029\n",
            "Epoch 811/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 812/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 813/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 814/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 815/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0028\n",
            "Epoch 816/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 817/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 818/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0028\n",
            "Epoch 819/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 820/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 821/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 822/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 823/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 824/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 825/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 826/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0028\n",
            "Epoch 827/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0028\n",
            "Epoch 828/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0028\n",
            "Epoch 829/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 830/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0028\n",
            "Epoch 831/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 832/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0027\n",
            "Epoch 833/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 834/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 835/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0027\n",
            "Epoch 836/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 837/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 838/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 839/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 840/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 841/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 842/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 843/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0027\n",
            "Epoch 844/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 845/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 846/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 847/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0027\n",
            "Epoch 848/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0027\n",
            "Epoch 849/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0027\n",
            "Epoch 850/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0027\n",
            "Epoch 851/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0026\n",
            "Epoch 852/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 853/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0026\n",
            "Epoch 854/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0026\n",
            "Epoch 855/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 856/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0026\n",
            "Epoch 857/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 858/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 859/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 860/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 861/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0026\n",
            "Epoch 862/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 863/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0026\n",
            "Epoch 864/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 865/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 866/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 867/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 868/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 869/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 870/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0026\n",
            "Epoch 871/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0026\n",
            "Epoch 872/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 873/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0025\n",
            "Epoch 874/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0025\n",
            "Epoch 875/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 876/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 877/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0025\n",
            "Epoch 878/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 879/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 880/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0025\n",
            "Epoch 881/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0025\n",
            "Epoch 882/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0025\n",
            "Epoch 883/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 884/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 885/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0025\n",
            "Epoch 886/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0025\n",
            "Epoch 887/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 888/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 889/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 890/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0025\n",
            "Epoch 891/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 892/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 893/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 894/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 895/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 896/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 897/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0024\n",
            "Epoch 898/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 899/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 900/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 901/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 902/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 903/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 904/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0024\n",
            "Epoch 905/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 906/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0024\n",
            "Epoch 907/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0024\n",
            "Epoch 908/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0024\n",
            "Epoch 909/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 910/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 911/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0024\n",
            "Epoch 912/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 913/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0024\n",
            "Epoch 914/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 915/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 916/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 917/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 918/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0024\n",
            "Epoch 919/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0024\n",
            "Epoch 920/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 921/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 922/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 923/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 924/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 925/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 926/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 927/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 928/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0023\n",
            "Epoch 929/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 930/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 931/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 932/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 933/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 934/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 935/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 936/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 937/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 938/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 939/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 940/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 941/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0023\n",
            "Epoch 942/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 943/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 944/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 945/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 946/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 947/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 948/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 949/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 950/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 951/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 952/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 953/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 954/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 955/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 956/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 957/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 958/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 959/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 960/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0022\n",
            "Epoch 961/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 962/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0022\n",
            "Epoch 963/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 964/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0022\n",
            "Epoch 965/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0022\n",
            "Epoch 966/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 967/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 968/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0022\n",
            "Epoch 969/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 970/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 971/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 972/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 973/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 974/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 975/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 976/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 977/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0021\n",
            "Epoch 978/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 979/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0021\n",
            "Epoch 980/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0021\n",
            "Epoch 981/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 982/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 983/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 984/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 985/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 986/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 987/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 988/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 989/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 990/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 991/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 992/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 993/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 994/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 995/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 996/1000\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.0021\n",
            "Epoch 997/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0021\n",
            "Epoch 998/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 999/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 1000/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "[[0.03149492]\n",
            " [0.9497347 ]\n",
            " [0.95497674]\n",
            " [0.0521006 ]]\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(8, input_dim=2))\n",
        "model.add(Activation('tanh'))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        " \n",
        "\n",
        "sgd = SGD(lr=0.1)\n",
        "model.compile(loss='mse', optimizer=sgd)\n",
        "model.fit(X, y, verbose=1, batch_size=1, epochs=1000)\n",
        "print(model.predict(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g14bWupgY4v5"
      },
      "source": [
        "MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tun8B397huRx"
      },
      "source": [
        "Train a small dense network on the MNIST data:\n",
        "\n",
        "\n",
        "*   Prepare the data:\n",
        "  *   Inputs = vectors of real numbers (between 0.0 and 1.0 of size 28*28\n",
        "  *   Labels = vectors of real numbers of size 10 (nine 0 and a 1 at the index of the label)\n",
        "*   Define the network:\n",
        "  * Fully connected network with 28*28 inputs and 10 outputs\n",
        "* Define the loss and the optimizer\n",
        "* Train the network\n",
        "* Test the network\n",
        "* Print an image in the test set and the predicted class\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVzRPrPFY5tD",
        "outputId": "8e4f3bd1-05ea-4b03-92cc-a6b9b8085866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PN4B5aN0Zb_M"
      },
      "outputs": [],
      "source": [
        "# modifie les dimensions en gardant le même nombre d'éléments\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "# va transformer des entiers en des vecteurs avec des 0 et 1 aux bons endroits\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjZ5gL5IjQa0"
      },
      "outputs": [],
      "source": [
        "#ma version fausse\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_dim=728))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "sgd = SGD(lr=0.1)\n",
        "model.compile(loss='mse', optimizer=sgd)\n",
        "model.fit(X, y, verbose=1, batch_size=1, epochs=1000)\n",
        "print(model.predict(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpkIWg4Rkk0f"
      },
      "outputs": [],
      "source": [
        "# correction\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "\n",
        "network = Sequential()\n",
        "network.add(Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "network.add(Dense(10, activation='softmax')) # somme des activations va faire 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIKWLiY_kzM7"
      },
      "outputs": [],
      "source": [
        "# Defining the optimizer and the loss:\n",
        "network.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "metrics=['accuracy']) # % d'exemple qui vont être bien classés"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x-cg10glRwn",
        "outputId": "5340f21c-2a89-48c5-cbff-edf846ce48e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 6s 11ms/step - loss: 0.2553 - accuracy: 0.9252\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1032 - accuracy: 0.9693\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 5s 12ms/step - loss: 0.0681 - accuracy: 0.9796\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0496 - accuracy: 0.9854\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0368 - accuracy: 0.9891\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f193f107ed0>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Training the network\n",
        "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKpp8L1OlStU",
        "outputId": "3adac64f-6755-49c2-f726-1745022a81a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0658 - accuracy: 0.9795\n",
            "test_acc: 0.9794999957084656\n"
          ]
        }
      ],
      "source": [
        "# Testing the network\n",
        "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
        "print('test_acc:', test_acc) # nombre d'images bien classées"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "uwz_n0HTlcZA",
        "outputId": "102c6647-7273-442e-ff15-8ebcef02dc1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[2.1075907e-08 4.3379483e-10 6.0874561e-07 5.5157543e-06 7.3060984e-12\n",
            "  2.7074177e-07 1.0774476e-13 9.9999177e-01 4.7164978e-08 1.8958870e-06]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Predicting the class of an example\n",
        "import matplotlib.pyplot as plt\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "plt.imshow(test_images [0])\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "img = test_images [0].reshape ((1, 28*28))\n",
        "print (network.predict(img))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnMhApKppNwc"
      },
      "source": [
        "# Exercide: preparing the data\n",
        "You can't feed lists of integers into a neural network.\n",
        "You have to turn your lists into tensors:\n",
        "\n",
        "\n",
        "*   One-hot encode your lists to turn them into vectors of 0s and 1s.\n",
        "*   This would mean, for instance, turning the sequence [3, 5] into a 10,000-dimensional vector that would be all 0s except for indices 3 and 5, hich would be 1s.\n",
        "*   Then you could use as the first layer in your network a dense layer, capable of hedling floatingpoint vector data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqAwwJIkqFc6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7lGxJi-OqqP",
        "outputId": "254de697-f2c9-4704-f0ae-1574a7fad6b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv9EykoLq0zZ"
      },
      "outputs": [],
      "source": [
        "# One-hot encode your lists to turn them into vectors of 0s and 1s.\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "train_data = to_categorical(train_data)\n",
        "test_data = to_categorical(test_data)\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "Cf4xB6fnO6CQ",
        "outputId": "7b4b0ca4-42ac-4bc4-bc88-1142fe0f2c97"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f7d501b39777>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \"\"\"\n\u001b[0;32m---> 62\u001b[0;31m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m   \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ],
      "source": [
        "#from tensorflow.python.keras.utils.np_utils import to_categorical\n",
        "# One-hot encode your lists to turn them into vectors of 0s and 1s.\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "train_data = to_categorical(train_data)\n",
        "test_data = to_categorical(test_data)\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86P7fEDmuCFE"
      },
      "outputs": [],
      "source": [
        "# correction\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i in range (len (sequences)):\n",
        "        for j in range (len (sequences [i])):\n",
        "            results [i] [sequences [i] [j]] = 1.\n",
        "    return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "# You should also convert your labels from integer to numeric, which is straightforward:\n",
        "\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h3E2fwZPzUB"
      },
      "outputs": [],
      "source": [
        "# correction\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "#(train_data, train_labels), (test_data, test_labels) imdb.load_data(\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(    \n",
        "    num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  #results = np.zeros(len(sequences), dimension))\n",
        "  #results = np.zeros(len(sequences), dimension))\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i in range (len (sequences)):\n",
        "    #for j in range (len (sequences), dimension)):\n",
        "    for j in range (len (sequences [i])):\n",
        "      results [i] [sequences [i] [j]] = 1.\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "# You should also convert your labels from integer to numeric, which is straightforward:\n",
        "\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "#y_test = np.asarray(test_labels).astype('float')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMUxyZryrA49"
      },
      "outputs": [],
      "source": [
        "# proposition\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "\n",
        "network = Sequential()\n",
        "network.add(Dense(512, activation='relu', input_shape=(10000,)))\n",
        "network.add(Dense(2, activation='softmax')) # somme des activations va faire 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzJ1Nf4sS4uZ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "\n",
        "network = Sequential()\n",
        "network.add(Dense(512, activation='relu', input_shape=(10000,)))\n",
        "network.add(Dense(2, activation='softmax')) # somme des activations va faire 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDC9ebs0s4Ki"
      },
      "outputs": [],
      "source": [
        "# Defining the optimizer and the loss:\n",
        "network.compile(\n",
        "    optimizer='rmsprop', loss='BinaryCrossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDTJy4UgShAp"
      },
      "outputs": [],
      "source": [
        "# Defining the optimizer and the loss:\n",
        "network.compile(\n",
        "    optimizer='rmsprop', loss='BinaryCrossentropy', metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbwlhL4RtUOa",
        "outputId": "5340f21c-2a89-48c5-cbff-edf846ce48e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 6s 11ms/step - loss: 0.2553 - accuracy: 0.9252\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1032 - accuracy: 0.9693\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 5s 12ms/step - loss: 0.0681 - accuracy: 0.9796\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0496 - accuracy: 0.9854\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0368 - accuracy: 0.9891\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f193f107ed0>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Training the network\n",
        "network.fit(train_data, train_labels, epochs=5, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "id": "taLkLLSHTVeZ",
        "outputId": "8386d06c-1581-4a2b-cdfb-b3ebb640017a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-88c58cd9deac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#network.fit(train_data, train_labels, epochs=5, batch_size=128)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 810, in train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 1807, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 5158, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 2) vs (None, 1)).\n"
          ]
        }
      ],
      "source": [
        "# Training the network\n",
        "#network.fit(train_data, train_labels, epochs=5, batch_size=128)\n",
        "network.fit(x_train, y_train, epochs=5, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRwy_F-hsWd8"
      },
      "outputs": [],
      "source": [
        "# Testing the network\n",
        "test_loss, test_acc = network.evaluate(test_data, test_labels)\n",
        "print('test_acc:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UALQ5PHuq_AC"
      },
      "outputs": [],
      "source": [
        "# Predicting the class of an example\n",
        "import matplotlib.pyplot as plt\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "plt.imshow(test_images [0])\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "img = test_images [0].reshape ((1, 28*28))\n",
        "print (network.predict(img))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWmwZTO7VV5p"
      },
      "source": [
        "IMDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnAYvhyeVVAN"
      },
      "outputs": [],
      "source": [
        "# Defining the network:\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model= Sequential()\n",
        "model.add(Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8yt4dDpXpp3"
      },
      "outputs": [],
      "source": [
        "# Definig a validation set:\n",
        "\n",
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "\n",
        "y_val = y_train[:1000]\n",
        "partial_y_train = y_train[10000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w-0HJDXX16v"
      },
      "outputs": [],
      "source": [
        "# Trainign with a validation set:\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "history= model.fit(\n",
        "    partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(\n",
        "        x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDAmkEUoYU_2"
      },
      "outputs": [],
      "source": [
        "# visualizethe training loss:\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHGFvFAwZccu"
      },
      "source": [
        "la loss est le carré des poids des poids des paramètres : quand on fait le gradient, on enlève le ...?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STaMFyP-cu7R"
      },
      "source": [
        "Weight Rgularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "bLYwbxZnat5i",
        "outputId": "1acb295f-1ab1-4561-e720-30951da4075e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-65f7fb15069f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# every coeff in the weight matrix of the layer will add .001 * weight_coefficient_value to the total loss of the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model.add(layers.Dense(\n\u001b[0m\u001b[1;32m      6\u001b[0m     16, kernel_regularizer=regularizers.l2(\n\u001b[1;32m      7\u001b[0m         0.001), activation='relu', input_shape=(10000,)))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import regularizers\n",
        "#model = Sequetial()\n",
        "model = Sequential()\n",
        "# every coeff in the weight matrix of the layer will add .001 * weight_coefficient_value to the total loss of the network\n",
        "model.add(layers.Dense(\n",
        "    16, kernel_regularizer=regularizers.l2(\n",
        "        0.001), activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN_coU_9eB-4"
      },
      "outputs": [],
      "source": [
        "#, regularizers\n",
        "#from tensorflowkeras import regularizers\n",
        "from tensorflow.keras import regularizers\n",
        "from keras import layers\n",
        "model = Sequential()\n",
        "# every coeff in the weight matrix of the layer will add  * weight_coefficient_value to the total loss of the network\n",
        "model.add(layers.Dense(\n",
        "    #16, kernel_regularizer=regulari*.l2(\n",
        "    16, kernel_regularizer=regularizers.l2(\n",
        "        0.001), activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(1, activation='sigmoid'\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7LmhIuNm4es"
      },
      "outputs": [],
      "source": [
        "dropout_m = Sequential()\n",
        "dropout_m.add(layers.Dense)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpxIHKqCnr4g"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16,activation='relu',\n",
        "                       input_shape=(10000,)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K5qI48ZploR"
      },
      "source": [
        "Multiclass Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSCHt32Opnbo",
        "outputId": "ca7ea8da-0723-476f-cf38-d52d8aeded2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n",
            "2121728/2110848 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import reuters\n",
        "(train_data, train_labels), (\n",
        "    test_data, test_labels) = reuters.load_data(num_words=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR2ftlhTr0x2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.datasets import imdb\n",
        "#(train_data, train_labels), (test_data, test_labels) imdb.load_data(\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(    \n",
        "    num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  #results = np.zeros(len(sequences), dimension))\n",
        "  #results = np.zeros(len(sequences), dimension))\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i in range (len (sequences)):\n",
        "    #for j in range (len (sequences), dimension)):\n",
        "    for j in range (len (sequences [i])):\n",
        "      results [i] [sequences [i] [j]] = 1.\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "# You should also convert your labels from integer to numeric, which is straightforward:\n",
        "\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "#y_test = np.asarray(test_labels).astype('float')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_59Vfq3tger"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SckkBZDVr5Cy"
      },
      "outputs": [],
      "source": [
        "# encode the data\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i in range (len (sequences)):\n",
        "    for j in range (len (sequences [i])):\n",
        "      results [i] [sequences [i] [j]] = 1.\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorier_sequences(test_data)  \n",
        "\n",
        "# You should also convert your labels from integer to numeric, which is straightforward:\n",
        "\n",
        "y_train = npn.asarray(train_labels).astype('float32')\n",
        "y_test = np.array(test_labels).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4qhvXraqyLB"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from keras import layers\n",
        "model = Sequential()\n",
        "# every coeff in the weight matrix of the layer will add  * weight_coefficient_value to the total loss of the network\n",
        "model.add(layers.Dense(\n",
        "    16, kernel_regularizer=regularizers.l2(\n",
        "        0.001), activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(1, activation='sigmoid'\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuAHmc9vOfN3"
      },
      "source": [
        "Reuters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njaa9HirOjPu"
      },
      "source": [
        "Encoding the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYkbvamcPBoE"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import reuters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTH37n4KPLYm"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "#from tensorflow.keras.datasets import imdb\n",
        "#(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\n",
        "    num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i in range (len (sequences)):\n",
        "        for j in range (len (sequences [i])):\n",
        "            results [i] [sequences [i] [j]] = 1.\n",
        "    return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "# You should also convert your labels from integer to numeric, which is straightforward:\n",
        "\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgJqlEAhOdsi"
      },
      "outputs": [],
      "source": [
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "#one_hot_train_labels = tp_categorical(train_labels)\n",
        "#onr_hot_train_labels= to_categorical(train_labels)\n",
        "one_hot_train_labels= to_categorical(train_labels)\n",
        "one_hit_test_labels = to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgRx3mwkPCs2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjLlriA5RBBX",
        "outputId": "510ed2fd-849a-4360-f86d-7b6a4c850f60"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(8, input_dim=2))\n",
        "model.add(Activation('tanh'))\n",
        "model.add(Dense(1))\n",
        "#model.add(Activation('sigmoid'))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        " \n",
        "# compile model\n",
        "sgd = SGD(lr=0.1)\n",
        "#model.compile(loss='mse', optimizer=sgd)\n",
        "model.compile(loss='categorical_crossentropy' ,optimizer=sgd)#opt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf9yAXLDRJ_e"
      },
      "outputs": [],
      "source": [
        "# Define a validation set:\n",
        "\n",
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "\n",
        "y_val = y_train[:1000]\n",
        "partial_y_train = y_train[10000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "Apy19OUgRQYv",
        "outputId": "612a7aa9-a624-4e73-9949-639d7779826f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-5f183a7c2736>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#model.fit(X, y, verbose=1, batch_size=1, epochs=1000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial_x_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial_x_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m           raise ValueError('Unexpected result of `train_function` '\n\u001b[0m\u001b[1;32m   1228\u001b[0m                            \u001b[0;34m'(Empty logs). Please use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m                            \u001b[0;34m'`Model.compile(..., run_eagerly=True)`, or '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
          ]
        }
      ],
      "source": [
        "# Train the network using a validation set\n",
        "\n",
        "#model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "#history= model.fit(\n",
        "    #partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(\n",
        "        #x_val, y_val))\n",
        "\n",
        "#model.fit(X, y, verbose=1, batch_size=1, epochs=1000)\n",
        "model.fit(partial_x_train, partial_y_train, verbose=1, batch_size=1, epochs=1000)\n",
        "print(model.predict(partial_x_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfG193zaRTLH"
      },
      "outputs": [],
      "source": [
        "# visualizethe training loss:\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgJ3og-zT-P5"
      },
      "outputs": [],
      "source": [
        "# correction possible\n",
        "\n",
        "# defining the network\n",
        "\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCC2vvmMU6BU"
      },
      "outputs": [],
      "source": [
        "# defining the optimizer and the loss:\n",
        "\n",
        "model.compile(\n",
        "    optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K09-yutGVdmd"
      },
      "outputs": [],
      "source": [
        "# Defining a validation set:\n",
        "\n",
        "x_val= x_train[:1000]\n",
        "partial_x_train= x_train[1000:]\n",
        "y_val= one_hot_train_labels[:1000]\n",
        "partial_y_train= one_hot_train_labels[1000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFzBqQ2nV2OA",
        "outputId": "a5f92935-daf6-4629-8256-80f1b7996288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.6678 - accuracy: 0.6815\n",
            "Epoch 2/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.5881 - accuracy: 0.7375\n",
            "Epoch 3/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.5334 - accuracy: 0.7660\n",
            "Epoch 4/1000\n",
            "7982/7982 [==============================] - 71s 9ms/step - loss: 1.5094 - accuracy: 0.7898\n",
            "Epoch 5/1000\n",
            "7982/7982 [==============================] - 70s 9ms/step - loss: 1.4703 - accuracy: 0.7982\n",
            "Epoch 6/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.4526 - accuracy: 0.8038\n",
            "Epoch 7/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.4468 - accuracy: 0.8098\n",
            "Epoch 8/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.4512 - accuracy: 0.8086\n",
            "Epoch 9/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.4479 - accuracy: 0.8096\n",
            "Epoch 10/1000\n",
            "7982/7982 [==============================] - 60s 8ms/step - loss: 1.4408 - accuracy: 0.8111\n",
            "Epoch 11/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.4497 - accuracy: 0.8172\n",
            "Epoch 12/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.4202 - accuracy: 0.8173\n",
            "Epoch 13/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.4448 - accuracy: 0.8193\n",
            "Epoch 14/1000\n",
            "7982/7982 [==============================] - 62s 8ms/step - loss: 1.4228 - accuracy: 0.8244\n",
            "Epoch 15/1000\n",
            "7982/7982 [==============================] - 59s 7ms/step - loss: 1.4470 - accuracy: 0.8269\n",
            "Epoch 16/1000\n",
            "7982/7982 [==============================] - 60s 7ms/step - loss: 1.4323 - accuracy: 0.8289\n",
            "Epoch 17/1000\n",
            "7982/7982 [==============================] - 62s 8ms/step - loss: 1.4250 - accuracy: 0.8289\n",
            "Epoch 18/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.4155 - accuracy: 0.8242\n",
            "Epoch 19/1000\n",
            "7982/7982 [==============================] - 62s 8ms/step - loss: 1.4229 - accuracy: 0.8311\n",
            "Epoch 20/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.3867 - accuracy: 0.8335\n",
            "Epoch 21/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.3856 - accuracy: 0.8348\n",
            "Epoch 22/1000\n",
            "7982/7982 [==============================] - 62s 8ms/step - loss: 1.3793 - accuracy: 0.8385\n",
            "Epoch 23/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.3515 - accuracy: 0.8365\n",
            "Epoch 24/1000\n",
            "7982/7982 [==============================] - 63s 8ms/step - loss: 1.3124 - accuracy: 0.8385\n",
            "Epoch 25/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.2843 - accuracy: 0.8400\n",
            "Epoch 26/1000\n",
            "7982/7982 [==============================] - 68s 9ms/step - loss: 1.3447 - accuracy: 0.8413\n",
            "Epoch 27/1000\n",
            "7982/7982 [==============================] - 69s 9ms/step - loss: 1.2927 - accuracy: 0.8424\n",
            "Epoch 28/1000\n",
            "7982/7982 [==============================] - 68s 9ms/step - loss: 1.3297 - accuracy: 0.8425\n",
            "Epoch 29/1000\n",
            "7982/7982 [==============================] - 70s 9ms/step - loss: 1.3169 - accuracy: 0.8450\n",
            "Epoch 30/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.3603 - accuracy: 0.8411\n",
            "Epoch 31/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.2972 - accuracy: 0.8454\n",
            "Epoch 32/1000\n",
            "7982/7982 [==============================] - 68s 8ms/step - loss: 1.3031 - accuracy: 0.8448\n",
            "Epoch 33/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.3259 - accuracy: 0.8463\n",
            "Epoch 34/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.2312 - accuracy: 0.8441\n",
            "Epoch 35/1000\n",
            "7982/7982 [==============================] - 63s 8ms/step - loss: 1.2598 - accuracy: 0.8475\n",
            "Epoch 36/1000\n",
            "7982/7982 [==============================] - 63s 8ms/step - loss: 1.2835 - accuracy: 0.8480\n",
            "Epoch 37/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.2502 - accuracy: 0.8475\n",
            "Epoch 38/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.2373 - accuracy: 0.8479\n",
            "Epoch 39/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.2192 - accuracy: 0.8478\n",
            "Epoch 40/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1804 - accuracy: 0.8494\n",
            "Epoch 41/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.2202 - accuracy: 0.8484\n",
            "Epoch 42/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.1994 - accuracy: 0.8509\n",
            "Epoch 43/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.2210 - accuracy: 0.8492\n",
            "Epoch 44/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1872 - accuracy: 0.8505\n",
            "Epoch 45/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.2191 - accuracy: 0.8519\n",
            "Epoch 46/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.2330 - accuracy: 0.8530\n",
            "Epoch 47/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.1788 - accuracy: 0.8542\n",
            "Epoch 48/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1764 - accuracy: 0.8520\n",
            "Epoch 49/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1347 - accuracy: 0.8535\n",
            "Epoch 50/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1581 - accuracy: 0.8527\n",
            "Epoch 51/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1447 - accuracy: 0.8542\n",
            "Epoch 52/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1990 - accuracy: 0.8537\n",
            "Epoch 53/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1437 - accuracy: 0.8555\n",
            "Epoch 54/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1457 - accuracy: 0.8545\n",
            "Epoch 55/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1650 - accuracy: 0.8542\n",
            "Epoch 56/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1450 - accuracy: 0.8572\n",
            "Epoch 57/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1598 - accuracy: 0.8525\n",
            "Epoch 58/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1686 - accuracy: 0.8525\n",
            "Epoch 59/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1958 - accuracy: 0.8566\n",
            "Epoch 60/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1726 - accuracy: 0.8550\n",
            "Epoch 61/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1329 - accuracy: 0.8549\n",
            "Epoch 62/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1207 - accuracy: 0.8566\n",
            "Epoch 63/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1247 - accuracy: 0.8530\n",
            "Epoch 64/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.1459 - accuracy: 0.8555\n",
            "Epoch 65/1000\n",
            "7982/7982 [==============================] - 74s 9ms/step - loss: 1.1432 - accuracy: 0.8555\n",
            "Epoch 66/1000\n",
            "7982/7982 [==============================] - 69s 9ms/step - loss: 1.1642 - accuracy: 0.8563\n",
            "Epoch 67/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.1704 - accuracy: 0.8569\n",
            "Epoch 68/1000\n",
            "7982/7982 [==============================] - 70s 9ms/step - loss: 1.1207 - accuracy: 0.8572\n",
            "Epoch 69/1000\n",
            "7982/7982 [==============================] - 81s 10ms/step - loss: 1.1446 - accuracy: 0.8542\n",
            "Epoch 70/1000\n",
            "7982/7982 [==============================] - 76s 10ms/step - loss: 1.1387 - accuracy: 0.8592\n",
            "Epoch 71/1000\n",
            "7982/7982 [==============================] - 74s 9ms/step - loss: 1.1493 - accuracy: 0.8566\n",
            "Epoch 72/1000\n",
            "7982/7982 [==============================] - 70s 9ms/step - loss: 1.0907 - accuracy: 0.8571\n",
            "Epoch 73/1000\n",
            "7982/7982 [==============================] - 73s 9ms/step - loss: 1.1221 - accuracy: 0.8557\n",
            "Epoch 74/1000\n",
            "7982/7982 [==============================] - 70s 9ms/step - loss: 1.1103 - accuracy: 0.8555\n",
            "Epoch 75/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.0901 - accuracy: 0.8561\n",
            "Epoch 76/1000\n",
            "7982/7982 [==============================] - 76s 9ms/step - loss: 1.0762 - accuracy: 0.8591\n",
            "Epoch 77/1000\n",
            "7982/7982 [==============================] - 74s 9ms/step - loss: 1.1356 - accuracy: 0.8567\n",
            "Epoch 78/1000\n",
            "7982/7982 [==============================] - 68s 9ms/step - loss: 1.0985 - accuracy: 0.8578\n",
            "Epoch 79/1000\n",
            "7982/7982 [==============================] - 72s 9ms/step - loss: 1.1163 - accuracy: 0.8567\n",
            "Epoch 80/1000\n",
            "7982/7982 [==============================] - 70s 9ms/step - loss: 1.0773 - accuracy: 0.8592\n",
            "Epoch 81/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.1329 - accuracy: 0.8587\n",
            "Epoch 82/1000\n",
            "7982/7982 [==============================] - 69s 9ms/step - loss: 1.0984 - accuracy: 0.8561\n",
            "Epoch 83/1000\n",
            "7982/7982 [==============================] - 71s 9ms/step - loss: 1.0831 - accuracy: 0.8586\n",
            "Epoch 84/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1021 - accuracy: 0.8579\n",
            "Epoch 85/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.1251 - accuracy: 0.8564\n",
            "Epoch 86/1000\n",
            "7982/7982 [==============================] - 68s 8ms/step - loss: 1.0904 - accuracy: 0.8588\n",
            "Epoch 87/1000\n",
            "7982/7982 [==============================] - 68s 9ms/step - loss: 1.0901 - accuracy: 0.8596\n",
            "Epoch 88/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1555 - accuracy: 0.8586\n",
            "Epoch 89/1000\n",
            "7982/7982 [==============================] - 71s 9ms/step - loss: 1.0795 - accuracy: 0.8597\n",
            "Epoch 90/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.1052 - accuracy: 0.8574\n",
            "Epoch 91/1000\n",
            "7982/7982 [==============================] - 61s 8ms/step - loss: 1.0834 - accuracy: 0.8611\n",
            "Epoch 92/1000\n",
            "7982/7982 [==============================] - 63s 8ms/step - loss: 1.0576 - accuracy: 0.8587\n",
            "Epoch 93/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.0487 - accuracy: 0.8586\n",
            "Epoch 94/1000\n",
            "7982/7982 [==============================] - 70s 9ms/step - loss: 1.0983 - accuracy: 0.8618\n",
            "Epoch 95/1000\n",
            "7982/7982 [==============================] - 72s 9ms/step - loss: 1.1091 - accuracy: 0.8573\n",
            "Epoch 96/1000\n",
            "7982/7982 [==============================] - 74s 9ms/step - loss: 1.1219 - accuracy: 0.8612\n",
            "Epoch 97/1000\n",
            "7982/7982 [==============================] - 73s 9ms/step - loss: 1.0972 - accuracy: 0.8577\n",
            "Epoch 98/1000\n",
            "7982/7982 [==============================] - 76s 10ms/step - loss: 1.1075 - accuracy: 0.8587\n",
            "Epoch 99/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.0698 - accuracy: 0.8603\n",
            "Epoch 100/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.0834 - accuracy: 0.8596\n",
            "Epoch 101/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.0549 - accuracy: 0.8593\n",
            "Epoch 102/1000\n",
            "7982/7982 [==============================] - 70s 9ms/step - loss: 1.0779 - accuracy: 0.8602\n",
            "Epoch 103/1000\n",
            "7982/7982 [==============================] - 71s 9ms/step - loss: 1.0572 - accuracy: 0.8626\n",
            "Epoch 104/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.0641 - accuracy: 0.8621\n",
            "Epoch 105/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.0615 - accuracy: 0.8611\n",
            "Epoch 106/1000\n",
            "7982/7982 [==============================] - 69s 9ms/step - loss: 1.0339 - accuracy: 0.8626\n",
            "Epoch 107/1000\n",
            "7982/7982 [==============================] - 74s 9ms/step - loss: 1.0249 - accuracy: 0.8612\n",
            "Epoch 108/1000\n",
            "7982/7982 [==============================] - 70s 9ms/step - loss: 1.0683 - accuracy: 0.8608\n",
            "Epoch 109/1000\n",
            "7982/7982 [==============================] - 69s 9ms/step - loss: 1.0577 - accuracy: 0.8584\n",
            "Epoch 110/1000\n",
            "7982/7982 [==============================] - 68s 8ms/step - loss: 1.0245 - accuracy: 0.8601\n",
            "Epoch 111/1000\n",
            "7982/7982 [==============================] - 72s 9ms/step - loss: 1.0215 - accuracy: 0.8607\n",
            "Epoch 112/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.0810 - accuracy: 0.8607\n",
            "Epoch 113/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.0833 - accuracy: 0.8578\n",
            "Epoch 114/1000\n",
            "7982/7982 [==============================] - 70s 9ms/step - loss: 1.0280 - accuracy: 0.8629\n",
            "Epoch 115/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.0524 - accuracy: 0.8626\n",
            "Epoch 116/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.0703 - accuracy: 0.8623\n",
            "Epoch 117/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.1314 - accuracy: 0.8598\n",
            "Epoch 118/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.0623 - accuracy: 0.8586\n",
            "Epoch 119/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.0787 - accuracy: 0.8613\n",
            "Epoch 120/1000\n",
            "7982/7982 [==============================] - 68s 8ms/step - loss: 1.0653 - accuracy: 0.8606\n",
            "Epoch 121/1000\n",
            "7982/7982 [==============================] - 68s 8ms/step - loss: 1.0301 - accuracy: 0.8618\n",
            "Epoch 122/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.1259 - accuracy: 0.8573\n",
            "Epoch 123/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.1242 - accuracy: 0.8612\n",
            "Epoch 124/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1021 - accuracy: 0.8603\n",
            "Epoch 125/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1200 - accuracy: 0.8576\n",
            "Epoch 126/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.0873 - accuracy: 0.8614\n",
            "Epoch 127/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.0696 - accuracy: 0.8587\n",
            "Epoch 128/1000\n",
            "7982/7982 [==============================] - 62s 8ms/step - loss: 1.1392 - accuracy: 0.8618\n",
            "Epoch 129/1000\n",
            "7982/7982 [==============================] - 62s 8ms/step - loss: 1.0440 - accuracy: 0.8582\n",
            "Epoch 130/1000\n",
            "7982/7982 [==============================] - 62s 8ms/step - loss: 1.0783 - accuracy: 0.8589\n",
            "Epoch 131/1000\n",
            "7982/7982 [==============================] - 63s 8ms/step - loss: 1.0842 - accuracy: 0.8597\n",
            "Epoch 132/1000\n",
            "7982/7982 [==============================] - 63s 8ms/step - loss: 1.0773 - accuracy: 0.8603\n",
            "Epoch 133/1000\n",
            "7982/7982 [==============================] - 63s 8ms/step - loss: 1.1000 - accuracy: 0.8624\n",
            "Epoch 134/1000\n",
            "7982/7982 [==============================] - 62s 8ms/step - loss: 1.1153 - accuracy: 0.8596\n",
            "Epoch 135/1000\n",
            "7982/7982 [==============================] - 60s 8ms/step - loss: 1.1007 - accuracy: 0.8611\n",
            "Epoch 136/1000\n",
            "7982/7982 [==============================] - 60s 8ms/step - loss: 1.0676 - accuracy: 0.8593\n",
            "Epoch 137/1000\n",
            "7982/7982 [==============================] - 62s 8ms/step - loss: 1.0923 - accuracy: 0.8624\n",
            "Epoch 138/1000\n",
            "7982/7982 [==============================] - 62s 8ms/step - loss: 1.1201 - accuracy: 0.8613\n",
            "Epoch 139/1000\n",
            "7982/7982 [==============================] - 62s 8ms/step - loss: 1.0633 - accuracy: 0.8609\n",
            "Epoch 140/1000\n",
            "7982/7982 [==============================] - 61s 8ms/step - loss: 1.0369 - accuracy: 0.8617\n",
            "Epoch 141/1000\n",
            "7982/7982 [==============================] - 62s 8ms/step - loss: 1.0719 - accuracy: 0.8612\n",
            "Epoch 142/1000\n",
            "7982/7982 [==============================] - 63s 8ms/step - loss: 1.1030 - accuracy: 0.8626\n",
            "Epoch 143/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.1040 - accuracy: 0.8613\n",
            "Epoch 144/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.0518 - accuracy: 0.8633\n",
            "Epoch 145/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.1185 - accuracy: 0.8623\n",
            "Epoch 146/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.0692 - accuracy: 0.8624\n",
            "Epoch 147/1000\n",
            "7982/7982 [==============================] - 63s 8ms/step - loss: 1.1258 - accuracy: 0.8622\n",
            "Epoch 148/1000\n",
            "7982/7982 [==============================] - 62s 8ms/step - loss: 1.1002 - accuracy: 0.8623\n",
            "Epoch 149/1000\n",
            "7982/7982 [==============================] - 63s 8ms/step - loss: 1.0497 - accuracy: 0.8599\n",
            "Epoch 150/1000\n",
            "7982/7982 [==============================] - 63s 8ms/step - loss: 1.0999 - accuracy: 0.8603\n",
            "Epoch 151/1000\n",
            "7982/7982 [==============================] - 63s 8ms/step - loss: 1.0515 - accuracy: 0.8619\n",
            "Epoch 152/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1039 - accuracy: 0.8614\n",
            "Epoch 153/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.0460 - accuracy: 0.8636\n",
            "Epoch 154/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1229 - accuracy: 0.8621\n",
            "Epoch 155/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.0943 - accuracy: 0.8633\n",
            "Epoch 156/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.0016 - accuracy: 0.8613\n",
            "Epoch 157/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.0799 - accuracy: 0.8627\n",
            "Epoch 158/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.0755 - accuracy: 0.8626\n",
            "Epoch 159/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.0681 - accuracy: 0.8636\n",
            "Epoch 160/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.0295 - accuracy: 0.8621\n",
            "Epoch 161/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.0308 - accuracy: 0.8608\n",
            "Epoch 162/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.0271 - accuracy: 0.8637\n",
            "Epoch 163/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.0472 - accuracy: 0.8641\n",
            "Epoch 164/1000\n",
            "3264/7982 [===========>..................] - ETA: 37s - loss: 0.9356 - accuracy: 0.8680"
          ]
        }
      ],
      "source": [
        "model.fit(partial_x_train, partial_y_train, verbose=1, batch_size=1, epochs=1000)\n",
        "print(model.predict(partial_x_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVclXoeYV9to"
      },
      "outputs": [],
      "source": [
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
        "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "# plot loss during training\n",
        "pyplot.subplot(211)\n",
        "pyplot.title('Loss')\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "# plot accuracy during training\n",
        "pyplot.subplot(212)\n",
        "pyplot.title('Accuracy')\n",
        "pyplot.plot(history.history['accuracy'], label='train')\n",
        "pyplot.plot(history.history['val_accuracy'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz96CNFyW3wt"
      },
      "outputs": [],
      "source": [
        "# correction pour accélérer l'apprentissage\n",
        "\n",
        "# training with a validation set:\n",
        "history= model.fit(\n",
        "    partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(\n",
        "        x_val, y_val))\n",
        "\n",
        "# Visualize the training:\n",
        "import matplotlib.pyplot as plt\n",
        "loss = history.history['loss']\n",
        "val_loss= history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEdGdtiDZdWz"
      },
      "source": [
        "Further experiments:\n",
        "\n",
        "\n",
        "*   Train on less epochs.\n",
        "*   Try using larger or smaller layers: 32 units, 128 units, and so on.\n",
        "*   The network used two hidden layers. Now try using a single hidden layer, or three hidden layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8iCZY5xZ2AJ"
      },
      "outputs": [],
      "source": [
        "# defining the optimizer and the loss:\n",
        "\n",
        "#model.compile(\n",
        " #   optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "epochs_h.compile(\n",
        "    optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train on less epochs:\n",
        "epochs_h= model.fit(\n",
        "    partial_x_train, partial_y_train, epochs=10, batch_size= 512,\n",
        "    validation_data=(x_val, y_val))\n",
        "\n",
        "# Visualize the training:\n",
        "epochs_l = epochs_h.epochs_h['loss']\n",
        "epochs_vl= epochs_h.epochs_h['val_loss']\n",
        "epochs_e= range(1, len(epochs_l) + 1)\n",
        "plt.plot(epochs_e, epochs_l, 'bo', label='Training loss')\n",
        "plt.plot(epochs_e, epochs_vl, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9GBopNdbf2S"
      },
      "outputs": [],
      "source": [
        "# Try using 32 units\n",
        "\n",
        "#model = models.Sequential()\n",
        "#model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "#model.add(layers.Dense(64, activation='relu'))\n",
        "#model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model_32= models.Sequential()\n",
        "model_32.add(layers.Dense(32, activation='relu', input_shape=(1000,)))\n",
        "model_32.add(layers.Dense(32, activation='relu'))\n",
        "model_32.add(layers.Dense(32, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dszVETicaS6"
      },
      "outputs": [],
      "source": [
        "# Try using 128 units\n",
        "\n",
        "model_128 = models.Sequential()\n",
        "model_128.add(layers.Dense(128, activation='relu', input_shape=(1000,)))\n",
        "model_128.add(layers.Dense(128, activation='relu'))\n",
        "model_128.add(layers.Dense(128, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7hbEO2Wl-2c"
      },
      "source": [
        "Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lvNyJ_9mANb"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import boston_housing\n",
        "(train_data, train_targets), (test_data, test_targets)= boston_housing.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh98LewAmPEw"
      },
      "source": [
        "Train a small dense network on the Boston Housing data:\n",
        "\n",
        "\n",
        "*   Encode the data\n",
        "*   Define the network\n",
        "*   Define the loss and the optimizer\n",
        "*   Define a validation set\n",
        "*   Train the network using a validation set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dadRutJ5mhXI"
      },
      "outputs": [],
      "source": [
        "from numpy.lib.function_base import vectorize\n",
        "# Encode the data\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i in range (len (sequences)):\n",
        "    for j in range (len (sequences [i])):\n",
        "      results [i] [sequences [i] [j]] = 1.\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize\n",
        "\n",
        "# You should also convert your labels from integer to numeric:\n",
        "\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KRnzsMrqqQs"
      },
      "outputs": [],
      "source": [
        "# correction\n",
        "\n",
        "mean = train_data.mean(axis=0)\n",
        "train_data -= mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data /= std\n",
        "test_data -= mean\n",
        "test_data /= std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xopn-TuocRf"
      },
      "outputs": [],
      "source": [
        "# Define the network\n",
        "\n",
        "#from tensorflow.keras import Sequential\n",
        "#from tensorflow.keras.layers import Dense, Activation\n",
        "#from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "#model_128 = models.Sequential()\n",
        "#model_128.add(layers.Dense(128, activation='relu', input_shape=(1000,)))\n",
        "#model_128.add(layers.Dense(128, activation='relu'))\n",
        "#model_128.add(layers.Dense(128, activation='softmax'))\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "Housing_model = Sequential()\n",
        "Housing_model.add(layers.Dense(25, activation='relu', input_dim=20))\n",
        "Housing_model.add(layers.Dense(25, activation=('relu'))\n",
        "Housing_model.add(layers.Dense(1, activation=('linear')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VegBZZKt-3T"
      },
      "outputs": [],
      "source": [
        "# correction\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "def build_model():\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(64, activation='relu',\n",
        "                         input_shape=(train_data.shape[1],)))\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(64,activation='relu'))\n",
        "  model.add(layers.Dense(1))\n",
        "  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QwXcH31r2bq"
      },
      "outputs": [],
      "source": [
        "# define the loss and the optimizer\n",
        "\n",
        "#epochs_h.compile(\n",
        " #   optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy']\n",
        "#)\n",
        "\n",
        "opt = SGD(lr=0.01, momentum=0.9)\n",
        "Housing_model.compile(\n",
        "    optimizer='opt', loss='mean_squared_error', optimizer=opt, \n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipNkBWIitLaT"
      },
      "outputs": [],
      "source": [
        "# Define a validation set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6gKdINetYmf"
      },
      "outputs": [],
      "source": [
        "# Defining a validation set:\n",
        "\n",
        "x_val= x_train[:1000]\n",
        "partial_x_train= x_train[1000:]\n",
        "y_val= one_hot_train_labels[:1000]\n",
        "partial_y_train= one_hot_train_labels[1000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ-TGmTUv4Rw"
      },
      "outputs": [],
      "source": [
        "# inspiration sans cross-validation\n",
        "\n",
        "num_epochs = 50\n",
        "model = build_model()\n",
        "\n",
        "num_val_samples = len(train_data) // 4\n",
        "\n",
        "val_data = train_data[:num_val_samples]\n",
        "val_targets = train_targets[:num_val_samples]\n",
        "partial_train_data = train_data[num_val_samples:]\n",
        "partial_train_targets = train_targets[num_val_samples:]\n",
        "\n",
        "history = model.fit(partial_train_data, partial_train_targets, validation_data=(\n",
        "    val_data, val_targets),\n",
        "    epochs=num_epochs, batch_size=10, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrl5RtC0tlbp"
      },
      "outputs": [],
      "source": [
        "# incomplete: Train the netork using a validation set\n",
        "housing_hm = Housing_model.fit(\n",
        "    partial\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLZ_x4GlsLWm"
      },
      "outputs": [],
      "source": [
        "# not corrected: Train on less epochs:\n",
        "epochs_h= model.fit(\n",
        "    partial_x_train, partial_y_train, epochs=10, batch_size= 512,\n",
        "    validation_data=(x_val, y_val))\n",
        "\n",
        "# not corrected: Visualize the training:\n",
        "epochs_l = epochs_h.epochs_h['loss']\n",
        "epochs_vl= epochs_h.epochs_h['val_loss']\n",
        "epochs_e= range(1, len(epochs_l) + 1)\n",
        "plt.plot(epochs_e, epochs_l, 'bo', label='Training loss')\n",
        "plt.plot(epochs_e, epochs_vl, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wu0t6x7qx-mi"
      },
      "outputs": [],
      "source": [
        "# correction\n",
        "\n",
        "import numpy as np\n",
        "k = 4\n",
        "num_val_samples = len(train_data) // k\n",
        "num_epochs = 100\n",
        "all_scores = []\n",
        "for i in range(k):\n",
        "  print('processing fold #', i)\n",
        "  val_data = train_data[i * num_val_samples]\n",
        "  val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "  partial_train_data = np.concatenate(\n",
        "      [\n",
        "       train_data[\n",
        "                  :i * num_val_samples], train_data[\n",
        "                                                    (\n",
        "                                                        i + 1)\n",
        "                                                     * num_val_samples:]], \n",
        "                                      axis=0)\n",
        "  partial_train_targets = no.concatenate(\n",
        "      [\n",
        "       train_targets[\n",
        "                     :i * num_val_samples], train_targets[\n",
        "                                                          (\n",
        "                                                              i + 1\n",
        "                                                          ) * num_val_samples:\n",
        "                     ]\n",
        "       ], axis=0\n",
        "      )\n",
        "  model = build_model()\n",
        "  model.fit(partial_train_data, partial_train_targets,\n",
        "            epochs=num_epochs, batch_size=1, verbose=0)\n",
        "  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
        "  all_scores.append(val_mae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RicW699w1JdK"
      },
      "outputs": [],
      "source": [
        "num_epochs = 500\n",
        "all_mae_histories = []\n",
        "for i in range(k):\n",
        "    print('processing fold #', i)\n",
        "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    partial_train_data = np.concatenate ([train_data[:i * num_val_samples],\n",
        "                                                               train_data[(i + 1) * num_val_samples:]], axis=0)\n",
        "    partial_train_targets = np.concatenate([train_targets[:i * num_val_samples],\n",
        "                                                                  train_targets[(i + 1) * num_val_samples:]], axis=0)\n",
        "    model = build_model()\n",
        "    history = model.fit(partial_train_data, partial_train_targets,\n",
        "                                  validation_data=(val_data, val_targets),\n",
        "                                  epochs=num_epochs, batch_size=1, verbose=0)\n",
        "    mae_history = history.history['val_mean_absolute_error']\n",
        "    all_mae_histories.append(mae_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROcSe77S1NXn"
      },
      "outputs": [],
      "source": [
        "average_mae_history = [\n",
        "np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
        " \n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(1, len(average_mae_history) + 1),\n",
        "            average_mae_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation MAE')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjtKRXZ01VhG"
      },
      "outputs": [],
      "source": [
        "# entrainement du modèle une fois hyperparamètres fixés\n",
        "model = build_model()\n",
        " \n",
        "model.fit(train_data, train_targets, \n",
        "               epochs=80, batch_size=16, verbose=0)\n",
        "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtNui1G_iVPW"
      },
      "source": [
        "CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM23N7sQiS5g"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "#(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RILz07OBlRRz"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dense, Flatten\n",
        "from keras.layers import Conv2D\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "# (x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9ZHwCb0mUyh"
      },
      "source": [
        "\n",
        "\n",
        "*   Implement a convolutional network for MNIST\n",
        "*   3x3 filters\n",
        "*   32 planes for the first layer\n",
        "*   64 planes for the second layer\n",
        "*   Fully connected layer with 128 neurons\n",
        "*   10 classes for the output layer\n",
        "*   ReLU\n",
        "*   Softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBUFxfNNpv64"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6v85T3lnXZ2",
        "outputId": "020eff62-04b0-4e92-dc18-c0f2d155bf92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# correction\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        " \n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "input_shape = (28, 28, 1)\n",
        " \n",
        "# normalisation of the inputs\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        " \n",
        "# convert class vectors to binary class matrices : 3 becomes [0,0,0,1,0,0,0,0,0 0]\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_R9SMfGbpldj"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NKXyfpTqeBn",
        "outputId": "dd56a8ac-e373-4046-bd6a-671e5978a5c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "118/118 [==============================] - 157s 1s/step - loss: 2.2702 - accuracy: 0.2778 - val_loss: 2.2295 - val_accuracy: 0.4186\n",
            "Epoch 2/10\n",
            "118/118 [==============================] - 156s 1s/step - loss: 2.1869 - accuracy: 0.4853 - val_loss: 2.1333 - val_accuracy: 0.5452\n",
            "Epoch 3/10\n",
            "118/118 [==============================] - 156s 1s/step - loss: 2.0794 - accuracy: 0.5718 - val_loss: 2.0076 - val_accuracy: 0.5987\n",
            "Epoch 4/10\n",
            "118/118 [==============================] - 154s 1s/step - loss: 1.9379 - accuracy: 0.6195 - val_loss: 1.8429 - val_accuracy: 0.6462\n",
            "Epoch 5/10\n",
            "118/118 [==============================] - 158s 1s/step - loss: 1.7547 - accuracy: 0.6690 - val_loss: 1.6342 - val_accuracy: 0.7068\n",
            "Epoch 6/10\n",
            "118/118 [==============================] - 155s 1s/step - loss: 1.5331 - accuracy: 0.7169 - val_loss: 1.3953 - val_accuracy: 0.7573\n",
            "Epoch 7/10\n",
            "118/118 [==============================] - 155s 1s/step - loss: 1.2954 - accuracy: 0.7617 - val_loss: 1.1561 - val_accuracy: 0.7983\n",
            "Epoch 8/10\n",
            "118/118 [==============================] - 153s 1s/step - loss: 1.0751 - accuracy: 0.7964 - val_loss: 0.9526 - val_accuracy: 0.8249\n",
            "Epoch 9/10\n",
            "118/118 [==============================] - 154s 1s/step - loss: 0.8978 - accuracy: 0.8209 - val_loss: 0.7982 - val_accuracy: 0.8408\n",
            "Epoch 10/10\n",
            "118/118 [==============================] - 157s 1s/step - loss: 0.7670 - accuracy: 0.8359 - val_loss: 0.6881 - val_accuracy: 0.8540\n",
            "Test loss: 0.688136100769043\n",
            "Test accuracy: 0.8539999723434448\n"
          ]
        }
      ],
      "source": [
        "# exemple aléatoire non optimisé pour savoir faire une archi standard\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        " \n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=512,\n",
        "          epochs=10,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        " \n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQQO3fZtipbY"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "#from tensorflow.keras.layers import Dense, Activation\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Reshape((3, 4), input_shape=(12,)))\n",
        "# now: model.output_shape == (None, 3, 4)\n",
        "# note: 'None' is the batch dimension\n",
        "\n",
        "# as intermediate layer in s Sequential model\n",
        "model.add(Reshape((6, 2)))\n",
        "# now: model.output_shape == (None, 6, 2)\n",
        "\n",
        "# also supports shape inference using '-1' as dimension\n",
        "model.add(Reshape((-1, 2, 2)))\n",
        "# now: model.output_shape == (None, 3, 2, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXA6aKICkQYV"
      },
      "outputs": [],
      "source": [
        "model.add(Convolution2D(64, 3, 3,\n",
        "                        border_mode='same',\n",
        "                        input_shape=(3, 32, 32)))\n",
        "# now: model.output_shape == (None, 64, 32, 32)\n",
        "\n",
        "model.add(Flatten())\n",
        "# now: model.output_shape == (None, 65536)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDJwzmsLlE5R"
      },
      "source": [
        "Max Pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG3Ie9dSlKKB"
      },
      "outputs": [],
      "source": [
        "# MaxPooling2D(pool_size=(2,2), strides=None, padding='valid', data_format=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1_q93XEoGiZ"
      },
      "source": [
        "CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ae4rHDAvROp"
      },
      "source": [
        "## Practical Work\n",
        "\n",
        "\n",
        "*   Add Dropout and MaxPooling to the MNIST network.\n",
        "*   Train a network on the CIFAR10 image dataset.\n",
        "*   Add Dropout and MaxPooling to the CIFAR10 network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFstKQz4tDit",
        "outputId": "9478c9e8-8fc6-4345-d349-395336bf20fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "#from tensorflow.keras.datasets import mnist\n",
        "\n",
        "#(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T15pX-gwc_A"
      },
      "outputs": [],
      "source": [
        "# MNIST\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(\n",
        "    28, 28, 1)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmfP22kqxUOH"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (\n",
        "    test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "train_labels = tf.keras.utils.to_categorical (train_labels)\n",
        "test_labels = tf.keras.utils.to_categorical (test_labels)\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUk26wEUoHqI",
        "outputId": "7e513e0f-9e58-4586-bef6-4f8f245a3880"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "98/98 [==============================] - 58s 580ms/step - loss: 2.0129 - accuracy: 0.2465 - val_loss: 1.7149 - val_accuracy: 0.3793\n",
            "Epoch 2/10\n",
            "98/98 [==============================] - 57s 585ms/step - loss: 1.7088 - accuracy: 0.3736 - val_loss: 1.5298 - val_accuracy: 0.4486\n",
            "Epoch 3/10\n",
            "98/98 [==============================] - 60s 598ms/step - loss: 1.5459 - accuracy: 0.4421 - val_loss: 1.3581 - val_accuracy: 0.5150\n",
            "Epoch 4/10\n",
            "98/98 [==============================] - 59s 598ms/step - loss: 1.4392 - accuracy: 0.4851 - val_loss: 1.3036 - val_accuracy: 0.5437\n",
            "Epoch 5/10\n",
            "98/98 [==============================] - 59s 598ms/step - loss: 1.3692 - accuracy: 0.5102 - val_loss: 1.2374 - val_accuracy: 0.5666\n",
            "Epoch 6/10\n",
            "98/98 [==============================] - 58s 597ms/step - loss: 1.3169 - accuracy: 0.5349 - val_loss: 1.1847 - val_accuracy: 0.5801\n",
            "Epoch 7/10\n",
            "98/98 [==============================] - 59s 598ms/step - loss: 1.2767 - accuracy: 0.5493 - val_loss: 1.1319 - val_accuracy: 0.6025\n",
            "Epoch 8/10\n",
            "98/98 [==============================] - 58s 593ms/step - loss: 1.2341 - accuracy: 0.5652 - val_loss: 1.1554 - val_accuracy: 0.5826\n",
            "Epoch 9/10\n",
            "98/98 [==============================] - 58s 595ms/step - loss: 1.1942 - accuracy: 0.5800 - val_loss: 1.0672 - val_accuracy: 0.6243\n",
            "Epoch 10/10\n",
            "98/98 [==============================] - 59s 604ms/step - loss: 1.1611 - accuracy: 0.5927 - val_loss: 1.0555 - val_accuracy: 0.6261\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f707aba6390>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "train_labels = tf.keras.utils.to_categorical (train_labels)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (\n",
        "    #3,3), activation = 'relu', import-shape=(32, 32, 3)))\n",
        "    #3,3), activation = 'relu', import_shape=(32, 32, 3)))\n",
        "    #3,3), activation = 'relu', imput_shape=(32, 32, 3)))\n",
        "    3,3), activation ='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#model.add(layers.Conv2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(\n",
        "        \n",
        "    ), metrics=['accuracy'])\n",
        "\n",
        "#model.fit(train_image, train_labels, batch_size=512, epochs=10, verbose=1, validation_data=(test_images, test_labels))\n",
        "model.fit(\n",
        "    train_images, train_labels, batch_size=512, epochs=10, verbose=1, validation_data=(\n",
        "        test_images, test_labels\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XbHl3nmq939"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import to_categorical\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "inputs = keras.Input(shape=(784,), name='img')\n",
        "x = layers.Dense(64, activation='relu')(inputs)\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "outputs = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs, name='mnist_model')\n",
        "model.compile(\n",
        "    optimizer='rmsprop', loss='categorical_crossentropy', metrics=[\n",
        "                                                                             'accuracy'])\n",
        "model.fit(train_images, train_labels, epoch=5, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVvCc1W8B6hM"
      },
      "source": [
        "Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOJEY43I9pjp"
      },
      "outputs": [],
      "source": [
        "encoder_input = keras.input(shape=(28, 28, 1), name='img')\n",
        "x = layers.Conv2D(16, 3, activation='relu')(encoder_input)\n",
        "x = layers.Conv2D(32, 3, activation='relu')(x)\n",
        "x = layers.Conv2D(16, 3, activation='relu')(x)\n",
        "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
        "\n",
        "encoder = keras.Model(encoder_input, encoder_output, name='encoder')\n",
        "encoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-0Fru7z-oqS"
      },
      "outputs": [],
      "source": [
        "x = layers.Reshape((4, 4, 1))(encoder_output)\n",
        "x = layers.Conv2DTranspose(16, 3, activation='relu')(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation='relu')(x)\n",
        "x = layers.UpSampling2D(3)(x)\n",
        "x = layers.Conv2DTranspose(16, 3, activation='relu')(x)\n",
        "\n",
        "autoencoder = keras.Model(encoder_input, decoder_output, name='autoencoder')\n",
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtvyOTmY_g-0"
      },
      "outputs": [],
      "source": [
        "encoder_input = keras.input(shape=(28, 28, 1), name='original_img')\n",
        "x = layers.Conv2D(16, 3, activation='relu')(encoder_input)\n",
        "x = layers.Conv2D(32, 3, activation='relu')(x)\n",
        "x = layers.MaxPooling2D(3)(x)\n",
        "x = layers.Conv2D(16, 3, activation='relu')(x)\n",
        "x = layers.Conv2D(16, 3, activation='relu')(x)\n",
        "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
        "\n",
        "encoder = keras.Model(encoder_input, encoder_output, name='encoder')\n",
        "encoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XeZizzBAcwG"
      },
      "outputs": [],
      "source": [
        "decoder_input = keras.input(shape=(16,), name='encoded_img')\n",
        "x = layers.Reshape((4, 4, 1))(decoder_input)\n",
        "x = layers.Conv2DTranspose(16, 3, activation='relu')(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation='relu')(x)\n",
        "x = layers.UpSampling2D(3)(x)\n",
        "x = layers.Conv2DTranspose(16, 3, activation='relu')(x)\n",
        "decoder_output = layers.Conv2Dtranspose(1, 3, activation='relu')(x)\n",
        "\n",
        "decoder = keras.Model(decoder_input, decoder_output, name='decoder')\n",
        "decoder.summary()\n",
        "\n",
        "autoencoder_input = keras.input(shape=(28, 28, 1), name='img')\n",
        "encoded_img = encoder(autoencoder_input)\n",
        "decoded_img = decoder(encoded_img)\n",
        "autoencoder = keras.Model(autoencoder_input, decoded_img, name='autoencoder')\n",
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5YE3bro9iWE"
      },
      "source": [
        "## Exercise\n",
        "Train an autoencoder for fashion_mnist with the functional API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "tXgDz5aSB9GS",
        "outputId": "72d44a9e-3d75-4076-b8be-55f4d896e0e7"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a7b006a840d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'encoded_img'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2DTranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2DTranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUpSampling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
          ]
        }
      ],
      "source": [
        "decoder_input = keras.input(shape=(16,), name='encoded_img')\n",
        "x = layers.Reshape((4, 4, 1))(decoder_input)\n",
        "x = layers.Conv2DTranspose(16, 3, activation='relu')(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation='relu')(x)\n",
        "x = layers.UpSampling2D(3)(x)\n",
        "x = layers.Conv2DTranspose(16, 3, activation='relu')(x)\n",
        "decoder_output = layers.Conv2DTranspose(1, 3, activation='relu')(x)\n",
        "\n",
        "decoder = keras.Model(decoder_input, decoder_output, name='decoder')\n",
        "decoder.summary()\n",
        "\n",
        "autoencoder_input = keras.Input(shape=(28, 28, 1), name='img')\n",
        "encoded_img = encoder(autoencoder_input)\n",
        "decoded_img = decoder(encoded_img)\n",
        "autoencoder = keras.Model(autoencoder_input, decoded_img, name='autoencoder')\n",
        "autoencoder.summary()\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fasion_mnisty.load_data()\n",
        "\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.astype('float32') / 255\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "encoder_input = tf.keras.Input(shape=(28, 28, 1), name='original_img')\n",
        "x = layers.Conv2D(16, 3, activation='relu')(encoder_input)\n",
        "x = layers.Conv2D(32, 3, activation='relu')(x)\n",
        "x = layers.MaxPooling2D(3)(x)\n",
        "x = layers.Conv2D(32, 3, activation='relu')(x)\n",
        "x = layers.Conv2D(16, 3, activation='relu')(x)\n",
        "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
        "\n",
        "encoder = tf.keras.Model(encoder_input, encoder_output, name='encoder')\n",
        "encoder.summary()\n",
        "\n",
        "decoder_input = tf.keras.Input(shape=(16,), name='encoded_img')\n",
        "x = layers.Reshape((4, 4, 1))(decoder_input)\n",
        "x = layers.Conv2DTranspose(16, 3, activation='relu')(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation='relu')(x)\n",
        "x = layers.UpSampling2D(3)(x)\n",
        "x = layers.Conv2DTranspose(16, 3, activation='relu')(x)\n",
        "decoder_output = layers.Conv2DTranspose(1, 3, activation='relu')(x)\n",
        "\n",
        "decoder = tf.keras.Model(decoder_input, decoder_output, name='decoder')\n",
        "decoder.summary()\n",
        "\n",
        "autoencoder_input = tf.keras.Input(shape=(28, 28, 1), name='img')\n",
        "encoded_img = encoder(autoencoder_input)\n",
        "decoded_img = decoder(encoded_img)\n",
        "autoencoder = tf.keras.Model(autoencoder_input, decoded_img, name='autoencoder')\n",
        "autoencoder.summary()\n",
        "\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "autoencoder.fit(train_images, train_images, epochs=5, batch_size=12/8, shuffle=true, validation_data=(test_images, test_images))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"and the actual image looks like\")\n",
        "img_out = autoencoder.predict(test_images[0].reshape(1,28,28,1))\n",
        "f, axarr = plt.subplots(1,2)\n",
        "axarr[0].imshow(test_images[0].reshape(28,28),cmap='Greys')\n",
        "axarr[1].imshow(img_out.reshape(28,28), cmap='Greys')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyke1kb4IB8v"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRTgFj8bbM36"
      },
      "source": [
        "## Multiple Inputs and Outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS1XZaABbS7L"
      },
      "source": [
        "\n",
        "\n",
        "*   Models with multiple inputs and outputs\n",
        "*   The functional API makes it easy to manipulate multiple inputs and outputs. This cannot be handled with the Sequential API.\n",
        "*   Let's say you're building a system for ranking custom issue tickets by priority and routing them to the right department.\n",
        "*   Your model will have 3 inputs:\n",
        "\n",
        "\n",
        "1.   Title of the ticket (text input)\n",
        "2.   Text body of the ticket (text input)\n",
        "3.   Any tags added by the user (categorical input)\n",
        "\n",
        "*   It will have two outputs:\n",
        "\n",
        "\n",
        "1.   Priority score between 0 and 1 (scalar sigmoid output)\n",
        "2.   The department that should handle the ticket (softmax output over the set of departments)\n",
        "\n",
        "*   Let's build this model in a few lines with the Functional API.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Au6O-y2xcp6G"
      },
      "outputs": [],
      "source": [
        "num_tags = 12 # Number of unique issue tags\n",
        "num_words = 10000 # Size of vocabulary obtained when preprocessing text data\n",
        "num_departments = 4 # Number of departments for predictions    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTILytbCbRS5"
      },
      "outputs": [],
      "source": [
        "title_input = keras.input(\n",
        "    shape=(None,), name='title') # Variable-length sequence of ints\n",
        "body_input = keras.Input(\n",
        "    shape=(None,), name='body') # Variable-length sequence of ints\n",
        "tags_input = keras.Input(\n",
        "    shape=(num_tags,), name='tags') # Binary vectors of size 'num_tags'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuDlsQkZdszi"
      },
      "outputs": [],
      "source": [
        "# Embed each word in the title into a 64-dimensional vector\n",
        "title_features = layers.Embedding(num_words, 64)(title_input)\n",
        "# Embed each word in the text into a 64-dimensional vector\n",
        "body_features = layers.Embedding(num_words, 64)(body_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7ToCmZweF_A"
      },
      "outputs": [],
      "source": [
        "# Reduce sequence of embedded words in the title into a single 128-dimensional vector\n",
        "title_features = layers.LSTM(128)(title_features)\n",
        "# Reduce sequence of embedded words in the body into a single 32-dimensional vector\n",
        "body_features = layers.LSTM(32)(body_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPyRcjD6fAOF"
      },
      "outputs": [],
      "source": [
        "# Merge all available features into a single large vector via concatenation\n",
        "x = layers.concatenate([title_features, body_features, tags_input])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxA6bPJEffNw"
      },
      "outputs": [],
      "source": [
        "# Stick a logistic regression for priority prediction on top of the features\n",
        "priority_pred = layers.Dense(1, activation='sigmoid', name='priority')(x)\n",
        "# Stick a department classifier on top of the features\n",
        "department_pred = layers.Dense(\n",
        "    num_departments, activation='softmax', name='department')(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lf4vRjpygBeL"
      },
      "outputs": [],
      "source": [
        "# Instantiate an end-to-end model predicting both priority and department\n",
        "model = keras.Model(inputs=[title_input, body_input, tags_input],\n",
        "                    output=[priority_pred, department_pred])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpHrnuxlgY4t"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(model, 'multi_input_and_output.png', show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NIS9zQagpKG"
      },
      "source": [
        "* When compiling this model, we can assign different losses to each output.\n",
        "* You can even assign different weights to each loss, to modulate their contribution to the total training loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PH2GdadOg4dU"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optipmizer=keras.optimizers.RMSprop(\n",
        "        1e-3),\n",
        "              loss=[\n",
        "                    'binary_crossentropy', 'categorical_crossentropy'],\n",
        "              loss_weights=[1., 0.2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4dAcS50hRvL"
      },
      "source": [
        "Since we gave names to our output layers, we could also specify the loss like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlQtbIPthYsq"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "              loss={'priority': 'binary_crossentropy',\n",
        "                    'department': 'categorical_crossentropy'},\n",
        "              loss_weights=[1., 0.2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kmOPZWMhy3Z"
      },
      "source": [
        "We can train the model by passing lists of Numpy arrays of inputs and targets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iRnAj1rh4Of"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYhm27sJh6nz"
      },
      "outputs": [],
      "source": [
        "# Dummy input data\n",
        "title_data = np.random.randint(num_words, size=(1280, 10))\n",
        "body_data = np.random.randint(num_words, size=(1280, 100))\n",
        "tags_data = np.random.randint(\n",
        "    2, size=(1280, num_tags)).astype('float32')\n",
        "# Dummy target data\n",
        "priority_targets = np.random.random(size=(1280, 1))\n",
        "dept_targets = np.random.randint(2, size=(1280, num_departments))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tm_nWfn_i9pq"
      },
      "outputs": [],
      "source": [
        "model.fit({'title': title_data, 'body': body_data, 'tags': tags_data},\n",
        "          {'priority': priority_targets, 'department': dept_targets},\n",
        "          epochs=2,\n",
        "          batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqsUy7KZjg_W"
      },
      "source": [
        "### Exercice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLg2hT67jkMl"
      },
      "source": [
        "Train the ticket model on the randomly generated data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1FXMBstjvNj"
      },
      "source": [
        "### Exercice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEt5WK8rjwg7"
      },
      "source": [
        "\n",
        "\n",
        "*   Write convolutional model that takes 21 19x19 planes as input and that outputs a vector of 361 with a softmax (the policy) and an output of 1 (the value).\n",
        "\n",
        "*   Train it on a randomly generated data with different losses for the policy (categorical cross entropy) and the value (mse)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DY3_2TWkeSo"
      },
      "outputs": [],
      "source": [
        "N = 10000\n",
        "planes = 31\n",
        "moves = 361"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvybOUDpsZna"
      },
      "outputs": [],
      "source": [
        "input_data = np.random.randint(\n",
        "    2, size=(N, 19, 19, planes))\n",
        "planes = 31\n",
        "moves = 361"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sivV1WZfslaZ"
      },
      "outputs": [],
      "source": [
        "input_data = np.random.randint(\n",
        "    2, size=(N, 19, 19, planes))\n",
        "input_data = input_data.astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Jg8Uxmhsxnx"
      },
      "outputs": [],
      "source": [
        "policy = np.random.randint(moves, size=(N,))\n",
        "# transformation en vecteurs de taille 361\n",
        "policy = keras.utils.to_categorical(policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHhVMx64s8zw"
      },
      "outputs": [],
      "source": [
        "value = np.random.randint(2, size=(N,))\n",
        "value = value.astype('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWMmOCyUtT-b"
      },
      "source": [
        "Building the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJqAVkrWtS2H"
      },
      "outputs": [],
      "source": [
        "input = keras.Input(\n",
        "    shape=(19, 19, planes), name='board')\n",
        "x = layers.Conv2D(\n",
        "    32, 3, activation='relu', padding='same')(input)\n",
        "x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
        "x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
        "x = layers.Conv2D(32, 3#...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrKd2H_bugUU"
      },
      "source": [
        "Training and saving the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3di4yXgwuSX"
      },
      "outputs": [],
      "source": [
        "# correction\n",
        "\n",
        "N = 10000\n",
        "planes = 31\n",
        "moves = 361\n",
        "\n",
        "input_data = np.random.randint(2, size=(N, 19, 19, planes))\n",
        "input_data = input_data.astype ('float32')\n",
        "   \n",
        "policy = np.random.randint(moves, size=(N,))\n",
        "policy = keras.utils.to_categorical (policy)\n",
        " \n",
        "value = np.random.randint(2, size=(N,))\n",
        "value = value.astype ('float32')\n",
        "\n",
        "input = keras.Input(shape=(19, 19, planes), name='board')\n",
        "x = layers.Conv2D(32, 3, activation='relu', padding='same')(input)\n",
        "x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
        "x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
        "x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
        "x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
        "x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
        "policy_head = layers.Conv2D(1, 3, activation='relu', padding='same')(x)\n",
        "policy_head = layers.Flatten()(policy_head)\n",
        "policy_head = layers.Dense(moves, activation='softmax', name='policy')(policy_head)\n",
        "value_head = layers.Flatten()(x)\n",
        "value_head = layers.Dense(1, activation='sigmoid', name='value')(value_head)\n",
        "\n",
        "model = keras.Model(inputs=input, outputs=[policy_head, value_head])\n",
        "\n",
        "model.summary ()\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.SGD(lr=0.001),\n",
        "                        loss={'value': 'mse', 'policy': 'categorical_crossentropy'})\n",
        "\n",
        "model.fit(input_data, {'policy': policy, 'value': value},\n",
        "              epochs=20, batch_size=128, validation_split=0.1)\n",
        "\n",
        "model.save ('test.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQqivtHUxgZI"
      },
      "source": [
        "## Residual Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZwQ4gCWxj9I"
      },
      "source": [
        "Defining a residual model with layers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXBZSNLSxnKp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "input = layers.Input(shape=(32, 32, 3))\n",
        "x = layers.Conv2D(\n",
        "    32, 1, activation='relu', padding='same')(input)\n",
        "    ident = x\n",
        "x = layers.Conv2D(\n",
        "    32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = layers.Conv2D(\n",
        "    32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = layers.add([ident,x])\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(10)(x)\n",
        "model = tf.keras.models.Model(inputs=input, outputs=x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXgA9dzbyo_p"
      },
      "source": [
        "Train a standard convolutional network on the CIFAR10 image dataset.<br>\n",
        "Compare it to a residual network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msY5hPs0zanK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers # datasets, layers, models\n",
        "# from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "\n",
        "(train_images, train_labels), (\n",
        "    test_images, test_labels)=datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images/255.0, test_images/255.0\n",
        "\n",
        "train_labels = tf.keras.utils.to_categorical (train_labels)\n",
        "test_labels = tf.keras.utils.to_categorical (test_labels)\n",
        "\n",
        "\"\"\"\n",
        "model = models.Sequential()\n",
        "model.add(\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='softmax'))\n",
        "\"\"\"\n",
        "\n",
        "x = layers.Conv2D() qq \n",
        "model.compile(\n",
        "    loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(\n",
        "        \n",
        "    ), metrics=['accuracy']\n",
        "\n",
        "model.fit(\n",
        "    train_images, train_labels, batch_size=512, epochs=10, verbose=1, validation_data=(\n",
        "        test_images, test_labels\n",
        "    )\n",
        ")    \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGhry1__34jq"
      },
      "outputs": [],
      "source": [
        "# Correction\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "input = layers.Input(shape=(32, 32, 3))\n",
        "x = layers.Conv2D(\n",
        "    64, 1, activation='relu', padding='same')(input)\n",
        "for i in range (5):\n",
        "  ident = x\n",
        "  x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "  x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "  x = layers.add([ident,x])\n",
        "flatten = layers.Flatten()(x)\n",
        "#dense = layers.Dense(10, activation=\"softmax\")(flattent)  \n",
        "dense = layers.Dense(10, activation=\"softmax\")(flatten)\n",
        "model = tf.keras.models.Model(inputs=input, outputs=dense)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUakEO5L43WO",
        "outputId": "9b1ecf3d-a3de-4f15-be24-53061239ae12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 3s 0us/step\n",
            "170508288/170498071 [==============================] - 3s 0us/step\n",
            "625/625 [==============================] - 2352s 4s/step - loss: 1.8904 - acc: 0.4212 - val_loss: 1.3717 - val_acc: 0.5176\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f389b1ad550>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(x_train, y_train), (\n",
        "    x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "x_train = x_train.astype('float32') /255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(1e-3),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=64,\n",
        "          epochs=1,\n",
        "          validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning"
      ],
      "metadata": {
        "id": "a7yHKSY5p-Xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise"
      ],
      "metadata": {
        "id": "D6-4O1-PqD4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reuse the first layers of VGG19 to train a model with a different head on CIFAR10."
      ],
      "metadata": {
        "id": "IqemdC64qIAM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p13XyZ9M6Bun"
      },
      "outputs": [],
      "source": [
        "# model = VGG19(include_top=False,\n",
        "              input_shape=(32, 32, 3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "input = layers.Input(shape=(32, 32, 3))\n",
        "x = layers.Conv2D(\n",
        "    64, 1, activation='relu', padding='same')(input)\n",
        "for i in range (5):\n",
        "  ident = x\n",
        "  x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "  x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "  x = layers.add([ident,x])\n",
        "flatten = layers.Flatten()(x) #add([ident,x])\n",
        "dense = layers.Dense(10, activation=\"softmax\")(flatten)\n",
        "model = tf.keras.models.Model(inputs=input, outputs=dense) #=dense)\n",
        "#)\n",
        "#)"
      ],
      "metadata": {
        "id": "cyP67RbCrKGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correction\n",
        "model.trainable = False\n",
        "input_layer = model.input\n",
        "x = model(input_layer)\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(10, activation='softmax')(x)"
      ],
      "metadata": {
        "id": "v0ozC5Ftu2ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model.summary()"
      ],
      "metadata": {
        "id": "mGOGBz3W1IIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tristan_model = tf.keras.models.Model(\n",
        "    inputs=input_layer, outputs=x\n",
        ")\n",
        "Tristan_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCuGv1zPvVL2",
        "outputId": "b139a709-c948-4e01-a177-9187772c0e85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " model_1 (Functional)        (None, 10)                1024906   \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 10)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               2816      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,030,292\n",
            "Trainable params: 5,386\n",
            "Non-trainable params: 1,024,906\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Tristan_model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam', metrics=[\n",
        "                               'acc'\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "oCLkSgLCvfBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = Tristan_model.fit(\n",
        "    x_train, y_train, #X_train, y_train,\n",
        "    # batch_size est par défaut 32, iso 64\n",
        "    validation_split=0.1, epochs=5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cx5nEKmaz0_z",
        "outputId": "563e9682-fc91-4392-f591-b9eab1d65601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1407/1407 [==============================] - 726s 515ms/step - loss: 2.2037 - acc: 0.1963 - val_loss: 2.1711 - val_acc: 0.2174\n",
            "Epoch 2/5\n",
            "1407/1407 [==============================] - 715s 508ms/step - loss: 2.1638 - acc: 0.2132 - val_loss: 2.1502 - val_acc: 0.2114\n",
            "Epoch 3/5\n",
            "1407/1407 [==============================] - 717s 510ms/step - loss: 2.1511 - acc: 0.2193 - val_loss: 2.1378 - val_acc: 0.2242\n",
            "Epoch 4/5\n",
            "1407/1407 [==============================] - 714s 508ms/step - loss: 2.1413 - acc: 0.2233 - val_loss: 2.1289 - val_acc: 0.2324\n",
            "Epoch 5/5\n",
            "1407/1407 [==============================] - 723s 514ms/step - loss: 2.1344 - acc: 0.2273 - val_loss: 2.1329 - val_acc: 0.2212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Tristan_model.evaluate(\n",
        "    x_test, y_test #x_train, y_test\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V733u16s0RQh",
        "outputId": "355ff4a5-a5cd-4c16-f324-6159263848b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 141s 451ms/step - loss: 2.1337 - acc: 0.2262\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.133739948272705, 0.22619999945163727]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lambda layers"
      ],
      "metadata": {
        "id": "HtsETWcp32S4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise<br>\n",
        "Write a lamnda layer that flips a board from left to right"
      ],
      "metadata": {
        "id": "NK77ue7b34d9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flip(x):\n",
        "  x = tf.reverse(x, [-1])\n",
        "  return x"
      ],
      "metadata": {
        "id": "6mc2fMjj0qZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Lambda(flip))"
      ],
      "metadata": {
        "id": "X_oMHrgA4NJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MobileNet"
      ],
      "metadata": {
        "id": "PGkR8tWj_uF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise<br>\n",
        "\n",
        "\n",
        "1.   Define an inverted residual block.\n",
        "2.   Define a MobileNet v2 network with a tower of inverted residual blocks.\n",
        "3.   Train the network on CIFAR 10.\n",
        "\n"
      ],
      "metadata": {
        "id": "4QA7SOn__x2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Define an inverted residual block.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "input = layers.Input(\n",
        "    shape=(\n",
        "        32, 32, 3\n",
        "    )\n",
        ")\n",
        "x = layers.Conv2D(32, 1, activation='relu', padding='same')(input)\n",
        "for i in range (5):\n",
        "  ident = x\n",
        "x = layers.Conv2D(\n",
        "    32, (3, 3), activation='relu', padding='same'\n",
        ")(x)\n",
        "x = layers.Conv2D(\n",
        "    32, (3, 3), activation='relu', padding='same'\n",
        ")(x)\n",
        "x = layers.add(\n",
        "    [\n",
        "     ident,x\n",
        "    ]\n",
        ")\n",
        "x = layers.add([ident,x])\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(10)(x)\n",
        "model = tf.keras.models.Model(\n",
        "    inputs=input, outputs=x\n",
        ")"
      ],
      "metadata": {
        "id": "HbkgFrG4AMIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cf [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://towardsdatascience.com/mobilenetv2-inverted-residuals-and-linear-bottlenecks-8a4362f4ffd5)"
      ],
      "metadata": {
        "id": "F63H0Om3HlPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define a MobileNet v2 network with a tower of inverted residual blocks.\n",
        "\n",
        "def relu6(x):\n",
        "  return min(max(0, x), 6)"
      ],
      "metadata": {
        "id": "9w3T2ibOGr2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bottleneck_block(x, expand=64, squeeze=16):\n",
        "  m = Conv2D(expand, (1,1))(x)\n",
        "  m = BatchNormalization()(m)\n",
        "  m = Activation('relu6')(m)\n",
        "  m = DepthwiseConv2D((3,3))(m)\n",
        "  m = BatchNormalization() (m)\n",
        "  m = Activation('relu6') (m)\n",
        "  m = BatchNormalization() (m)\n",
        "  return Add() ([m, x])"
      ],
      "metadata": {
        "id": "0c6XH8sJHikd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correction\n",
        "def bottleneck_block(x, expand=96, squeeze=16):\n",
        "  m = layers.Conv2D(\n",
        "      expand, (\n",
        "          1,1), kernel_regularizer=regularizers.l2(\n",
        "              0.0001), use_bias = False)(\n",
        "                  x\n",
        "              )\n",
        "  m = layers.BatchNormalization()(m)\n",
        "  m = layers.Activation('relu')(m)\n",
        "  m = layers.DepthwiseConv2D(\n",
        "      (\n",
        "          3,3\n",
        "      ), padding='same', kernel_regularizer=regularizers.l2(\n",
        "          0.0001\n",
        "      ), use_bias = False\n",
        "  )(m)\n",
        "  m = layers.BatchNormalization()(\n",
        "      m\n",
        "  )\n",
        "  m = layers.Activation(\n",
        "      'relu'\n",
        "  )(\n",
        "      m\n",
        "  )\n",
        "  m = layers.Conv2D(squeeze, (\n",
        "      1, 1\n",
        "  ), kernel_regularizer=regularizers.l2(\n",
        "      0.0001\n",
        "  ), use_bias = False)(\n",
        "      m\n",
        "  )\n",
        "\n",
        "  return layers.Add()(\n",
        "      [\n",
        "       m, x\n",
        "      ]\n",
        "  )"
      ],
      "metadata": {
        "id": "78q0U-CLI3_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers"
      ],
      "metadata": {
        "id": "Z0AjaWI8Ml6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correction\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "input = layers.Input(shape=(32, 32, 3))\n",
        "x = layers.Conv2D(\n",
        "    16, 1, activation='relu', padding='same'\n",
        ")(\n",
        "    input\n",
        ")\n",
        "for i in range (5):\n",
        "  x = bottleneck_block(x)\n",
        "flatten = layers.Flatten()(x)\n",
        "dense = layers.Dense(\n",
        "    10, activation=\"softmax\"\n",
        ")(flatten)\n",
        "model = tf.keras.models.Model(inputs=input, outputs=dense)"
      ],
      "metadata": {
        "id": "wWpo_OIOLUsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Train the network on CIFAR 10\n",
        "(x_train, y_train), (\n",
        "    x_test, y_test\n",
        ") = tf.keras.datasets.cifar10.load_data()\n",
        "x_train = x_train.astype(\n",
        "    'float32'\n",
        ")/ 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "y_train = tf.keras.utils.to_categorical(\n",
        "    y_train, 10\n",
        ")\n",
        "y_test = tf.keras.utils.to_categorical"
      ],
      "metadata": {
        "id": "sgdiYNTvIcBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correction\n"
      ],
      "metadata": {
        "id": "q7ztXG2ALgjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (\n",
        "    x_test, y_test\n",
        ") = tf.keras.datasets.cifar10.load_data()\n",
        "x_train = x_train.astype(\n",
        "    'float32'\n",
        ")/ 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "y_train = tf.keras.utils.to_categorical(\n",
        "    y_train, 10) #-categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.RMSprop(1e-3),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['acc']\n",
        ")\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=64,\n",
        "          epochs=1,\n",
        "          validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY2eNp1qsYO4",
        "outputId": "aaf9de0d-7a1e-4208-8e96-93435a63039e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n",
            "625/625 [==============================] - 36s 40ms/step - loss: 2.1882 - acc: 0.4322 - val_loss: 1.7649 - val_acc: 0.4128\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f22aca4f250>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GKMog0AtMrWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time Series"
      ],
      "metadata": {
        "id": "X_eQR74gzwxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "f = open(filename, 'rb').read()<br>\n",
        "data = f.decode().split('\\n')<br>\n",
        "\n",
        "Make an array of arrays of size 50.<br>\n",
        "Shuffle the data.<br>\n",
        "Separate training and test sets.<br>"
      ],
      "metadata": {
        "id": "PotHSjdrz1Rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = '/content/sinwave.csv' #/content/sinwave.csv\n",
        "# correction\n",
        "import numpy as np\n",
        "\n",
        "# lecture des donnees et découpage en sequence de seq_len elements\n",
        "def load_data(filename, seq_len):\n",
        "  f = open(filename, 'rb').read()\n",
        "  data = np.array(f.decode().split('\\n'), dtype = np.float32)\n",
        "\n",
        "  sequence_length = seq_len + 1\n",
        "  result = []\n",
        "  for index in range(len(data) - sequence_length):\n",
        "    result.append(data[index: index + sequence_length])\n",
        "\n",
        "  result = np.array(result)\n",
        "\n",
        "  np.random.shuffle(result)\n",
        "  row = round(0.9 * result.shape[0])\n",
        "  train = result[:int(row), :] # 90 % des exemples pour l'apprentissage\n",
        "  x_train = train[:, :-1] # On prend les séquences jusqu'à l'avant dernier élément\n",
        "  y_train = train[:, -1] # On prend le dernier élément comme sortie à apprendre\n",
        "  x_test = result[int(row):, :-1]\n",
        "  y_test = result[int(row):, -1]\n",
        "\n",
        "  # on transforme en un tenseur de dimension 3 avec une seule feature\n",
        "  x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
        "  x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
        "\n",
        "  return x_train, y_train, x_test, y_test\n",
        "\n",
        "load_data('sinwave.csv', 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRJZXd7szy03",
        "outputId": "92fcdded-4289-4d74-a0ab-50e36c27034e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[[-0.12006371],\n",
              "         [-0.1821631 ],\n",
              "         [-0.24354357],\n",
              "         ...,\n",
              "         [-0.06808878],\n",
              "         [-0.00530962],\n",
              "         [ 0.05749049]],\n",
              " \n",
              "        [[ 0.9976793 ],\n",
              "         [ 0.99998593],\n",
              "         [ 0.99834603],\n",
              "         ...,\n",
              "         [-0.96724904],\n",
              "         [-0.9812785 ],\n",
              "         [-0.9914352 ]],\n",
              " \n",
              "        [[-0.80588466],\n",
              "         [-0.84147096],\n",
              "         [-0.8737364 ],\n",
              "         ...,\n",
              "         [ 0.6806669 ],\n",
              "         [ 0.7253237 ],\n",
              "         [ 0.7671179 ]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[-0.7671179 ],\n",
              "         [-0.80588466],\n",
              "         [-0.84147096],\n",
              "         ...,\n",
              "         [ 0.63332385],\n",
              "         [ 0.6806669 ],\n",
              "         [ 0.7253237 ]],\n",
              " \n",
              "        [[ 0.00530962],\n",
              "         [-0.05749049],\n",
              "         [-0.12006371],\n",
              "         ...,\n",
              "         [-0.19259425],\n",
              "         [-0.13059922],\n",
              "         [-0.06808878]],\n",
              " \n",
              "        [[ 0.48639974],\n",
              "         [ 0.43057758],\n",
              "         [ 0.3730561 ],\n",
              "         ...,\n",
              "         [-0.64150614],\n",
              "         [-0.59207255],\n",
              "         [-0.5403023 ]]], dtype=float32),\n",
              " array([ 0.12006371, -0.9976793 ,  0.80588466, ...,  0.7671179 ,\n",
              "        -0.00530962, -0.48639974], dtype=float32),\n",
              " array([[[ 0.3730561 ],\n",
              "         [ 0.3140624 ],\n",
              "         [ 0.25382918],\n",
              "         ...,\n",
              "         [-0.5403023 ],\n",
              "         [-0.48639974],\n",
              "         [-0.43057758]],\n",
              " \n",
              "        [[ 0.7671179 ],\n",
              "         [ 0.80588466],\n",
              "         [ 0.84147096],\n",
              "         ...,\n",
              "         [-0.63332385],\n",
              "         [-0.6806669 ],\n",
              "         [-0.7253237 ]],\n",
              " \n",
              "        [[ 0.8737364 ],\n",
              "         [ 0.90255356],\n",
              "         [ 0.92780876],\n",
              "         ...,\n",
              "         [-0.7671179 ],\n",
              "         [-0.80588466],\n",
              "         [-0.84147096]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[-0.6806669 ],\n",
              "         [-0.7253237 ],\n",
              "         [-0.7671179 ],\n",
              "         ...,\n",
              "         [ 0.5313362 ],\n",
              "         [ 0.5834814 ],\n",
              "         [ 0.63332385]],\n",
              " \n",
              "        [[ 0.5834814 ],\n",
              "         [ 0.63332385],\n",
              "         [ 0.6806669 ],\n",
              "         ...,\n",
              "         [-0.420969  ],\n",
              "         [-0.47709402],\n",
              "         [-0.5313362 ]],\n",
              " \n",
              "        [[ 0.7253237 ],\n",
              "         [ 0.7671179 ],\n",
              "         [ 0.80588466],\n",
              "         ...,\n",
              "         [-0.5834814 ],\n",
              "         [-0.63332385],\n",
              "         [-0.6806669 ]]], dtype=float32),\n",
              " array([-0.3730561 , -0.7671179 , -0.8737364 ,  0.92780876, -0.9914352 ,\n",
              "         0.6806669 , -0.5403023 ,  0.73259306,  0.8737364 , -0.5403023 ,\n",
              "        -0.80588466, -0.92780876,  0.8737364 ,  0.9914352 , -0.5403023 ,\n",
              "        -0.99998593,  0.43057758,  0.25382918, -0.90707505,  0.96724904,\n",
              "        -0.24354357, -0.99834603, -0.47709402,  0.96988994,  0.9832683 ,\n",
              "         0.87885225, -0.05749049,  0.3140624 ,  0.9927662 , -0.90707505,\n",
              "         0.84716105,  0.12006371, -0.9976793 , -0.90707505,  0.73259306,\n",
              "        -0.84147096,  0.5403023 , -0.92780876, -0.94940233,  0.3140624 ,\n",
              "         0.688408  ,  0.80588466,  0.12006371,  0.9927662 , -0.96988994,\n",
              "         0.80588466,  0.92780876,  0.3039629 , -0.7671179 ,  0.3730561 ,\n",
              "         0.12006371,  0.5313362 , -0.77388686, -0.48639974, -0.96724904,\n",
              "        -0.99998593, -0.77388686, -0.73259306,  0.3140624 ,  0.8121265 ,\n",
              "         0.99998593, -0.24354357,  0.84147096,  0.420969  ,  0.80588466,\n",
              "        -0.420969  ,  0.13059922, -0.3140624 , -0.9812785 ,  0.84147096,\n",
              "         0.12006371, -0.9976793 ,  0.7253237 , -0.80588466, -0.5834814 ,\n",
              "         0.5313362 ,  0.420969  , -0.90707505, -0.43057758,  0.9914352 ,\n",
              "         0.7671179 ,  0.12006371, -0.90707505, -0.8121265 , -0.5403023 ,\n",
              "        -0.688408  ,  0.6806669 ,  0.6806669 ,  0.3730561 , -0.47709402,\n",
              "        -0.84716105, -0.19259425,  0.77388686,  0.92780876,  0.19259425,\n",
              "        -0.3140624 , -0.96988994, -0.87885225,  0.47709402,  0.96988994,\n",
              "         0.84147096,  0.5313362 , -0.84716105, -0.7253237 , -0.90255356,\n",
              "         0.24354357,  0.9976793 ,  0.84147096,  0.96988994, -0.420969  ,\n",
              "        -0.688408  , -0.12006371,  0.3730561 ,  0.77388686, -0.99834603,\n",
              "         0.90707505, -0.80588466, -0.420969  ,  0.1821631 , -0.84147096,\n",
              "        -0.00530962,  0.96988994,  0.19259425,  0.6806669 , -0.19259425,\n",
              "         0.1821631 ,  0.96988994,  0.96988994, -0.12006371,  0.95268387,\n",
              "         0.688408  , -0.931718  , -0.8737364 , -0.3039629 ,  0.87885225,\n",
              "        -0.8121265 ,  0.99834603,  0.96988994, -0.64150614, -0.96988994,\n",
              "        -0.420969  ,  0.63332385,  0.95268387,  0.64150614,  0.8121265 ,\n",
              "        -0.99834603, -0.9976793 ,  0.3039629 ,  0.63332385,  0.6806669 ,\n",
              "         0.3039629 ,  0.99834603, -0.84716105, -0.59207255,  0.8737364 ,\n",
              "         0.3140624 ,  0.9812785 , -0.99998593, -0.9812785 ,  0.90707505,\n",
              "         0.06808878, -0.12006371,  0.9832683 , -0.48639974,  0.90707505,\n",
              "        -0.9976793 , -0.3631826 , -0.9812785 , -0.24354357,  0.688408  ,\n",
              "         0.73259306, -0.25382918, -0.9812785 , -0.8737364 , -0.13059922,\n",
              "        -0.9927662 , -0.3039629 , -0.12006371, -0.19259425,  0.48639974,\n",
              "        -0.3039629 , -0.7671179 ,  0.84716105, -0.96724904, -0.84147096,\n",
              "        -0.47709402,  0.47709402,  0.5403023 , -0.95268387, -0.3039629 ,\n",
              "         0.77388686, -0.64150614, -0.9927662 , -0.5834814 , -0.43057758,\n",
              "         0.8737364 , -0.5403023 , -0.9914352 , -0.59207255,  0.92780876,\n",
              "        -0.931718  ,  0.3631826 , -0.6806669 ,  0.48639974, -0.99998593,\n",
              "         0.8737364 , -0.9832683 ,  0.9914352 ,  0.95268387,  0.90707505,\n",
              "         0.1821631 , -0.99998593,  0.3730561 ,  0.931718  ,  0.47709402,\n",
              "        -0.7671179 ,  0.1821631 , -0.43057758, -0.3140624 , -0.84716105,\n",
              "        -0.8121265 , -0.94940233, -0.5313362 , -0.47709402, -0.80588466,\n",
              "        -0.73259306, -0.92780876,  0.90707505, -0.96724904,  0.5313362 ,\n",
              "        -0.47709402,  0.5313362 ,  0.84147096,  0.7671179 ,  0.9976793 ,\n",
              "         0.99998593, -0.688408  , -0.19259425, -0.9927662 , -0.12006371,\n",
              "         0.43057758,  0.80588466,  0.9832683 ,  0.8737364 ,  0.9927662 ,\n",
              "        -0.00530962,  0.3039629 ,  0.99998593,  0.48639974, -0.94940233,\n",
              "        -0.9812785 , -0.19259425, -0.05749049, -0.12006371,  0.5313362 ,\n",
              "         0.3730561 ,  0.77388686, -0.19259425,  0.05749049,  0.64150614,\n",
              "        -0.688408  ,  0.7253237 , -0.90707505,  0.92780876, -0.96988994,\n",
              "         0.9976793 , -0.99834603, -0.420969  , -0.3730561 , -0.12006371,\n",
              "        -0.87885225,  0.96988994, -0.6806669 , -0.84147096, -0.12006371,\n",
              "         0.77388686,  0.3140624 , -0.90255356, -0.84716105,  0.931718  ,\n",
              "         0.3730561 ,  0.24354357, -0.7671179 ,  0.12006371,  0.7671179 ,\n",
              "        -0.64150614, -0.9927662 ,  0.94940233,  0.48639974,  0.9914352 ,\n",
              "        -0.06808878,  0.8121265 ,  0.9832683 , -0.84147096,  0.3730561 ,\n",
              "        -0.99998593, -0.3140624 , -0.94940233,  0.25382918,  0.90707505,\n",
              "         0.3631826 ,  0.48639974, -0.92780876,  0.96988994, -0.12006371,\n",
              "         0.64150614,  0.95268387, -0.24354357, -0.77388686,  0.12006371,\n",
              "        -0.87885225,  0.05749049,  0.1821631 , -0.24354357,  0.9914352 ,\n",
              "        -0.64150614,  0.06808878,  0.7253237 , -0.63332385, -0.73259306,\n",
              "        -0.5403023 , -0.80588466,  0.64150614,  0.3631826 , -0.92780876,\n",
              "        -0.00530962, -0.06808878,  0.48639974,  0.84147096,  0.3140624 ,\n",
              "         0.84716105,  0.90255356,  0.05749049, -0.25382918, -0.87885225,\n",
              "         0.64150614,  0.9812785 ,  0.6806669 ,  0.13059922, -0.96724904,\n",
              "         0.24354357, -0.59207255, -0.8121265 ,  0.92780876,  0.6806669 ,\n",
              "        -0.5313362 ,  0.931718  ,  0.9914352 , -0.73259306,  0.688408  ,\n",
              "         0.96724904, -0.3140624 ,  0.77388686,  0.3730561 , -0.95268387,\n",
              "        -0.77388686, -0.7671179 , -0.64150614,  0.05749049,  0.688408  ,\n",
              "         0.3631826 ,  0.688408  ,  0.3631826 , -0.3039629 , -0.8737364 ,\n",
              "        -0.59207255, -0.9812785 ,  0.96724904, -0.90707505, -0.25382918,\n",
              "         0.3039629 ,  0.99998593, -0.48639974,  0.77388686,  0.84147096,\n",
              "         0.99834603, -0.7671179 , -0.92780876,  0.931718  ,  0.931718  ,\n",
              "        -0.43057758,  0.73259306, -0.24354357,  0.9812785 ,  0.5834814 ,\n",
              "        -0.95268387, -0.688408  ,  0.931718  ,  0.00530962, -0.96988994,\n",
              "         0.1821631 , -0.24354357, -0.13059922,  0.5834814 , -0.7671179 ,\n",
              "        -0.5313362 , -0.99998593, -0.90255356,  0.9927662 , -0.80588466,\n",
              "         0.64150614, -0.84147096,  0.7253237 , -0.87885225,  0.5834814 ,\n",
              "         0.25382918,  0.96988994, -0.7253237 ,  0.00530962,  0.73259306,\n",
              "         0.931718  ,  0.6806669 ,  0.19259425, -0.84147096,  0.94940233,\n",
              "        -0.931718  , -0.9914352 , -0.99998593, -0.420969  ,  0.19259425,\n",
              "        -0.06808878, -0.94940233,  0.24354357, -0.3631826 ,  0.84716105,\n",
              "        -0.688408  ,  0.9812785 , -0.94940233,  0.688408  ,  0.13059922,\n",
              "        -0.96988994,  0.3730561 ,  0.3039629 , -0.3631826 ,  0.5403023 ,\n",
              "         0.90707505,  0.9832683 , -0.3039629 ,  0.420969  , -0.73259306,\n",
              "         0.99834603,  0.24354357, -0.12006371,  0.12006371,  0.8737364 ,\n",
              "        -0.84147096,  0.77388686, -0.05749049, -0.25382918,  0.95268387,\n",
              "         0.96724904,  0.1821631 , -0.73259306,  0.95268387, -0.73259306,\n",
              "        -0.94940233, -0.80588466, -0.420969  ,  0.96988994,  0.5313362 ,\n",
              "         0.420969  , -0.84147096, -0.43057758, -0.9927662 ,  0.7253237 ,\n",
              "         0.99998593,  0.1821631 , -0.96724904,  0.420969  , -0.9927662 ,\n",
              "        -0.5403023 , -0.80588466, -0.8121265 ,  0.7253237 ,  0.99998593,\n",
              "         0.5834814 , -0.96724904,  0.73259306,  0.25382918, -0.99998593,\n",
              "        -0.24354357,  0.48639974, -0.9914352 ,  0.9812785 , -0.3039629 ,\n",
              "        -0.8121265 ,  0.99834603, -0.73259306,  0.3631826 , -0.06808878,\n",
              "        -0.3730561 , -0.59207255,  0.6806669 , -0.5834814 , -0.7253237 ],\n",
              "       dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# faux\n",
        "lstm_model = tf.keras.models.Sequential(\n",
        "    [\n",
        "     tf.keras.layers.LSTM(\n",
        "         50, return_sequences=TRUE\n",
        "     ),\n",
        "     tf.keras.layers.LSTM(\n",
        "         100, return_sequences=TRUE\n",
        "     )\n",
        "     tf.keras.layers.Dense(\n",
        "         units=1, activation='relu'\n",
        "     )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "HW54X-go194-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install seq2seq-lstm\n",
        "#!pip install seq2seq-lstm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRWlmBm8BsYE",
        "outputId": "19cbdd85-33b1-42ce-cf40-245c81168552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting seq2seq-lstm\n",
            "  Downloading seq2seq-lstm-0.1.6.tar.gz (15 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/bd/87/29a7d393d953fc1f66d6af1edd13c19608046c818b68a060948372357ce5/seq2seq-lstm-0.1.6.tar.gz#sha256=8306d74474f160fb35bad046dd00a97f6019e7fb0ad12f53eae9554dfbc2ca30 (from https://pypi.org/simple/seq2seq-lstm/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading seq2seq_lstm-0.1.5-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.23.2 in /usr/local/lib/python3.7/dist-packages (from seq2seq-lstm) (1.0.2)\n",
            "Requirement already satisfied: tensorflow>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from seq2seq-lstm) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from seq2seq-lstm) (1.21.6)\n",
            "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from seq2seq-lstm) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.10.0->seq2seq-lstm) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.2->seq2seq-lstm) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.2->seq2seq-lstm) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.2->seq2seq-lstm) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.1->seq2seq-lstm) (57.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.1->seq2seq-lstm) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.1->seq2seq-lstm) (0.25.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.1->seq2seq-lstm) (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.1->seq2seq-lstm) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.1->seq2seq-lstm) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.1->seq2seq-lstm) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.1->seq2seq-lstm) (1.6.3)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.1->seq2seq-lstm) (0.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.1->seq2seq-lstm) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.1->seq2seq-lstm) (1.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.1->seq2seq-lstm) (1.44.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.1->seq2seq-lstm) (2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.1->seq2seq-lstm) (14.0.1)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.1->seq2seq-lstm) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.1->seq2seq-lstm) (0.2.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.1->seq2seq-lstm) (2.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.1->seq2seq-lstm) (4.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2.1->seq2seq-lstm) (0.37.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.1->seq2seq-lstm) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.1->seq2seq-lstm) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.1->seq2seq-lstm) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.1->seq2seq-lstm) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.1->seq2seq-lstm) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.1->seq2seq-lstm) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.1->seq2seq-lstm) (2.23.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.1->seq2seq-lstm) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.1->seq2seq-lstm) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.1->seq2seq-lstm) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.2.1->seq2seq-lstm) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.2.1->seq2seq-lstm) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.2.1->seq2seq-lstm) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.1->seq2seq-lstm) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2.1->seq2seq-lstm) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2.1->seq2seq-lstm) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2.1->seq2seq-lstm) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2.1->seq2seq-lstm) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.2.1->seq2seq-lstm) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly, seq2seq-lstm\n",
            "Successfully installed seq2seq-lstm-0.1.5 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import LSTM # import LSTM"
      ],
      "metadata": {
        "id": "6PfLiCwUAQys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout"
      ],
      "metadata": {
        "id": "4q_oFgjCCHJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "1wnynJEfCL2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(\n",
        "    LSTM(\n",
        "        50, input_shape=(\n",
        "            50, 1), return_sequences=True))\n",
        "model.add(\n",
        "    Dropout(\n",
        "        0.2\n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    LSTM(\n",
        "        100, return_sequences=False\n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    Dropout(\n",
        "        0.2\n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    Dense(\n",
        "        1\n",
        "    )\n",
        ")\n",
        "model.compile(\n",
        "    loss=\"mse\", optimizer=\"rmsprop\", metrics= [\n",
        "                                               'mae'\n",
        "    ]\n",
        ")\n",
        "X_train, y_train, X_test, y_test = load_data(\n",
        "    'sinwave.csv', 50\n",
        ")\n",
        "model.fit(\n",
        "    X_train, y_train, batch_size=512, epochs=1, validation_split=0.05\n",
        ")\n",
        "predict = model.predict(\n",
        "    X_test\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VshNFWPZ9WMo",
        "outputId": "710698a0-5ed6-4ae9-f15b-5e3bf139dc93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 9s 160ms/step - loss: 0.1812 - mae: 0.3570 - val_loss: 0.0239 - val_mae: 0.1401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Next, you'll extract partially overlapping sequences of length maxlen, one-hot encode them, and pack them in a 3D Numpy array x of shape (sequences, maxlen, unique_characters).\n",
        "*   Simultaneously, you'll prepare an array y containing the corresponding targets: the one_hot_encoded characters that come after each extracted sequence.\n",
        "\n"
      ],
      "metadata": {
        "id": "kzAMgioSCamQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QJ-kWS4fIELu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '/content/SFD_TDS_Extraction_Historique_Witron_V1.0.docx'"
      ],
      "metadata": {
        "id": "zvD7xLxtHIYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 60 #Maxlen = 60\n",
        "step = 3 #Step = 3\n",
        "sentences = [] #Sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step): #maxlen, step):\n",
        "  sentences.append(text[i: i + maxlen])\n",
        "  next_chars.append(text[i +maxlen])#])\n",
        "print('Number of sequences:',len(sentences))  \n",
        "chars = sorted(list(set(text)))\n",
        "print('Unique characters:',len(chars))\n",
        "char_indices = dict((char, chars.index(char)) for char in chars)\n",
        "print('Vectorization...')\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=bool) #np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=bool) #np.bool)\n",
        "for i, char in enumerate(sentences): #):\n",
        "  for t, char in enumerate(sentences):\n",
        "    x[i, t, char_indices[char]] = 1\n",
        "  y[i, char_indices[next_chars[i]]] = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Il6VEPyLAkl6",
        "outputId": "cac5990b-b7f5-4067-a03e-cce1998595d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sequences: 0\n",
            "Unique characters: 26\n",
            "Vectorization...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "model = keras.models.Sequential()\n",
        "model.add(\n",
        "    layers.LSTM(\n",
        "        128, input_shape=(\n",
        "            maxlen, len(\n",
        "                chars\n",
        "            )\n",
        "        )\n",
        "    )\n",
        ")\n",
        "model.add(\n",
        "    layers.Dense(\n",
        "        len(\n",
        "            chars\n",
        "        ), activation='softmax'\n",
        "    )\n",
        ")\n",
        "\n",
        "optimizer = keras.optimizers.RMSprop(\n",
        "    lr=0.01\n",
        ")\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy', optimizer=optimizer\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYyNI8xfHU8b",
        "outputId": "215c82f1-b641-4dbe-a944-8be9fb27cc54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(\n",
        "    preds, temperature=1.0):\n",
        "  preds = np.asarray(\n",
        "      preds).astype(\n",
        "          'float64'\n",
        "      )\n",
        "  preds = np.log(\n",
        "      preds)/ temperature\n",
        "  exp_preds = np.exp(\n",
        "      preds\n",
        "  )\n",
        "  preds = exp_preds = np.exp(preds)\n",
        "  probas = np.random.multinomial(\n",
        "      1, preds, 1)\n",
        "  return np.argmax(\n",
        "      probas)\n"
      ],
      "metadata": {
        "id": "iZYsbtVJJ6xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import sys #Sys\n",
        "for epoch in range(1, 60):\n",
        "  model.fit(\n",
        "      x, y, batch_size=128, epochs=1) \n",
        "  print(\n",
        "      'epoch', epoch\n",
        "  )\n",
        "  start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "  generated_text = text[start_index: start_index + maxlen]\n",
        "  for temperature in [\n",
        "                      0.2, 0.5, 1.0, 1.2]:\n",
        "                      sys.stdout.write(\n",
        "                          generated_text\n",
        "                      )\n",
        "                      print(\n",
        "                          '\\n\\n----------- temperature:', temperature\n",
        "                      )\n",
        "                      for i in range(400):\n",
        "                        sampled = np.zeros(\n",
        "                            (\n",
        "                                1, maxlen, len(\n",
        "                                    chars\n",
        "                                )\n",
        "                            )\n",
        "                        )\n",
        "                        for t, char in enumerate(\n",
        "                            generated_text):\n",
        "                          sampled[\n",
        "                                  0, t, char_indices[\n",
        "                                                     char\n",
        "                                  ]\n",
        "                          ] = 1.\n",
        "                        preds = model.predict(\n",
        "                            sampled, verbose=0\n",
        "                        )[0]\n",
        "                        next_index = sample(\n",
        "                            preds, temperature\n",
        "                        )\n",
        "                        next_char = chars[\n",
        "                                          next_index\n",
        "                        ]\n",
        "                        generated_text += next_char\n",
        "                        generated_text = generated_text[1:]\n",
        "                        sys.stdout.write(next_char)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "6IdCWQfWKl4B",
        "outputId": "2f5a7f38-9407-4ea2-fe5f-440f59f00dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-0edd30ce1d13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   model.fit(\n\u001b[0;32m----> 5\u001b[0;31m       x, y, batch_size=128, epochs=1) \n\u001b[0m\u001b[1;32m      6\u001b[0m   print(\n\u001b[1;32m      7\u001b[0m       \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m           raise ValueError('Unexpected result of `train_function` '\n\u001b[0m\u001b[1;32m   1396\u001b[0m                            \u001b[0;34m'(Empty logs). Please use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                            \u001b[0;34m'`Model.compile(..., run_eagerly=True)`, or '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sI_DrYwiOvb8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DeepLearning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNGWAapLJN+2Q/UXp3gPhR8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}