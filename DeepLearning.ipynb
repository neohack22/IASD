{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPe3BumiO2ntZ3dQPei9kVc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neohack22/IASD/blob/DeepLearning/DeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Implement a two layers XOR network (one hidden layer)\n",
        "*   Make it learn the XOR function with two inputs and one output.\n",
        "\n"
      ],
      "metadata": {
        "id": "XjADYmSoMmvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "SP80eKUtNe5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sz8T47CMPOXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhuKdTwtMjxF"
      },
      "outputs": [],
      "source": [
        "X=np.array([[0.,0.],[0.,1.],[1.,0.],[1.,1.]])\n",
        "y=np.array([[0.],[1.],[1.],[0.]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(self):\n",
        "  \"\"\"\n",
        "  Train a single layer perceptron.\n",
        "  \"\"\"\n",
        "  correct_counter = 0\n",
        "\n",
        "  for train, target in cycle(zip(self.X, self.y)):\n",
        "    # end if all points are correctly classified\n",
        "    if correct_counter = len(self.train_data):\n",
        "      break\n",
        "\n",
        "    output = self.classify(train)\n",
        "    self.node_val = train\n",
        "\n",
        "    if output == target:\n",
        "      correct_counter += 1\n",
        "    else:\n",
        "      # if incorrectly classified, update and reset correct_counter\n",
        "      self.update_weights(target, output)\n",
        "      correct_counter = 0"
      ],
      "metadata": {
        "id": "FNoR5XncPZC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron:\n",
        "  \"\"\"\n",
        "  Create a perceptron.\n",
        "  train_data: a 4x2 matrix with the input data.\n",
        "  target: a 4x1 matrix w/ the perceptron's expected outputs\n",
        "  lr: the learning rate. Defaults to 0.01\n",
        "  input_nodes: the number of nodes in the input layer of the perceptron.\n",
        "  Should be equal to the second dimension of train_data.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, train_data, target, lr=0.01, input_nodes=2):\n",
        "    self.train_data = train_data\n",
        "    self.target = target\n",
        "    self.lr = lr\n",
        "    self.input_nodes = input_nodes\n",
        "\n",
        "    # randomly initialize the weights and set the biais to -1.\n",
        "    self.w = np.random.uniform(size=self.input_nodes)\n",
        "    self.b = -1\n",
        "\n",
        "    # node_val hold the values of each node at a given point of time.\n",
        "    self.node_val = np.zeroes(self.input_nodes)\n",
        "\n",
        "  def _gradient(self, node, exp, output):\n",
        "    \"\"\"\n",
        "    Return the gradient for a weight.\n",
        "    This is the value of delta-w.\n",
        "    \"\"\"\n",
        "    return node * (exp - output)\n",
        "\n",
        "  def update_weights(self, exp, output):\n",
        "    \"\"\"\n",
        "    Update weights and bias based on their respective gradients\n",
        "    \"\"\"\n",
        "    for i in range(self.input_nodes):\n",
        "      self.w[i] += self.lr * self._gradient(self.node_val[i], exp, output)\n",
        "\n",
        "    # the value of the bias node ca be considered as being 1 and the weight\n",
        "    # between this node and the output node being self.b\n",
        "    self.b += self.lr * self._gradient(1, exp, output)\n",
        "\n",
        "  def forward(self, datapoint):\n",
        "    \"\"\"\n",
        "    One forward pass through the perceptron.\n",
        "    Implementation of \"wX + b\".\n",
        "    \"\"\"\n",
        "    return self.b + np.dot(self.w, datapoint)\n",
        "\n",
        "  def classify(self, datapoint):\n",
        "    \"\"\"\n",
        "    Return the class to which a datapoint belongs based on the perceptron's \n",
        "    output for that point.\n",
        "    \"\"\"\n",
        "    if.self.forward(datapoint) >= 0:\n",
        "      return 1\n",
        "\n",
        "    return 0"
      ],
      "metadata": {
        "id": "vX8L1O06SrTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _gradient(self, node, exp, output):\n",
        "  \"\"\"\n",
        "  Return the gradient for a weight.\n",
        "  This is the value of delta-w.\n",
        "  \"\"\"\n",
        "  return node * (exp - output)\n",
        "\n",
        "def update_weights(self, exp, output):\n",
        "  \"\"\"\n",
        "  Update weights and bias based on their respective gradients\n",
        "  \"\"\"\n",
        "  for i in range(self.input_nodes):\n",
        "    self.w[i] += self.lr * self._gradient(self.node_val[i], exp, output)\n",
        "\n",
        "  # the value of the bias node can be considered as being 1 and the weight\n",
        "  # between this node and the output node being self.b\n",
        "  self.b += self.lr * self._gradient(1, exp, output)\n",
        "\n",
        "def forward(self, datapoint):\n",
        "  \"\"\"\n",
        "  One forward pass throught the perceptron.\n",
        "  Implementation of \"wX + b\".\n",
        "  \"\"\"\n",
        "  return self.b + np.dot(self.w, datapoint)\n",
        "\n",
        "def classify(self, datapoint):\n",
        "  \"\"\"\n",
        "  Return the class to which a datapoint belongs based on\n",
        "  the perceptron's output for that point.\n",
        "  \"\"\"\n",
        "  if self.forward(datapoint) >= 0:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "eu_a872NQXCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_xor = Perceptron"
      ],
      "metadata": {
        "id": "vHM31dChSbyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7CHPYYPyWG4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# correction\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        " \n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        " \n",
        "\n",
        "import numpy as np\n",
        "X = np.array([[0.,0.],[0.,1.],[1.,0.],[1.,1.]])\n",
        "y = np.array([[0.],[1.],[1.],[0.]])"
      ],
      "metadata": {
        "id": "lkjUGxcXWIRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(8, input_dim=2))\n",
        "model.add(Activation('tanh'))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        " \n",
        "\n",
        "sgd = SGD(lr=0.1)\n",
        "model.compile(loss='mse', optimizer=sgd)\n",
        "model.fit(X, y, verbose=1, batch_size=1, epochs=1000)\n",
        "print(model.predict(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmLke7_OWN3P",
        "outputId": "3effe20a-b067-4e3a-9650-9f55cabe5024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "4/4 [==============================] - 1s 3ms/step - loss: 0.2935\n",
            "Epoch 2/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2879\n",
            "Epoch 3/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2841\n",
            "Epoch 4/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2812\n",
            "Epoch 5/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2785\n",
            "Epoch 6/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2774\n",
            "Epoch 7/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2764\n",
            "Epoch 8/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2749\n",
            "Epoch 9/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2748\n",
            "Epoch 10/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2741\n",
            "Epoch 11/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2723\n",
            "Epoch 12/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2725\n",
            "Epoch 13/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2720\n",
            "Epoch 14/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2715\n",
            "Epoch 15/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2711\n",
            "Epoch 16/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2705\n",
            "Epoch 17/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2700\n",
            "Epoch 18/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2697\n",
            "Epoch 19/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2692\n",
            "Epoch 20/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2688\n",
            "Epoch 21/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2679\n",
            "Epoch 22/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2678\n",
            "Epoch 23/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2675\n",
            "Epoch 24/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2667\n",
            "Epoch 25/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2669\n",
            "Epoch 26/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2665\n",
            "Epoch 27/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2662\n",
            "Epoch 28/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2657\n",
            "Epoch 29/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2654\n",
            "Epoch 30/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2651\n",
            "Epoch 31/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2643\n",
            "Epoch 32/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2643\n",
            "Epoch 33/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2640\n",
            "Epoch 34/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2633\n",
            "Epoch 35/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2634\n",
            "Epoch 36/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2632\n",
            "Epoch 37/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2627\n",
            "Epoch 38/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2621\n",
            "Epoch 39/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2620\n",
            "Epoch 40/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2613\n",
            "Epoch 41/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2614\n",
            "Epoch 42/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2609\n",
            "Epoch 43/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2605\n",
            "Epoch 44/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2604\n",
            "Epoch 45/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2594\n",
            "Epoch 46/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2596\n",
            "Epoch 47/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2593\n",
            "Epoch 48/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2591\n",
            "Epoch 49/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2581\n",
            "Epoch 50/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2578\n",
            "Epoch 51/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2582\n",
            "Epoch 52/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2577\n",
            "Epoch 53/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2568\n",
            "Epoch 54/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2571\n",
            "Epoch 55/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2562\n",
            "Epoch 56/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2564\n",
            "Epoch 57/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2556\n",
            "Epoch 58/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2558\n",
            "Epoch 59/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2551\n",
            "Epoch 60/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2545\n",
            "Epoch 61/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2546\n",
            "Epoch 62/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2542\n",
            "Epoch 63/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2534\n",
            "Epoch 64/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2534\n",
            "Epoch 65/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2532\n",
            "Epoch 66/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2523\n",
            "Epoch 67/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2524\n",
            "Epoch 68/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2515\n",
            "Epoch 69/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2516\n",
            "Epoch 70/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2508\n",
            "Epoch 71/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2508\n",
            "Epoch 72/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2502\n",
            "Epoch 73/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2494\n",
            "Epoch 74/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2495\n",
            "Epoch 75/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2490\n",
            "Epoch 76/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2481\n",
            "Epoch 77/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2481\n",
            "Epoch 78/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2475\n",
            "Epoch 79/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2471\n",
            "Epoch 80/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2466\n",
            "Epoch 81/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2460\n",
            "Epoch 82/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2454\n",
            "Epoch 83/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2449\n",
            "Epoch 84/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2440\n",
            "Epoch 85/1000\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.2434\n",
            "Epoch 86/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2432\n",
            "Epoch 87/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2428\n",
            "Epoch 88/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2421\n",
            "Epoch 89/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2416\n",
            "Epoch 90/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2409\n",
            "Epoch 91/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2399\n",
            "Epoch 92/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2397\n",
            "Epoch 93/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2390\n",
            "Epoch 94/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2377\n",
            "Epoch 95/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2376\n",
            "Epoch 96/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2363\n",
            "Epoch 97/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2362\n",
            "Epoch 98/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2353\n",
            "Epoch 99/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2346\n",
            "Epoch 100/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2338\n",
            "Epoch 101/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2325\n",
            "Epoch 102/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2323\n",
            "Epoch 103/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2314\n",
            "Epoch 104/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2299\n",
            "Epoch 105/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2296\n",
            "Epoch 106/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2287\n",
            "Epoch 107/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2273\n",
            "Epoch 108/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2269\n",
            "Epoch 109/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2259\n",
            "Epoch 110/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2250\n",
            "Epoch 111/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2234\n",
            "Epoch 112/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2226\n",
            "Epoch 113/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2215\n",
            "Epoch 114/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2210\n",
            "Epoch 115/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2195\n",
            "Epoch 116/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2187\n",
            "Epoch 117/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2176\n",
            "Epoch 118/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2165\n",
            "Epoch 119/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2153\n",
            "Epoch 120/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2140\n",
            "Epoch 121/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2128\n",
            "Epoch 122/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2115\n",
            "Epoch 123/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2104\n",
            "Epoch 124/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2084\n",
            "Epoch 125/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2076\n",
            "Epoch 126/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.2062\n",
            "Epoch 127/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2049\n",
            "Epoch 128/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2029\n",
            "Epoch 129/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2020\n",
            "Epoch 130/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.2006\n",
            "Epoch 131/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1987\n",
            "Epoch 132/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1972\n",
            "Epoch 133/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1962\n",
            "Epoch 134/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1942\n",
            "Epoch 135/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1932\n",
            "Epoch 136/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1913\n",
            "Epoch 137/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1900\n",
            "Epoch 138/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1885\n",
            "Epoch 139/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1867\n",
            "Epoch 140/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1846\n",
            "Epoch 141/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1829\n",
            "Epoch 142/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1816\n",
            "Epoch 143/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1799\n",
            "Epoch 144/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1777\n",
            "Epoch 145/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1765\n",
            "Epoch 146/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1748\n",
            "Epoch 147/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1730\n",
            "Epoch 148/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1709\n",
            "Epoch 149/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1697\n",
            "Epoch 150/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.1677\n",
            "Epoch 151/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1658\n",
            "Epoch 152/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1641\n",
            "Epoch 153/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1621\n",
            "Epoch 154/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1597\n",
            "Epoch 155/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1586\n",
            "Epoch 156/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1561\n",
            "Epoch 157/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1547\n",
            "Epoch 158/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1525\n",
            "Epoch 159/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1511\n",
            "Epoch 160/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1485\n",
            "Epoch 161/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1468\n",
            "Epoch 162/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1454\n",
            "Epoch 163/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1434\n",
            "Epoch 164/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1417\n",
            "Epoch 165/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1393\n",
            "Epoch 166/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1380\n",
            "Epoch 167/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1361\n",
            "Epoch 168/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1342\n",
            "Epoch 169/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1318\n",
            "Epoch 170/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1304\n",
            "Epoch 171/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1287\n",
            "Epoch 172/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1268\n",
            "Epoch 173/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1249\n",
            "Epoch 174/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1231\n",
            "Epoch 175/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1210\n",
            "Epoch 176/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1196\n",
            "Epoch 177/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1177\n",
            "Epoch 178/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1161\n",
            "Epoch 179/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1139\n",
            "Epoch 180/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1123\n",
            "Epoch 181/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1108\n",
            "Epoch 182/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1092\n",
            "Epoch 183/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1071\n",
            "Epoch 184/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.1059\n",
            "Epoch 185/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1039\n",
            "Epoch 186/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1026\n",
            "Epoch 187/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1009\n",
            "Epoch 188/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0990\n",
            "Epoch 189/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0979\n",
            "Epoch 190/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0963\n",
            "Epoch 191/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0948\n",
            "Epoch 192/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0929\n",
            "Epoch 193/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0917\n",
            "Epoch 194/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0903\n",
            "Epoch 195/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0886\n",
            "Epoch 196/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0875\n",
            "Epoch 197/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0861\n",
            "Epoch 198/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0847\n",
            "Epoch 199/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0833\n",
            "Epoch 200/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0820\n",
            "Epoch 201/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0805\n",
            "Epoch 202/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0794\n",
            "Epoch 203/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0779\n",
            "Epoch 204/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0769\n",
            "Epoch 205/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0757\n",
            "Epoch 206/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0745\n",
            "Epoch 207/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0733\n",
            "Epoch 208/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0721\n",
            "Epoch 209/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0710\n",
            "Epoch 210/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0699\n",
            "Epoch 211/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0688\n",
            "Epoch 212/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0677\n",
            "Epoch 213/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0665\n",
            "Epoch 214/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0656\n",
            "Epoch 215/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0645\n",
            "Epoch 216/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0636\n",
            "Epoch 217/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0626\n",
            "Epoch 218/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0617\n",
            "Epoch 219/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0607\n",
            "Epoch 220/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0598\n",
            "Epoch 221/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0588\n",
            "Epoch 222/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0579\n",
            "Epoch 223/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0571\n",
            "Epoch 224/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0562\n",
            "Epoch 225/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0554\n",
            "Epoch 226/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0545\n",
            "Epoch 227/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0537\n",
            "Epoch 228/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0530\n",
            "Epoch 229/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0521\n",
            "Epoch 230/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0514\n",
            "Epoch 231/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0507\n",
            "Epoch 232/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0500\n",
            "Epoch 233/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0493\n",
            "Epoch 234/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0486\n",
            "Epoch 235/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0479\n",
            "Epoch 236/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0472\n",
            "Epoch 237/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0465\n",
            "Epoch 238/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0458\n",
            "Epoch 239/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0453\n",
            "Epoch 240/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0446\n",
            "Epoch 241/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0440\n",
            "Epoch 242/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0434\n",
            "Epoch 243/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0428\n",
            "Epoch 244/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0422\n",
            "Epoch 245/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0417\n",
            "Epoch 246/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0411\n",
            "Epoch 247/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0406\n",
            "Epoch 248/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0401\n",
            "Epoch 249/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0395\n",
            "Epoch 250/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0390\n",
            "Epoch 251/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0385\n",
            "Epoch 252/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0380\n",
            "Epoch 253/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0375\n",
            "Epoch 254/1000\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0371\n",
            "Epoch 255/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0366\n",
            "Epoch 256/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0361\n",
            "Epoch 257/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0357\n",
            "Epoch 258/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0352\n",
            "Epoch 259/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0348\n",
            "Epoch 260/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0343\n",
            "Epoch 261/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0340\n",
            "Epoch 262/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0336\n",
            "Epoch 263/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0331\n",
            "Epoch 264/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0327\n",
            "Epoch 265/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0324\n",
            "Epoch 266/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0320\n",
            "Epoch 267/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0316\n",
            "Epoch 268/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0312\n",
            "Epoch 269/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0309\n",
            "Epoch 270/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0305\n",
            "Epoch 271/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0302\n",
            "Epoch 272/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0298\n",
            "Epoch 273/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0295\n",
            "Epoch 274/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0291\n",
            "Epoch 275/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0288\n",
            "Epoch 276/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0285\n",
            "Epoch 277/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0282\n",
            "Epoch 278/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0279\n",
            "Epoch 279/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0276\n",
            "Epoch 280/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0273\n",
            "Epoch 281/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0270\n",
            "Epoch 282/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0267\n",
            "Epoch 283/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0264\n",
            "Epoch 284/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0262\n",
            "Epoch 285/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0259\n",
            "Epoch 286/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0256\n",
            "Epoch 287/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0254\n",
            "Epoch 288/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0251\n",
            "Epoch 289/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0248\n",
            "Epoch 290/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0246\n",
            "Epoch 291/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0243\n",
            "Epoch 292/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0241\n",
            "Epoch 293/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0238\n",
            "Epoch 294/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0236\n",
            "Epoch 295/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0234\n",
            "Epoch 296/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0232\n",
            "Epoch 297/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0229\n",
            "Epoch 298/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0227\n",
            "Epoch 299/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0225\n",
            "Epoch 300/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0223\n",
            "Epoch 301/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0221\n",
            "Epoch 302/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0219\n",
            "Epoch 303/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0217\n",
            "Epoch 304/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0215\n",
            "Epoch 305/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0213\n",
            "Epoch 306/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0211\n",
            "Epoch 307/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0209\n",
            "Epoch 308/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0207\n",
            "Epoch 309/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0205\n",
            "Epoch 310/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0203\n",
            "Epoch 311/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0201\n",
            "Epoch 312/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0200\n",
            "Epoch 313/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0198\n",
            "Epoch 314/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0196\n",
            "Epoch 315/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0194\n",
            "Epoch 316/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0193\n",
            "Epoch 317/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0191\n",
            "Epoch 318/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0189\n",
            "Epoch 319/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0188\n",
            "Epoch 320/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0186\n",
            "Epoch 321/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0185\n",
            "Epoch 322/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0183\n",
            "Epoch 323/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0182\n",
            "Epoch 324/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0180\n",
            "Epoch 325/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0179\n",
            "Epoch 326/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0177\n",
            "Epoch 327/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0176\n",
            "Epoch 328/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0174\n",
            "Epoch 329/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0173\n",
            "Epoch 330/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0171\n",
            "Epoch 331/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0170\n",
            "Epoch 332/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0169\n",
            "Epoch 333/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0167\n",
            "Epoch 334/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0166\n",
            "Epoch 335/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0165\n",
            "Epoch 336/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0164\n",
            "Epoch 337/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0162\n",
            "Epoch 338/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0161\n",
            "Epoch 339/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0160\n",
            "Epoch 340/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0159\n",
            "Epoch 341/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0157\n",
            "Epoch 342/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0156\n",
            "Epoch 343/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0155\n",
            "Epoch 344/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0154\n",
            "Epoch 345/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0153\n",
            "Epoch 346/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0152\n",
            "Epoch 347/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0151\n",
            "Epoch 348/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0149\n",
            "Epoch 349/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0148\n",
            "Epoch 350/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0147\n",
            "Epoch 351/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0146\n",
            "Epoch 352/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0145\n",
            "Epoch 353/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0144\n",
            "Epoch 354/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0143\n",
            "Epoch 355/1000\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0142\n",
            "Epoch 356/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0141\n",
            "Epoch 357/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0140\n",
            "Epoch 358/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0139\n",
            "Epoch 359/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0138\n",
            "Epoch 360/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0137\n",
            "Epoch 361/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0136\n",
            "Epoch 362/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0135\n",
            "Epoch 363/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0135\n",
            "Epoch 364/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0134\n",
            "Epoch 365/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0133\n",
            "Epoch 366/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0132\n",
            "Epoch 367/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0131\n",
            "Epoch 368/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0130\n",
            "Epoch 369/1000\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0129\n",
            "Epoch 370/1000\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 371/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0128\n",
            "Epoch 372/1000\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 373/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0126\n",
            "Epoch 374/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0125\n",
            "Epoch 375/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0124\n",
            "Epoch 376/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0124\n",
            "Epoch 377/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0123\n",
            "Epoch 378/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0122\n",
            "Epoch 379/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0121\n",
            "Epoch 380/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0120\n",
            "Epoch 381/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0120\n",
            "Epoch 382/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0119\n",
            "Epoch 383/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0118\n",
            "Epoch 384/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0118\n",
            "Epoch 385/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0117\n",
            "Epoch 386/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0116\n",
            "Epoch 387/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0115\n",
            "Epoch 388/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0115\n",
            "Epoch 389/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0114\n",
            "Epoch 390/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0113\n",
            "Epoch 391/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0113\n",
            "Epoch 392/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0112\n",
            "Epoch 393/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0111\n",
            "Epoch 394/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0111\n",
            "Epoch 395/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0110\n",
            "Epoch 396/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0109\n",
            "Epoch 397/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0109\n",
            "Epoch 398/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0108\n",
            "Epoch 399/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0108\n",
            "Epoch 400/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0107\n",
            "Epoch 401/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0106\n",
            "Epoch 402/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0106\n",
            "Epoch 403/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0105\n",
            "Epoch 404/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0105\n",
            "Epoch 405/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0104\n",
            "Epoch 406/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0103\n",
            "Epoch 407/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0103\n",
            "Epoch 408/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0102\n",
            "Epoch 409/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0102\n",
            "Epoch 410/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0101\n",
            "Epoch 411/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0101\n",
            "Epoch 412/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0100\n",
            "Epoch 413/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0099\n",
            "Epoch 414/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0099\n",
            "Epoch 415/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0098\n",
            "Epoch 416/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0098\n",
            "Epoch 417/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0097\n",
            "Epoch 418/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0097\n",
            "Epoch 419/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0096\n",
            "Epoch 420/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0096\n",
            "Epoch 421/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0095\n",
            "Epoch 422/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0095\n",
            "Epoch 423/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0094\n",
            "Epoch 424/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0094\n",
            "Epoch 425/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0093\n",
            "Epoch 426/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0093\n",
            "Epoch 427/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0092\n",
            "Epoch 428/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0092\n",
            "Epoch 429/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0092\n",
            "Epoch 430/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0091\n",
            "Epoch 431/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0091\n",
            "Epoch 432/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0090\n",
            "Epoch 433/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0090\n",
            "Epoch 434/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0089\n",
            "Epoch 435/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0089\n",
            "Epoch 436/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0088\n",
            "Epoch 437/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0088\n",
            "Epoch 438/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0088\n",
            "Epoch 439/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0087\n",
            "Epoch 440/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0087\n",
            "Epoch 441/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0086\n",
            "Epoch 442/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0086\n",
            "Epoch 443/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0085\n",
            "Epoch 444/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0085\n",
            "Epoch 445/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0085\n",
            "Epoch 446/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0084\n",
            "Epoch 447/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0084\n",
            "Epoch 448/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0083\n",
            "Epoch 449/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0083\n",
            "Epoch 450/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0083\n",
            "Epoch 451/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0082\n",
            "Epoch 452/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0082\n",
            "Epoch 453/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0081\n",
            "Epoch 454/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0081\n",
            "Epoch 455/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0081\n",
            "Epoch 456/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0080\n",
            "Epoch 457/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0080\n",
            "Epoch 458/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0080\n",
            "Epoch 459/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0079\n",
            "Epoch 460/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0079\n",
            "Epoch 461/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0079\n",
            "Epoch 462/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0078\n",
            "Epoch 463/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0078\n",
            "Epoch 464/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0078\n",
            "Epoch 465/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0077\n",
            "Epoch 466/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0077\n",
            "Epoch 467/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0076\n",
            "Epoch 468/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0076\n",
            "Epoch 469/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0076\n",
            "Epoch 470/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0075\n",
            "Epoch 471/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0075\n",
            "Epoch 472/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0075\n",
            "Epoch 473/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0075\n",
            "Epoch 474/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0074\n",
            "Epoch 475/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0074\n",
            "Epoch 476/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0074\n",
            "Epoch 477/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0073\n",
            "Epoch 478/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0073\n",
            "Epoch 479/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0073\n",
            "Epoch 480/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0072\n",
            "Epoch 481/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0072\n",
            "Epoch 482/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0072\n",
            "Epoch 483/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0071\n",
            "Epoch 484/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0071\n",
            "Epoch 485/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0071\n",
            "Epoch 486/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0071\n",
            "Epoch 487/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0070\n",
            "Epoch 488/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0070\n",
            "Epoch 489/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0070\n",
            "Epoch 490/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0069\n",
            "Epoch 491/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0069\n",
            "Epoch 492/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0069\n",
            "Epoch 493/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0069\n",
            "Epoch 494/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0068\n",
            "Epoch 495/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0068\n",
            "Epoch 496/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0068\n",
            "Epoch 497/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0067\n",
            "Epoch 498/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0067\n",
            "Epoch 499/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0067\n",
            "Epoch 500/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0067\n",
            "Epoch 501/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0066\n",
            "Epoch 502/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0066\n",
            "Epoch 503/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0066\n",
            "Epoch 504/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0066\n",
            "Epoch 505/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0065\n",
            "Epoch 506/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0065\n",
            "Epoch 507/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0065\n",
            "Epoch 508/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0065\n",
            "Epoch 509/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0064\n",
            "Epoch 510/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0064\n",
            "Epoch 511/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0064\n",
            "Epoch 512/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0064\n",
            "Epoch 513/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0063\n",
            "Epoch 514/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0063\n",
            "Epoch 515/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0063\n",
            "Epoch 516/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0063\n",
            "Epoch 517/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0062\n",
            "Epoch 518/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0062\n",
            "Epoch 519/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0062\n",
            "Epoch 520/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0062\n",
            "Epoch 521/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0062\n",
            "Epoch 522/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0061\n",
            "Epoch 523/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0061\n",
            "Epoch 524/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0061\n",
            "Epoch 525/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0061\n",
            "Epoch 526/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0060\n",
            "Epoch 527/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0060\n",
            "Epoch 528/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0060\n",
            "Epoch 529/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0060\n",
            "Epoch 530/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0060\n",
            "Epoch 531/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0059\n",
            "Epoch 532/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0059\n",
            "Epoch 533/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0059\n",
            "Epoch 534/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0059\n",
            "Epoch 535/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0058\n",
            "Epoch 536/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0058\n",
            "Epoch 537/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0058\n",
            "Epoch 538/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0058\n",
            "Epoch 539/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0058\n",
            "Epoch 540/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0057\n",
            "Epoch 541/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0057\n",
            "Epoch 542/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0057\n",
            "Epoch 543/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0057\n",
            "Epoch 544/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0057\n",
            "Epoch 545/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0056\n",
            "Epoch 546/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0056\n",
            "Epoch 547/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0056\n",
            "Epoch 548/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0056\n",
            "Epoch 549/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0056\n",
            "Epoch 550/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0056\n",
            "Epoch 551/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0055\n",
            "Epoch 552/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0055\n",
            "Epoch 553/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0055\n",
            "Epoch 554/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0055\n",
            "Epoch 555/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0055\n",
            "Epoch 556/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0054\n",
            "Epoch 557/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0054\n",
            "Epoch 558/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0054\n",
            "Epoch 559/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0054\n",
            "Epoch 560/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0054\n",
            "Epoch 561/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0054\n",
            "Epoch 562/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0053\n",
            "Epoch 563/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0053\n",
            "Epoch 564/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0053\n",
            "Epoch 565/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0053\n",
            "Epoch 566/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0053\n",
            "Epoch 567/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0052\n",
            "Epoch 568/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0052\n",
            "Epoch 569/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0052\n",
            "Epoch 570/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0052\n",
            "Epoch 571/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0052\n",
            "Epoch 572/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0052\n",
            "Epoch 573/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0051\n",
            "Epoch 574/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0051\n",
            "Epoch 575/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0051\n",
            "Epoch 576/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0051\n",
            "Epoch 577/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0051\n",
            "Epoch 578/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0051\n",
            "Epoch 579/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0050\n",
            "Epoch 580/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0050\n",
            "Epoch 581/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0050\n",
            "Epoch 582/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0050\n",
            "Epoch 583/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0050\n",
            "Epoch 584/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0050\n",
            "Epoch 585/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0050\n",
            "Epoch 586/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0049\n",
            "Epoch 587/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0049\n",
            "Epoch 588/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0049\n",
            "Epoch 589/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0049\n",
            "Epoch 590/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0049\n",
            "Epoch 591/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0049\n",
            "Epoch 592/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0048\n",
            "Epoch 593/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0048\n",
            "Epoch 594/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0048\n",
            "Epoch 595/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0048\n",
            "Epoch 596/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0048\n",
            "Epoch 597/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0048\n",
            "Epoch 598/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0048\n",
            "Epoch 599/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0047\n",
            "Epoch 600/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0047\n",
            "Epoch 601/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0047\n",
            "Epoch 602/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0047\n",
            "Epoch 603/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0047\n",
            "Epoch 604/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0047\n",
            "Epoch 605/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0047\n",
            "Epoch 606/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0047\n",
            "Epoch 607/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0046\n",
            "Epoch 608/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0046\n",
            "Epoch 609/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0046\n",
            "Epoch 610/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0046\n",
            "Epoch 611/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0046\n",
            "Epoch 612/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0046\n",
            "Epoch 613/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0046\n",
            "Epoch 614/1000\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.0045\n",
            "Epoch 615/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0045\n",
            "Epoch 616/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0045\n",
            "Epoch 617/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0045\n",
            "Epoch 618/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0045\n",
            "Epoch 619/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0045\n",
            "Epoch 620/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0045\n",
            "Epoch 621/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0045\n",
            "Epoch 622/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0044\n",
            "Epoch 623/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0044\n",
            "Epoch 624/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0044\n",
            "Epoch 625/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0044\n",
            "Epoch 626/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0044\n",
            "Epoch 627/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0044\n",
            "Epoch 628/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0044\n",
            "Epoch 629/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0044\n",
            "Epoch 630/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0043\n",
            "Epoch 631/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0043\n",
            "Epoch 632/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0043\n",
            "Epoch 633/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0043\n",
            "Epoch 634/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0043\n",
            "Epoch 635/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0043\n",
            "Epoch 636/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0043\n",
            "Epoch 637/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0043\n",
            "Epoch 638/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0042\n",
            "Epoch 639/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0042\n",
            "Epoch 640/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0042\n",
            "Epoch 641/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0042\n",
            "Epoch 642/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0042\n",
            "Epoch 643/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0042\n",
            "Epoch 644/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0042\n",
            "Epoch 645/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0042\n",
            "Epoch 646/1000\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.0042\n",
            "Epoch 647/1000\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.0041\n",
            "Epoch 648/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0041\n",
            "Epoch 649/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0041\n",
            "Epoch 650/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0041\n",
            "Epoch 651/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0041\n",
            "Epoch 652/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0041\n",
            "Epoch 653/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0041\n",
            "Epoch 654/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0041\n",
            "Epoch 655/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0041\n",
            "Epoch 656/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0040\n",
            "Epoch 657/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0040\n",
            "Epoch 658/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0040\n",
            "Epoch 659/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0040\n",
            "Epoch 660/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0040\n",
            "Epoch 661/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0040\n",
            "Epoch 662/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0040\n",
            "Epoch 663/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0040\n",
            "Epoch 664/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0040\n",
            "Epoch 665/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0040\n",
            "Epoch 666/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0039\n",
            "Epoch 667/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0039\n",
            "Epoch 668/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0039\n",
            "Epoch 669/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0039\n",
            "Epoch 670/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0039\n",
            "Epoch 671/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0039\n",
            "Epoch 672/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0039\n",
            "Epoch 673/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0039\n",
            "Epoch 674/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0039\n",
            "Epoch 675/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0039\n",
            "Epoch 676/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0038\n",
            "Epoch 677/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0038\n",
            "Epoch 678/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0038\n",
            "Epoch 679/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0038\n",
            "Epoch 680/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0038\n",
            "Epoch 681/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0038\n",
            "Epoch 682/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0038\n",
            "Epoch 683/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0038\n",
            "Epoch 684/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0038\n",
            "Epoch 685/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0038\n",
            "Epoch 686/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0038\n",
            "Epoch 687/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 688/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 689/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 690/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 691/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 692/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 693/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 694/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 695/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 696/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 697/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 698/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0036\n",
            "Epoch 699/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 700/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 701/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 702/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 703/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 704/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0036\n",
            "Epoch 705/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 706/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0036\n",
            "Epoch 707/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 708/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 709/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 710/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 711/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 712/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0035\n",
            "Epoch 713/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 714/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 715/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 716/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 717/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 718/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 719/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 720/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 721/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 722/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 723/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 724/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0034\n",
            "Epoch 725/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 726/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 727/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 728/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 729/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 730/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 731/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 732/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 733/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 734/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 735/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0033\n",
            "Epoch 736/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0033\n",
            "Epoch 737/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0033\n",
            "Epoch 738/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0033\n",
            "Epoch 739/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0033\n",
            "Epoch 740/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0033\n",
            "Epoch 741/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0033\n",
            "Epoch 742/1000\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0033\n",
            "Epoch 743/1000\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.0033\n",
            "Epoch 744/1000\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0033\n",
            "Epoch 745/1000\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0033\n",
            "Epoch 746/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0033\n",
            "Epoch 747/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0033\n",
            "Epoch 748/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0033\n",
            "Epoch 749/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0032\n",
            "Epoch 750/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0032\n",
            "Epoch 751/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0032\n",
            "Epoch 752/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0032\n",
            "Epoch 753/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0032\n",
            "Epoch 754/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0032\n",
            "Epoch 755/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0032\n",
            "Epoch 756/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0032\n",
            "Epoch 757/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0032\n",
            "Epoch 758/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0032\n",
            "Epoch 759/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0032\n",
            "Epoch 760/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0032\n",
            "Epoch 761/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0032\n",
            "Epoch 762/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0032\n",
            "Epoch 763/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 764/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 765/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 766/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 767/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 768/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0031\n",
            "Epoch 769/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0031\n",
            "Epoch 770/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 771/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 772/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 773/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 774/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 775/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 776/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0031\n",
            "Epoch 777/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0031\n",
            "Epoch 778/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0031\n",
            "Epoch 779/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0030\n",
            "Epoch 780/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 781/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0030\n",
            "Epoch 782/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0030\n",
            "Epoch 783/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0030\n",
            "Epoch 784/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 785/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 786/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 787/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 788/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 789/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 790/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 791/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 792/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 793/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 794/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0030\n",
            "Epoch 795/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 796/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0029\n",
            "Epoch 797/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 798/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0029\n",
            "Epoch 799/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0029\n",
            "Epoch 800/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 801/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0029\n",
            "Epoch 802/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0029\n",
            "Epoch 803/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0029\n",
            "Epoch 804/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0029\n",
            "Epoch 805/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 806/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 807/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0029\n",
            "Epoch 808/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0029\n",
            "Epoch 809/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 810/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0029\n",
            "Epoch 811/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 812/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0029\n",
            "Epoch 813/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 814/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 815/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0028\n",
            "Epoch 816/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 817/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 818/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0028\n",
            "Epoch 819/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 820/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 821/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 822/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 823/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 824/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 825/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 826/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0028\n",
            "Epoch 827/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0028\n",
            "Epoch 828/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0028\n",
            "Epoch 829/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 830/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0028\n",
            "Epoch 831/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 832/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0027\n",
            "Epoch 833/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 834/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 835/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0027\n",
            "Epoch 836/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 837/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 838/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 839/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 840/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 841/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 842/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 843/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0027\n",
            "Epoch 844/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 845/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 846/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 847/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0027\n",
            "Epoch 848/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0027\n",
            "Epoch 849/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0027\n",
            "Epoch 850/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0027\n",
            "Epoch 851/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0026\n",
            "Epoch 852/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 853/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0026\n",
            "Epoch 854/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0026\n",
            "Epoch 855/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 856/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0026\n",
            "Epoch 857/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 858/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 859/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 860/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 861/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0026\n",
            "Epoch 862/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 863/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0026\n",
            "Epoch 864/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 865/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 866/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 867/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 868/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 869/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 870/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0026\n",
            "Epoch 871/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0026\n",
            "Epoch 872/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 873/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0025\n",
            "Epoch 874/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0025\n",
            "Epoch 875/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 876/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 877/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0025\n",
            "Epoch 878/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 879/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 880/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0025\n",
            "Epoch 881/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0025\n",
            "Epoch 882/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0025\n",
            "Epoch 883/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 884/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 885/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0025\n",
            "Epoch 886/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0025\n",
            "Epoch 887/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 888/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 889/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 890/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0025\n",
            "Epoch 891/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 892/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 893/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 894/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 895/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0025\n",
            "Epoch 896/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 897/1000\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.0024\n",
            "Epoch 898/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 899/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 900/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 901/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 902/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 903/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 904/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0024\n",
            "Epoch 905/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 906/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0024\n",
            "Epoch 907/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0024\n",
            "Epoch 908/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0024\n",
            "Epoch 909/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 910/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 911/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0024\n",
            "Epoch 912/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 913/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0024\n",
            "Epoch 914/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 915/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 916/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 917/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 918/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0024\n",
            "Epoch 919/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0024\n",
            "Epoch 920/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0024\n",
            "Epoch 921/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 922/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 923/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 924/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 925/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 926/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 927/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 928/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0023\n",
            "Epoch 929/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 930/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 931/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 932/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 933/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 934/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 935/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 936/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 937/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 938/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 939/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 940/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 941/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0023\n",
            "Epoch 942/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 943/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 944/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 945/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 946/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 947/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 948/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 949/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 950/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 951/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 952/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 953/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 954/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 955/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 956/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 957/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 958/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 959/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 960/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0022\n",
            "Epoch 961/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 962/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0022\n",
            "Epoch 963/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 964/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0022\n",
            "Epoch 965/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0022\n",
            "Epoch 966/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 967/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 968/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0022\n",
            "Epoch 969/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 970/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 971/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 972/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 973/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 974/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 975/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0022\n",
            "Epoch 976/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0022\n",
            "Epoch 977/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0021\n",
            "Epoch 978/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 979/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0021\n",
            "Epoch 980/1000\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.0021\n",
            "Epoch 981/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 982/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 983/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 984/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 985/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 986/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 987/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 988/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 989/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 990/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 991/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 992/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 993/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 994/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 995/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 996/1000\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.0021\n",
            "Epoch 997/1000\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.0021\n",
            "Epoch 998/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 999/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 1000/1000\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "[[0.03149492]\n",
            " [0.9497347 ]\n",
            " [0.95497674]\n",
            " [0.0521006 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST"
      ],
      "metadata": {
        "id": "g14bWupgY4v5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a small dense network on the MNIST data:\n",
        "\n",
        "\n",
        "*   Prepare the data:\n",
        "  *   Inputs = vectors of real numbers (between 0.0 and 1.0 of size 28*28\n",
        "  *   Labels = vectors of real numbers of size 10 (nine 0 and a 1 at the index of the label)\n",
        "*   Define the network:\n",
        "  * Fully connected network with 28*28 inputs and 10 outputs\n",
        "* Define the loss and the optimizer\n",
        "* Train the network\n",
        "* Test the network\n",
        "* Print an image in the test set and the predicted class\n",
        "\n"
      ],
      "metadata": {
        "id": "Tun8B397huRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVzRPrPFY5tD",
        "outputId": "8e4f3bd1-05ea-4b03-92cc-a6b9b8085866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# modifie les dimensions en gardant le mme nombre d'lments\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "# va transformer des entiers en des vecteurs avec des 0 et 1 aux bons endroits\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "metadata": {
        "id": "PN4B5aN0Zb_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ma version fausse\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_dim=728))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "sgd = SGD(lr=0.1)\n",
        "model.compile(loss='mse', optimizer=sgd)\n",
        "model.fit(X, y, verbose=1, batch_size=1, epochs=1000)\n",
        "print(model.predict(X))"
      ],
      "metadata": {
        "id": "rjZ5gL5IjQa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correction\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "\n",
        "network = Sequential()\n",
        "network.add(Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "network.add(Dense(10, activation='softmax')) # somme des activations va faire 1"
      ],
      "metadata": {
        "id": "zpkIWg4Rkk0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the optimizer and the loss:\n",
        "network.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "metrics=['accuracy']) # % d'exemple qui vont tre bien classs"
      ],
      "metadata": {
        "id": "IIKWLiY_kzM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training the network\n",
        "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x-cg10glRwn",
        "outputId": "5340f21c-2a89-48c5-cbff-edf846ce48e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 6s 11ms/step - loss: 0.2553 - accuracy: 0.9252\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1032 - accuracy: 0.9693\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 5s 12ms/step - loss: 0.0681 - accuracy: 0.9796\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0496 - accuracy: 0.9854\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0368 - accuracy: 0.9891\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f193f107ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the network\n",
        "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
        "print('test_acc:', test_acc) # nombre d'images bien classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKpp8L1OlStU",
        "outputId": "3adac64f-6755-49c2-f726-1745022a81a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0658 - accuracy: 0.9795\n",
            "test_acc: 0.9794999957084656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the class of an example\n",
        "import matplotlib.pyplot as plt\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "plt.imshow(test_images [0])\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "img = test_images [0].reshape ((1, 28*28))\n",
        "print (network.predict(img))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "uwz_n0HTlcZA",
        "outputId": "102c6647-7273-442e-ff15-8ebcef02dc1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.1075907e-08 4.3379483e-10 6.0874561e-07 5.5157543e-06 7.3060984e-12\n",
            "  2.7074177e-07 1.0774476e-13 9.9999177e-01 4.7164978e-08 1.8958870e-06]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANiklEQVR4nO3df4wc9XnH8c8n/kV8QGtDcF3j4ISQqE4aSHWBRNDKESUFImSiJBRLtVyJ5lALElRRW0QVBalVSlEIok0aySluHESgaQBhJTSNa6W1UKljg4yxgdaEmsau8QFOaxPAP/DTP24cHXD7vWNndmft5/2SVrs7z87Oo/F9PLMzO/t1RAjA8e9tbTcAoD8IO5AEYQeSIOxAEoQdSGJ6Pxc207PiBA31c5FAKq/qZzoYBzxRrVbYbV8s6XZJ0yT9bUTcXHr9CRrSeb6wziIBFGyIdR1rXe/G254m6auSLpG0WNIy24u7fT8AvVXnM/u5kp6OiGci4qCkeyQtbaYtAE2rE/YFkn4y7vnOatrr2B6xvcn2pkM6UGNxAOro+dH4iFgZEcMRMTxDs3q9OAAd1An7LkkLxz0/vZoGYADVCftGSWfZfpftmZKulLSmmbYANK3rU28Rcdj2tZL+SWOn3lZFxLbGOgPQqFrn2SPiQUkPNtQLgB7i67JAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJGoN2Wx7h6T9kl6TdDgihptoCkDzaoW98rGIeKGB9wHQQ+zGA0nUDXtI+oHtR2yPTPQC2yO2N9nedEgHai4OQLfq7sZfEBG7bJ8maa3tpyJi/fgXRMRKSSsl6WTPjZrLA9ClWlv2iNhV3Y9Kul/SuU00BaB5XYfd9pDtk44+lvRxSVubagxAs+rsxs+TdL/to+/zrYj4fiNdAWhc12GPiGcknd1gLwB6iFNvQBKEHUiCsANJEHYgCcIOJNHEhTApvPjZj3asvXP508V5nxqdV6wfPDCjWF9wd7k+e+dLHWtHNj9RnBd5sGUHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQ4zz5Ff/xH3+pY+9TQT8szn1lz4UvK5R2HX+5Yu/35j9Vc+LHrR6NndKwN3foLxXmnr3uk6XZax5YdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JwRP8GaTnZc+M8X9i35TXpZ58+r2PthQ+W/8+c82R5Hf/0V1ysz/zg/xbrt3zgvo61i97+SnHe7718YrH+idmdr5Wv65U4WKxvODBUrC854VDXy37P964u1t87srHr927ThlinfbF3wj8otuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATXs0/R0Hc2FGr13vvkerPrr39pScfan5+/qLzsfy3/5v0tS97TRUdTM/2VI8X60Jbdxfop6+8t1n91Zuff25+9o/xb/MejSbfstlfZHrW9ddy0ubbX2t5e3c/pbZsA6prKbvw3JF38hmk3SFoXEWdJWlc9BzDAJg17RKyXtPcNk5dKWl09Xi3p8ob7AtCwbj+zz4uIox+onpPUcTAz2yOSRiTpBM3ucnEA6qp9ND7GrqTpeKVHRKyMiOGIGJ6hWXUXB6BL3YZ9j+35klTdjzbXEoBe6DbsayStqB6vkPRAM+0A6JVJP7Pbvltjv1x+qu2dkr4g6WZJ37Z9laRnJV3RyyZRdvi5PR1rQ/d2rknSa5O899B3Xuyio2bs+b2PFuvvn1n+8/3S3vd1rC36u2eK8x4uVo9Nk4Y9IpZ1KB2bv0IBJMXXZYEkCDuQBGEHkiDsQBKEHUiCS1zRmulnLCzWv3LjV4r1GZ5WrP/D7b/ZsXbK7oeL8x6P2LIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKcZ0drnvrDBcX6h2eVh7LedrA8HPXcJ15+yz0dz9iyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASnGdHTx34xIc71h799G2TzF0eQej3r7uuWH/7v/1okvfPhS07kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBeXb01H9f0nl7cqLL59GX/ddFxfrs7z9WrEexms+kW3bbq2yP2t46btpNtnfZ3lzdLu1tmwDqmspu/DckXTzB9Nsi4pzq9mCzbQFo2qRhj4j1kvb2oRcAPVTnAN21trdUu/lzOr3I9ojtTbY3HdKBGosDUEe3Yf+apDMlnSNpt6RbO70wIlZGxHBEDM+Y5MIGAL3TVdgjYk9EvBYRRyR9XdK5zbYFoGldhd32/HFPPylpa6fXAhgMk55nt323pCWSTrW9U9IXJC2xfY7GTmXukHR1D3vEAHvbSScV68t//aGOtX1HXi3OO/rFdxfrsw5sLNbxepOGPSKWTTD5jh70AqCH+LoskARhB5Ig7EAShB1IgrADSXCJK2rZftP7i/Xvnvo3HWtLt3+qOO+sBzm11iS27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOfZUfR/v/ORYn3Lb/9Vsf7jw4c61l76y9OL887S7mIdbw1bdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgvPsyU1f8MvF+vWf//tifZbLf0JXPra8Y+0d/8j16v3Elh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkuA8+3HO08v/xGd/d2ex/pkTXyzW79p/WrE+7/OdtydHinOiaZNu2W0vtP1D20/Y3mb7umr6XNtrbW+v7uf0vl0A3ZrKbvxhSZ+LiMWSPiLpGtuLJd0gaV1EnCVpXfUcwICaNOwRsTsiHq0e75f0pKQFkpZKWl29bLWky3vVJID63tJndtuLJH1I0gZJ8yLi6I+EPSdpXod5RiSNSNIJmt1tnwBqmvLReNsnSrpX0vURsW98LSJCUkw0X0SsjIjhiBieoVm1mgXQvSmF3fYMjQX9roi4r5q8x/b8qj5f0mhvWgTQhEl3421b0h2SnoyIL48rrZG0QtLN1f0DPekQ9Zz9vmL5z067s9bbf/WLnynWf/Gxh2u9P5ozlc/s50taLulx25uraTdqLOTftn2VpGclXdGbFgE0YdKwR8RDktyhfGGz7QDoFb4uCyRB2IEkCDuQBGEHkiDsQBJc4nocmLb4vR1rI/fU+/rD4lXXFOuL7vz3Wu+P/mHLDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJcJ79OPDUH3T+Yd/LZu/rWJuK0//lYPkFMeEPFGEAsWUHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQ4z34MePWyc4v1dZfdWqgy5BbGsGUHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSSmMj77QknflDRPUkhaGRG3275J0mclPV+99MaIeLBXjWb2P+dPK9bfOb37c+l37T+tWJ+xr3w9O1ezHzum8qWaw5I+FxGP2j5J0iO211a12yLiS71rD0BTpjI++25Ju6vH+20/KWlBrxsD0Ky39Jnd9iJJH5K0oZp0re0ttlfZnvC3kWyP2N5ke9MhHajVLIDuTTnstk+UdK+k6yNin6SvSTpT0jka2/JP+AXtiFgZEcMRMTxDsxpoGUA3phR22zM0FvS7IuI+SYqIPRHxWkQckfR1SeWrNQC0atKw27akOyQ9GRFfHjd9/riXfVLS1ubbA9CUqRyNP1/SckmP295cTbtR0jLb52js7MsOSVf3pEPU8hcvLi7WH/6tRcV67H68wW7QpqkcjX9IkicocU4dOIbwDTogCcIOJEHYgSQIO5AEYQeSIOxAEo4+Drl7sufGeb6wb8sDstkQ67Qv9k50qpwtO5AFYQeSIOxAEoQdSIKwA0kQdiAJwg4k0dfz7Lafl/TsuEmnSnqhbw28NYPa26D2JdFbt5rs7YyIeMdEhb6G/U0LtzdFxHBrDRQMam+D2pdEb93qV2/sxgNJEHYgibbDvrLl5ZcMam+D2pdEb93qS2+tfmYH0D9tb9kB9AlhB5JoJey2L7b9H7aftn1DGz10YnuH7cdtb7a9qeVeVtketb113LS5ttfa3l7dTzjGXku93WR7V7XuNtu+tKXeFtr+oe0nbG+zfV01vdV1V+irL+ut75/ZbU+T9J+SLpK0U9JGScsi4om+NtKB7R2ShiOi9S9g2P4NSS9J+mZEfKCadoukvRFxc/Uf5ZyI+JMB6e0mSS+1PYx3NVrR/PHDjEu6XNLvqsV1V+jrCvVhvbWxZT9X0tMR8UxEHJR0j6SlLfQx8CJivaS9b5i8VNLq6vFqjf2x9F2H3gZCROyOiEerx/slHR1mvNV1V+irL9oI+wJJPxn3fKcGa7z3kPQD24/YHmm7mQnMi4jd1ePnJM1rs5kJTDqMdz+9YZjxgVl33Qx/XhcH6N7sgoj4NUmXSLqm2l0dSDH2GWyQzp1OaRjvfplgmPGfa3PddTv8eV1thH2XpIXjnp9eTRsIEbGruh+VdL8GbyjqPUdH0K3uR1vu5+cGaRjviYYZ1wCsuzaHP28j7BslnWX7XbZnSrpS0poW+ngT20PVgRPZHpL0cQ3eUNRrJK2oHq+Q9ECLvbzOoAzj3WmYcbW87lof/jwi+n6TdKnGjsj/WNKfttFDh77eLemx6rat7d4k3a2x3bpDGju2cZWkUyStk7Rd0j9LmjtAvd0p6XFJWzQWrPkt9XaBxnbRt0jaXN0ubXvdFfrqy3rj67JAEhygA5Ig7EAShB1IgrADSRB2IAnCDiRB2IEk/h9BCfQTVPflJQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercide: preparing the data\n",
        "You can't feed lists of integers into a neural network.\n",
        "You have to turn your lists into tensors:\n",
        "\n",
        "\n",
        "*   One-hot encode your lists to turn them into vectors of 0s and 1s.\n",
        "*   This would mean, for instance, turning the sequence [3, 5] into a 10,000-dimensional vector that would be all 0s except for indices 3 and 5, hich would be 1s.\n",
        "*   Then you could use as the first layer in your network a dense layer, capable of hedling floatingpoint vector data.\n",
        "\n"
      ],
      "metadata": {
        "id": "wnMhApKppNwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ],
      "metadata": {
        "id": "IqAwwJIkqFc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7lGxJi-OqqP",
        "outputId": "254de697-f2c9-4704-f0ae-1574a7fad6b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode your lists to turn them into vectors of 0s and 1s.\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "train_data = to_categorical(train_data)\n",
        "test_data = to_categorical(test_data)\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "metadata": {
        "id": "hv9EykoLq0zZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from tensorflow.python.keras.utils.np_utils import to_categorical\n",
        "# One-hot encode your lists to turn them into vectors of 0s and 1s.\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "train_data = to_categorical(train_data)\n",
        "test_data = to_categorical(test_data)\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "Cf4xB6fnO6CQ",
        "outputId": "7b4b0ca4-42ac-4bc4-bc88-1142fe0f2c97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f7d501b39777>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \"\"\"\n\u001b[0;32m---> 62\u001b[0;31m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m   \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# correction\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i in range (len (sequences)):\n",
        "        for j in range (len (sequences [i])):\n",
        "            results [i] [sequences [i] [j]] = 1.\n",
        "    return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "# You should also convert your labels from integer to numeric, which is straightforward:\n",
        "\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "metadata": {
        "id": "86P7fEDmuCFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correction\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "#(train_data, train_labels), (test_data, test_labels) imdb.load_data(\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(    \n",
        "    num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  #results = np.zeros(len(sequences), dimension))\n",
        "  #results = np.zeros(len(sequences), dimension))\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i in range (len (sequences)):\n",
        "    #for j in range (len (sequences), dimension)):\n",
        "    for j in range (len (sequences [i])):\n",
        "      results [i] [sequences [i] [j]] = 1.\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "# You should also convert your labels from integer to numeric, which is straightforward:\n",
        "\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "#y_test = np.asarray(test_labels).astype('float')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "metadata": {
        "id": "0h3E2fwZPzUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# proposition\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "\n",
        "network = Sequential()\n",
        "network.add(Dense(512, activation='relu', input_shape=(10000,)))\n",
        "network.add(Dense(2, activation='softmax')) # somme des activations va faire 1"
      ],
      "metadata": {
        "id": "kMUxyZryrA49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "\n",
        "network = Sequential()\n",
        "network.add(Dense(512, activation='relu', input_shape=(10000,)))\n",
        "network.add(Dense(2, activation='softmax')) # somme des activations va faire 1"
      ],
      "metadata": {
        "id": "hzJ1Nf4sS4uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the optimizer and the loss:\n",
        "network.compile(\n",
        "    optimizer='rmsprop', loss='BinaryCrossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "PDC9ebs0s4Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the optimizer and the loss:\n",
        "network.compile(\n",
        "    optimizer='rmsprop', loss='BinaryCrossentropy', metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "fDTJy4UgShAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the network\n",
        "network.fit(train_data, train_labels, epochs=5, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5340f21c-2a89-48c5-cbff-edf846ce48e4",
        "id": "JbwlhL4RtUOa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 6s 11ms/step - loss: 0.2553 - accuracy: 0.9252\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1032 - accuracy: 0.9693\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 5s 12ms/step - loss: 0.0681 - accuracy: 0.9796\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0496 - accuracy: 0.9854\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0368 - accuracy: 0.9891\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f193f107ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the network\n",
        "#network.fit(train_data, train_labels, epochs=5, batch_size=128)\n",
        "network.fit(x_train, y_train, epochs=5, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "id": "taLkLLSHTVeZ",
        "outputId": "8386d06c-1581-4a2b-cdfb-b3ebb640017a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-88c58cd9deac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#network.fit(train_data, train_labels, epochs=5, batch_size=128)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 810, in train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/losses.py\", line 1807, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 5158, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 2) vs (None, 1)).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the network\n",
        "test_loss, test_acc = network.evaluate(test_data, test_labels)\n",
        "print('test_acc:', test_acc)"
      ],
      "metadata": {
        "id": "NRwy_F-hsWd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the class of an example\n",
        "import matplotlib.pyplot as plt\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "plt.imshow(test_images [0])\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "img = test_images [0].reshape ((1, 28*28))\n",
        "print (network.predict(img))"
      ],
      "metadata": {
        "id": "UALQ5PHuq_AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMDB"
      ],
      "metadata": {
        "id": "BWmwZTO7VV5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the network:\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model= Sequential()\n",
        "model.add(Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "lnAYvhyeVVAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definig a validation set:\n",
        "\n",
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "\n",
        "y_val = y_train[:1000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "metadata": {
        "id": "_8yt4dDpXpp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainign with a validation set:\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "history= model.fit(\n",
        "    partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(\n",
        "        x_val, y_val))"
      ],
      "metadata": {
        "id": "-w-0HJDXX16v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizethe training loss:\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "QDAmkEUoYU_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "la loss est le carr des poids des poids des paramtres : quand on fait le gradient, on enlve le ...?"
      ],
      "metadata": {
        "id": "vHGFvFAwZccu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight Rgularization"
      ],
      "metadata": {
        "id": "STaMFyP-cu7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "#model = Sequetial()\n",
        "model = Sequential()\n",
        "# every coeff in the weight matrix of the layer will add .001 * weight_coefficient_value to the total loss of the network\n",
        "model.add(layers.Dense(\n",
        "    16, kernel_regularizer=regularizers.l2(\n",
        "        0.001), activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "bLYwbxZnat5i",
        "outputId": "1acb295f-1ab1-4561-e720-30951da4075e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-65f7fb15069f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# every coeff in the weight matrix of the layer will add .001 * weight_coefficient_value to the total loss of the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model.add(layers.Dense(\n\u001b[0m\u001b[1;32m      6\u001b[0m     16, kernel_regularizer=regularizers.l2(\n\u001b[1;32m      7\u001b[0m         0.001), activation='relu', input_shape=(10000,)))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#, regularizers\n",
        "#from tensorflowkeras import regularizers\n",
        "from tensorflow.keras import regularizers\n",
        "from keras import layers\n",
        "model = Sequential()\n",
        "# every coeff in the weight matrix of the layer will add  * weight_coefficient_value to the total loss of the network\n",
        "model.add(layers.Dense(\n",
        "    #16, kernel_regularizer=regulari*.l2(\n",
        "    16, kernel_regularizer=regularizers.l2(\n",
        "        0.001), activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(1, activation='sigmoid'\n",
        "))"
      ],
      "metadata": {
        "id": "TN_coU_9eB-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_m = Sequential()\n",
        "dropout_m.add(layers.Dense)"
      ],
      "metadata": {
        "id": "-7LmhIuNm4es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16,activation='relu',\n",
        "                       input_shape=(10000,)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "RpxIHKqCnr4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiclass Classification"
      ],
      "metadata": {
        "id": "8K5qI48ZploR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import reuters\n",
        "(train_data, train_labels), (\n",
        "    test_data, test_labels) = reuters.load_data(num_words=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSCHt32Opnbo",
        "outputId": "ca7ea8da-0723-476f-cf38-d52d8aeded2c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n",
            "2121728/2110848 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.datasets import imdb\n",
        "#(train_data, train_labels), (test_data, test_labels) imdb.load_data(\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(    \n",
        "    num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  #results = np.zeros(len(sequences), dimension))\n",
        "  #results = np.zeros(len(sequences), dimension))\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i in range (len (sequences)):\n",
        "    #for j in range (len (sequences), dimension)):\n",
        "    for j in range (len (sequences [i])):\n",
        "      results [i] [sequences [i] [j]] = 1.\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "# You should also convert your labels from integer to numeric, which is straightforward:\n",
        "\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "#y_test = np.asarray(test_labels).astype('float')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "metadata": {
        "id": "iR2ftlhTr0x2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "G_59Vfq3tger"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the data\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i in range (len (sequences)):\n",
        "    for j in range (len (sequences [i])):\n",
        "      results [i] [sequences [i] [j]] = 1.\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorier_sequences(test_data)  \n",
        "\n",
        "# You should also convert your labels from integer to numeric, which is straightforward:\n",
        "\n",
        "y_train = npn.asarray(train_labels).astype('float32')\n",
        "y_test = np.array(test_labels).astype('float32')"
      ],
      "metadata": {
        "id": "SckkBZDVr5Cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from keras import layers\n",
        "model = Sequential()\n",
        "# every coeff in the weight matrix of the layer will add  * weight_coefficient_value to the total loss of the network\n",
        "model.add(layers.Dense(\n",
        "    16, kernel_regularizer=regularizers.l2(\n",
        "        0.001), activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(1, activation='sigmoid'\n",
        "))"
      ],
      "metadata": {
        "id": "O4qhvXraqyLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reuters"
      ],
      "metadata": {
        "id": "BuAHmc9vOfN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding the data"
      ],
      "metadata": {
        "id": "Njaa9HirOjPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import reuters"
      ],
      "metadata": {
        "id": "kYkbvamcPBoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "#from tensorflow.keras.datasets import imdb\n",
        "#(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\n",
        "    num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i in range (len (sequences)):\n",
        "        for j in range (len (sequences [i])):\n",
        "            results [i] [sequences [i] [j]] = 1.\n",
        "    return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "# You should also convert your labels from integer to numeric, which is straightforward:\n",
        "\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "metadata": {
        "id": "FTH37n4KPLYm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "#one_hot_train_labels = tp_categorical(train_labels)\n",
        "#onr_hot_train_labels= to_categorical(train_labels)\n",
        "one_hot_train_labels= to_categorical(train_labels)\n",
        "one_hit_test_labels = to_categorical(test_labels)"
      ],
      "metadata": {
        "id": "SgJqlEAhOdsi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import SGD"
      ],
      "metadata": {
        "id": "UgRx3mwkPCs2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(8, input_dim=2))\n",
        "model.add(Activation('tanh'))\n",
        "model.add(Dense(1))\n",
        "#model.add(Activation('sigmoid'))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        " \n",
        "# compile model\n",
        "sgd = SGD(lr=0.1)\n",
        "#model.compile(loss='mse', optimizer=sgd)\n",
        "model.compile(loss='categorical_crossentropy' ,optimizer=sgd)#opt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjLlriA5RBBX",
        "outputId": "510ed2fd-849a-4360-f86d-7b6a4c850f60"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a validation set:\n",
        "\n",
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "\n",
        "y_val = y_train[:1000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "metadata": {
        "id": "Zf9yAXLDRJ_e"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the network using a validation set\n",
        "\n",
        "#model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "#history= model.fit(\n",
        "    #partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(\n",
        "        #x_val, y_val))\n",
        "\n",
        "#model.fit(X, y, verbose=1, batch_size=1, epochs=1000)\n",
        "model.fit(partial_x_train, partial_y_train, verbose=1, batch_size=1, epochs=1000)\n",
        "print(model.predict(partial_x_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "Apy19OUgRQYv",
        "outputId": "612a7aa9-a624-4e73-9949-639d7779826f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-5f183a7c2736>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#model.fit(X, y, verbose=1, batch_size=1, epochs=1000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial_x_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial_x_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m           raise ValueError('Unexpected result of `train_function` '\n\u001b[0m\u001b[1;32m   1228\u001b[0m                            \u001b[0;34m'(Empty logs). Please use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m                            \u001b[0;34m'`Model.compile(..., run_eagerly=True)`, or '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizethe training loss:\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "XfG193zaRTLH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correction possible\n",
        "\n",
        "# defining the network\n",
        "\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))"
      ],
      "metadata": {
        "id": "LgJ3og-zT-P5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the optimizer and the loss:\n",
        "\n",
        "model.compile(\n",
        "    optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "sCC2vvmMU6BU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a validation set:\n",
        "\n",
        "x_val= x_train[:1000]\n",
        "partial_x_train= x_train[1000:]\n",
        "y_val= one_hot_train_labels[:1000]\n",
        "partial_y_train= one_hot_train_labels[1000:]"
      ],
      "metadata": {
        "id": "K09-yutGVdmd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(partial_x_train, partial_y_train, verbose=1, batch_size=1, epochs=1000)\n",
        "print(model.predict(partial_x_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFzBqQ2nV2OA",
        "outputId": "356d576a-5377-40ab-f1a2-4f277d17e477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.6678 - accuracy: 0.6815\n",
            "Epoch 2/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.5881 - accuracy: 0.7375\n",
            "Epoch 3/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.5334 - accuracy: 0.7660\n",
            "Epoch 4/1000\n",
            "7982/7982 [==============================] - 71s 9ms/step - loss: 1.5094 - accuracy: 0.7898\n",
            "Epoch 5/1000\n",
            "7982/7982 [==============================] - 70s 9ms/step - loss: 1.4703 - accuracy: 0.7982\n",
            "Epoch 6/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.4526 - accuracy: 0.8038\n",
            "Epoch 7/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.4468 - accuracy: 0.8098\n",
            "Epoch 8/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.4512 - accuracy: 0.8086\n",
            "Epoch 9/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.4479 - accuracy: 0.8096\n",
            "Epoch 10/1000\n",
            "7982/7982 [==============================] - 60s 8ms/step - loss: 1.4408 - accuracy: 0.8111\n",
            "Epoch 11/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.4497 - accuracy: 0.8172\n",
            "Epoch 12/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.4202 - accuracy: 0.8173\n",
            "Epoch 13/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.4448 - accuracy: 0.8193\n",
            "Epoch 14/1000\n",
            "7982/7982 [==============================] - 62s 8ms/step - loss: 1.4228 - accuracy: 0.8244\n",
            "Epoch 15/1000\n",
            "7982/7982 [==============================] - 59s 7ms/step - loss: 1.4470 - accuracy: 0.8269\n",
            "Epoch 16/1000\n",
            "7982/7982 [==============================] - 60s 7ms/step - loss: 1.4323 - accuracy: 0.8289\n",
            "Epoch 17/1000\n",
            "7982/7982 [==============================] - 62s 8ms/step - loss: 1.4250 - accuracy: 0.8289\n",
            "Epoch 18/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.4155 - accuracy: 0.8242\n",
            "Epoch 19/1000\n",
            "7982/7982 [==============================] - 62s 8ms/step - loss: 1.4229 - accuracy: 0.8311\n",
            "Epoch 20/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.3867 - accuracy: 0.8335\n",
            "Epoch 21/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.3856 - accuracy: 0.8348\n",
            "Epoch 22/1000\n",
            "7982/7982 [==============================] - 62s 8ms/step - loss: 1.3793 - accuracy: 0.8385\n",
            "Epoch 23/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.3515 - accuracy: 0.8365\n",
            "Epoch 24/1000\n",
            "7982/7982 [==============================] - 63s 8ms/step - loss: 1.3124 - accuracy: 0.8385\n",
            "Epoch 25/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.2843 - accuracy: 0.8400\n",
            "Epoch 26/1000\n",
            "7982/7982 [==============================] - 68s 9ms/step - loss: 1.3447 - accuracy: 0.8413\n",
            "Epoch 27/1000\n",
            "7982/7982 [==============================] - 69s 9ms/step - loss: 1.2927 - accuracy: 0.8424\n",
            "Epoch 28/1000\n",
            "7982/7982 [==============================] - 68s 9ms/step - loss: 1.3297 - accuracy: 0.8425\n",
            "Epoch 29/1000\n",
            "7982/7982 [==============================] - 70s 9ms/step - loss: 1.3169 - accuracy: 0.8450\n",
            "Epoch 30/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.3603 - accuracy: 0.8411\n",
            "Epoch 31/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.2972 - accuracy: 0.8454\n",
            "Epoch 32/1000\n",
            "7982/7982 [==============================] - 68s 8ms/step - loss: 1.3031 - accuracy: 0.8448\n",
            "Epoch 33/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.3259 - accuracy: 0.8463\n",
            "Epoch 34/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.2312 - accuracy: 0.8441\n",
            "Epoch 35/1000\n",
            "7982/7982 [==============================] - 63s 8ms/step - loss: 1.2598 - accuracy: 0.8475\n",
            "Epoch 36/1000\n",
            "7982/7982 [==============================] - 63s 8ms/step - loss: 1.2835 - accuracy: 0.8480\n",
            "Epoch 37/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.2502 - accuracy: 0.8475\n",
            "Epoch 38/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.2373 - accuracy: 0.8479\n",
            "Epoch 39/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.2192 - accuracy: 0.8478\n",
            "Epoch 40/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1804 - accuracy: 0.8494\n",
            "Epoch 41/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.2202 - accuracy: 0.8484\n",
            "Epoch 42/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.1994 - accuracy: 0.8509\n",
            "Epoch 43/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.2210 - accuracy: 0.8492\n",
            "Epoch 44/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1872 - accuracy: 0.8505\n",
            "Epoch 45/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.2191 - accuracy: 0.8519\n",
            "Epoch 46/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.2330 - accuracy: 0.8530\n",
            "Epoch 47/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.1788 - accuracy: 0.8542\n",
            "Epoch 48/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1764 - accuracy: 0.8520\n",
            "Epoch 49/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1347 - accuracy: 0.8535\n",
            "Epoch 50/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1581 - accuracy: 0.8527\n",
            "Epoch 51/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1447 - accuracy: 0.8542\n",
            "Epoch 52/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1990 - accuracy: 0.8537\n",
            "Epoch 53/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1437 - accuracy: 0.8555\n",
            "Epoch 54/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1457 - accuracy: 0.8545\n",
            "Epoch 55/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1650 - accuracy: 0.8542\n",
            "Epoch 56/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1450 - accuracy: 0.8572\n",
            "Epoch 57/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1598 - accuracy: 0.8525\n",
            "Epoch 58/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1686 - accuracy: 0.8525\n",
            "Epoch 59/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1958 - accuracy: 0.8566\n",
            "Epoch 60/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1726 - accuracy: 0.8550\n",
            "Epoch 61/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1329 - accuracy: 0.8549\n",
            "Epoch 62/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1207 - accuracy: 0.8566\n",
            "Epoch 63/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1247 - accuracy: 0.8530\n",
            "Epoch 64/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.1459 - accuracy: 0.8555\n",
            "Epoch 65/1000\n",
            "7982/7982 [==============================] - 74s 9ms/step - loss: 1.1432 - accuracy: 0.8555\n",
            "Epoch 66/1000\n",
            "7982/7982 [==============================] - 69s 9ms/step - loss: 1.1642 - accuracy: 0.8563\n",
            "Epoch 67/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.1704 - accuracy: 0.8569\n",
            "Epoch 68/1000\n",
            "7982/7982 [==============================] - 70s 9ms/step - loss: 1.1207 - accuracy: 0.8572\n",
            "Epoch 69/1000\n",
            "7982/7982 [==============================] - 81s 10ms/step - loss: 1.1446 - accuracy: 0.8542\n",
            "Epoch 70/1000\n",
            "7982/7982 [==============================] - 76s 10ms/step - loss: 1.1387 - accuracy: 0.8592\n",
            "Epoch 71/1000\n",
            "7982/7982 [==============================] - 74s 9ms/step - loss: 1.1493 - accuracy: 0.8566\n",
            "Epoch 72/1000\n",
            "7982/7982 [==============================] - 70s 9ms/step - loss: 1.0907 - accuracy: 0.8571\n",
            "Epoch 73/1000\n",
            "7982/7982 [==============================] - 73s 9ms/step - loss: 1.1221 - accuracy: 0.8557\n",
            "Epoch 74/1000\n",
            "7982/7982 [==============================] - 70s 9ms/step - loss: 1.1103 - accuracy: 0.8555\n",
            "Epoch 75/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.0901 - accuracy: 0.8561\n",
            "Epoch 76/1000\n",
            "7982/7982 [==============================] - 76s 9ms/step - loss: 1.0762 - accuracy: 0.8591\n",
            "Epoch 77/1000\n",
            "7982/7982 [==============================] - 74s 9ms/step - loss: 1.1356 - accuracy: 0.8567\n",
            "Epoch 78/1000\n",
            "7982/7982 [==============================] - 68s 9ms/step - loss: 1.0985 - accuracy: 0.8578\n",
            "Epoch 79/1000\n",
            "7982/7982 [==============================] - 72s 9ms/step - loss: 1.1163 - accuracy: 0.8567\n",
            "Epoch 80/1000\n",
            "7982/7982 [==============================] - 70s 9ms/step - loss: 1.0773 - accuracy: 0.8592\n",
            "Epoch 81/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.1329 - accuracy: 0.8587\n",
            "Epoch 82/1000\n",
            "7982/7982 [==============================] - 69s 9ms/step - loss: 1.0984 - accuracy: 0.8561\n",
            "Epoch 83/1000\n",
            "7982/7982 [==============================] - 71s 9ms/step - loss: 1.0831 - accuracy: 0.8586\n",
            "Epoch 84/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1021 - accuracy: 0.8579\n",
            "Epoch 85/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.1251 - accuracy: 0.8564\n",
            "Epoch 86/1000\n",
            "7982/7982 [==============================] - 68s 8ms/step - loss: 1.0904 - accuracy: 0.8588\n",
            "Epoch 87/1000\n",
            "7982/7982 [==============================] - 68s 9ms/step - loss: 1.0901 - accuracy: 0.8596\n",
            "Epoch 88/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1555 - accuracy: 0.8586\n",
            "Epoch 89/1000\n",
            "7982/7982 [==============================] - 71s 9ms/step - loss: 1.0795 - accuracy: 0.8597\n",
            "Epoch 90/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.1052 - accuracy: 0.8574\n",
            "Epoch 91/1000\n",
            "7982/7982 [==============================] - 61s 8ms/step - loss: 1.0834 - accuracy: 0.8611\n",
            "Epoch 92/1000\n",
            "7982/7982 [==============================] - 63s 8ms/step - loss: 1.0576 - accuracy: 0.8587\n",
            "Epoch 93/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.0487 - accuracy: 0.8586\n",
            "Epoch 94/1000\n",
            "7982/7982 [==============================] - 70s 9ms/step - loss: 1.0983 - accuracy: 0.8618\n",
            "Epoch 95/1000\n",
            "7982/7982 [==============================] - 72s 9ms/step - loss: 1.1091 - accuracy: 0.8573\n",
            "Epoch 96/1000\n",
            "7982/7982 [==============================] - 74s 9ms/step - loss: 1.1219 - accuracy: 0.8612\n",
            "Epoch 97/1000\n",
            "7982/7982 [==============================] - 73s 9ms/step - loss: 1.0972 - accuracy: 0.8577\n",
            "Epoch 98/1000\n",
            "7982/7982 [==============================] - 76s 10ms/step - loss: 1.1075 - accuracy: 0.8587\n",
            "Epoch 99/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.0698 - accuracy: 0.8603\n",
            "Epoch 100/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.0834 - accuracy: 0.8596\n",
            "Epoch 101/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.0549 - accuracy: 0.8593\n",
            "Epoch 102/1000\n",
            "7982/7982 [==============================] - 70s 9ms/step - loss: 1.0779 - accuracy: 0.8602\n",
            "Epoch 103/1000\n",
            "7982/7982 [==============================] - 71s 9ms/step - loss: 1.0572 - accuracy: 0.8626\n",
            "Epoch 104/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.0641 - accuracy: 0.8621\n",
            "Epoch 105/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.0615 - accuracy: 0.8611\n",
            "Epoch 106/1000\n",
            "7982/7982 [==============================] - 69s 9ms/step - loss: 1.0339 - accuracy: 0.8626\n",
            "Epoch 107/1000\n",
            "7982/7982 [==============================] - 74s 9ms/step - loss: 1.0249 - accuracy: 0.8612\n",
            "Epoch 108/1000\n",
            "7982/7982 [==============================] - 70s 9ms/step - loss: 1.0683 - accuracy: 0.8608\n",
            "Epoch 109/1000\n",
            "7982/7982 [==============================] - 69s 9ms/step - loss: 1.0577 - accuracy: 0.8584\n",
            "Epoch 110/1000\n",
            "7982/7982 [==============================] - 68s 8ms/step - loss: 1.0245 - accuracy: 0.8601\n",
            "Epoch 111/1000\n",
            "7982/7982 [==============================] - 72s 9ms/step - loss: 1.0215 - accuracy: 0.8607\n",
            "Epoch 112/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.0810 - accuracy: 0.8607\n",
            "Epoch 113/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.0833 - accuracy: 0.8578\n",
            "Epoch 114/1000\n",
            "7982/7982 [==============================] - 70s 9ms/step - loss: 1.0280 - accuracy: 0.8629\n",
            "Epoch 115/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.0524 - accuracy: 0.8626\n",
            "Epoch 116/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.0703 - accuracy: 0.8623\n",
            "Epoch 117/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.1314 - accuracy: 0.8598\n",
            "Epoch 118/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.0623 - accuracy: 0.8586\n",
            "Epoch 119/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.0787 - accuracy: 0.8613\n",
            "Epoch 120/1000\n",
            "7982/7982 [==============================] - 68s 8ms/step - loss: 1.0653 - accuracy: 0.8606\n",
            "Epoch 121/1000\n",
            "7982/7982 [==============================] - 68s 8ms/step - loss: 1.0301 - accuracy: 0.8618\n",
            "Epoch 122/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.1259 - accuracy: 0.8573\n",
            "Epoch 123/1000\n",
            "7982/7982 [==============================] - 64s 8ms/step - loss: 1.1242 - accuracy: 0.8612\n",
            "Epoch 124/1000\n",
            "7982/7982 [==============================] - 66s 8ms/step - loss: 1.1021 - accuracy: 0.8603\n",
            "Epoch 125/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.1200 - accuracy: 0.8576\n",
            "Epoch 126/1000\n",
            "7982/7982 [==============================] - 65s 8ms/step - loss: 1.0873 - accuracy: 0.8614\n",
            "Epoch 127/1000\n",
            "7982/7982 [==============================] - 67s 8ms/step - loss: 1.0696 - accuracy: 0.8587\n",
            "Epoch 128/1000\n",
            "2878/7982 [=========>....................] - ETA: 39s - loss: 1.0436 - accuracy: 0.8634"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
        "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "# plot loss during training\n",
        "pyplot.subplot(211)\n",
        "pyplot.title('Loss')\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "# plot accuracy during training\n",
        "pyplot.subplot(212)\n",
        "pyplot.title('Accuracy')\n",
        "pyplot.plot(history.history['accuracy'], label='train')\n",
        "pyplot.plot(history.history['val_accuracy'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "fVclXoeYV9to"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correction pour acclrer l'apprentissage\n",
        "\n",
        "# training with a validation set:\n",
        "history= model.fit(\n",
        "    partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(\n",
        "        x_val, y_val))\n",
        "\n",
        "# Visualize the training:\n",
        "import matplotlib.pyplot as plt\n",
        "loss = history.history['loss']\n",
        "val_loss= history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lz96CNFyW3wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further experiments:\n",
        "\n",
        "\n",
        "*   Train on less epochs.\n",
        "*   Try using larger or smaller layers: 32 units, 128 units, and so on.\n",
        "*   The network used two hidden layers. Now try using a single hidden layer, or three hidden layers.\n",
        "\n"
      ],
      "metadata": {
        "id": "WEdGdtiDZdWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the optimizer and the loss:\n",
        "\n",
        "#model.compile(\n",
        " #   optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "epochs_h.compile(\n",
        "    optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train on less epochs:\n",
        "epochs_h= model.fit(\n",
        "    partial_x_train, partial_y_train, epochs=10, batch_size= 512,\n",
        "    validation_data=(x_val, y_val))\n",
        "\n",
        "# Visualize the training:\n",
        "epochs_l = epochs_h.epochs_h['loss']\n",
        "epochs_vl= epochs_h.epochs_h['val_loss']\n",
        "epochs_e= range(1, len(epochs_l) + 1)\n",
        "plt.plot(epochs_e, epochs_l, 'bo', label='Training loss')\n",
        "plt.plot(epochs_e, epochs_vl, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i8iCZY5xZ2AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try using 32 units\n",
        "\n",
        "#model = models.Sequential()\n",
        "#model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "#model.add(layers.Dense(64, activation='relu'))\n",
        "#model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model_32= models.Sequential()\n",
        "model_32.add(layers.Dense(32, activation='relu', input_shape=(1000,)))\n",
        "model_32.add(layers.Dense(32, activation='relu'))\n",
        "model_32.add(layers.Dense(32, activation='softmax'))"
      ],
      "metadata": {
        "id": "k9GBopNdbf2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try using 128 units\n",
        "\n",
        "model_128 = models.Sequential()\n",
        "model_128.add(layers.Dense(128, activation='relu', input_shape=(1000,)))\n",
        "model_128.add(layers.Dense(128, activation='relu'))\n",
        "model_128.add(layers.Dense(128, activation='softmax'))"
      ],
      "metadata": {
        "id": "8dszVETicaS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression"
      ],
      "metadata": {
        "id": "S7hbEO2Wl-2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import boston_housing\n",
        "(train_data, train_targets), (test_data, test_targets)= boston_housing.load_data()"
      ],
      "metadata": {
        "id": "6lvNyJ_9mANb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train a small dense network on the Boston Housing data:\n",
        "\n",
        "\n",
        "*   Encode the data\n",
        "*   Define the network\n",
        "*   Define the loss and the optimizer\n",
        "*   Define a validation set\n",
        "*   Train the network using a validation set\n",
        "\n"
      ],
      "metadata": {
        "id": "Kh98LewAmPEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.lib.function_base import vectorize\n",
        "# Encode the data\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i in range (len (sequences)):\n",
        "    for j in range (len (sequences [i])):\n",
        "      results [i] [sequences [i] [j]] = 1.\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize\n",
        "\n",
        "# You should also convert your labels from integer to numeric:\n",
        "\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "metadata": {
        "id": "dadRutJ5mhXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correction\n",
        "\n",
        "mean = train_data.mean(axis=0)\n",
        "train_data -= mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data /= std\n",
        "test_data -= mean\n",
        "test_data /= std"
      ],
      "metadata": {
        "id": "1KRnzsMrqqQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the network\n",
        "\n",
        "#from tensorflow.keras import Sequential\n",
        "#from tensorflow.keras.layers import Dense, Activation\n",
        "#from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "#model_128 = models.Sequential()\n",
        "#model_128.add(layers.Dense(128, activation='relu', input_shape=(1000,)))\n",
        "#model_128.add(layers.Dense(128, activation='relu'))\n",
        "#model_128.add(layers.Dense(128, activation='softmax'))\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "Housing_model = Sequential()\n",
        "Housing_model.add(layers.Dense(25, activation='relu', input_dim=20))\n",
        "Housing_model.add(layers.Dense(25, activation=('relu'))\n",
        "Housing_model.add(layers.Dense(1, activation=('linear')))"
      ],
      "metadata": {
        "id": "9xopn-TuocRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correction\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "def build_model():\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(64, activation='relu',\n",
        "                         input_shape=(train_data.shape[1],)))\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(64,activation='relu'))\n",
        "  model.add(layers.Dense(1))\n",
        "  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "4VegBZZKt-3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the loss and the optimizer\n",
        "\n",
        "#epochs_h.compile(\n",
        " #   optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy']\n",
        "#)\n",
        "\n",
        "opt = SGD(lr=0.01, momentum=0.9)\n",
        "Housing_model.compile(\n",
        "    optimizer='opt', loss='mean_squared_error', optimizer=opt, \n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "5QwXcH31r2bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a validation set\n",
        "\n"
      ],
      "metadata": {
        "id": "ipNkBWIitLaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a validation set:\n",
        "\n",
        "x_val= x_train[:1000]\n",
        "partial_x_train= x_train[1000:]\n",
        "y_val= one_hot_train_labels[:1000]\n",
        "partial_y_train= one_hot_train_labels[1000:]"
      ],
      "metadata": {
        "id": "x6gKdINetYmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspiration sans cross-validation\n",
        "\n",
        "num_epochs = 50\n",
        "model = build_model()\n",
        "\n",
        "num_val_samples = len(train_data) // 4\n",
        "\n",
        "val_data = train_data[:num_val_samples]\n",
        "val_targets = train_targets[:num_val_samples]\n",
        "partial_train_data = train_data[num_val_samples:]\n",
        "partial_train_targets = train_targets[num_val_samples:]\n",
        "\n",
        "history = model.fit(partial_train_data, partial_train_targets, validation_data=(\n",
        "    val_data, val_targets),\n",
        "    epochs=num_epochs, batch_size=10, verbose=0)"
      ],
      "metadata": {
        "id": "JJ-TGmTUv4Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# incomplete: Train the netork using a validation set\n",
        "housing_hm = Housing_model.fit(\n",
        "    partial\n",
        ")\n"
      ],
      "metadata": {
        "id": "qrl5RtC0tlbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# not corrected: Train on less epochs:\n",
        "epochs_h= model.fit(\n",
        "    partial_x_train, partial_y_train, epochs=10, batch_size= 512,\n",
        "    validation_data=(x_val, y_val))\n",
        "\n",
        "# not corrected: Visualize the training:\n",
        "epochs_l = epochs_h.epochs_h['loss']\n",
        "epochs_vl= epochs_h.epochs_h['val_loss']\n",
        "epochs_e= range(1, len(epochs_l) + 1)\n",
        "plt.plot(epochs_e, epochs_l, 'bo', label='Training loss')\n",
        "plt.plot(epochs_e, epochs_vl, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OLZ_x4GlsLWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correction\n",
        "\n",
        "import numpy as np\n",
        "k = 4\n",
        "num_val_samples = len(train_data) // k\n",
        "num_epochs = 100\n",
        "all_scores = []\n",
        "for i in range(k):\n",
        "  print('processing fold #', i)\n",
        "  val_data = train_data[i * num_val_samples]\n",
        "  val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "  partial_train_data = np.concatenate(\n",
        "      [\n",
        "       train_data[\n",
        "                  :i * num_val_samples], train_data[\n",
        "                                                    (\n",
        "                                                        i + 1)\n",
        "                                                     * num_val_samples:]], \n",
        "                                      axis=0)\n",
        "  partial_train_targets = no.concatenate(\n",
        "      [\n",
        "       train_targets[\n",
        "                     :i * num_val_samples], train_targets[\n",
        "                                                          (\n",
        "                                                              i + 1\n",
        "                                                          ) * num_val_samples:\n",
        "                     ]\n",
        "       ], axis=0\n",
        "      )\n",
        "  model = build_model()\n",
        "  model.fit(partial_train_data, partial_train_targets,\n",
        "            epochs=num_epochs, batch_size=1, verbose=0)\n",
        "  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
        "  all_scores.append(val_mae)"
      ],
      "metadata": {
        "id": "wu0t6x7qx-mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 500\n",
        "all_mae_histories = []\n",
        "for i in range(k):\n",
        "    print('processing fold #', i)\n",
        "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    partial_train_data = np.concatenate ([train_data[:i * num_val_samples],\n",
        "                                                               train_data[(i + 1) * num_val_samples:]], axis=0)\n",
        "    partial_train_targets = np.concatenate([train_targets[:i * num_val_samples],\n",
        "                                                                  train_targets[(i + 1) * num_val_samples:]], axis=0)\n",
        "    model = build_model()\n",
        "    history = model.fit(partial_train_data, partial_train_targets,\n",
        "                                  validation_data=(val_data, val_targets),\n",
        "                                  epochs=num_epochs, batch_size=1, verbose=0)\n",
        "    mae_history = history.history['val_mean_absolute_error']\n",
        "    all_mae_histories.append(mae_history)"
      ],
      "metadata": {
        "id": "RicW699w1JdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_mae_history = [\n",
        "np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
        " \n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(1, len(average_mae_history) + 1),\n",
        "            average_mae_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation MAE')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ROcSe77S1NXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# entrainement du modle une fois hyperparamtres fixs\n",
        "model = build_model()\n",
        " \n",
        "model.fit(train_data, train_targets, \n",
        "               epochs=80, batch_size=16, verbose=0)\n",
        "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)"
      ],
      "metadata": {
        "id": "LjtKRXZ01VhG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}