{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neohack22/IASD/blob/BigData/CCF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   [Guidelines](https://github.com/neohack22/IASD/blob/master/Spark/indications-Project.pdf)\n",
        "*   [Report](https://1drv.ms/w/s!Ak6qyNru7KEYjd1uHcwlefgA6G8mQw?e=M7c2w5)\n"
      ],
      "metadata": {
        "id": "NfDe6KkPazFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "E2I9638Zx5DC",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "422d016b-fcd3-46e1-bdf9-5c259f4cb61e"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # SparkContext to program with RDD and to connect to Spark Cluster\n",
        "#set different parameters using the SparkConf object and their parameters will take priority over the system properties\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from time import time"
      ],
      "metadata": {
        "id": "njCj5mXroXYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ],
      "metadata": {
        "id": "-vtTs3ONyK8w",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d536e669-4eeb-4e93-a317-082c78c5c2c9"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "lvQjSK93rOiu",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "989bdfbb-cb6f-4cc8-a3e2-1163ebd1c63c"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets"
      ],
      "metadata": {
        "id": "H8BFugq0yVEw",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "46178078-108a-4982-ac6e-121b70db6397"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data = sc.textFile... # import dataset as RDD"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "50712f76-e174-4f1a-b45b-f30f18085a9c"
        },
        "id": "kBizJXgfte-Q",
        "outputId": "0696dceb-bc35-4963-9bac-10396f279510"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "data": "",
              "errorSummary": "",
              "metadata": {},
              "errorTraceType": null,
              "type": "ipynbError",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          },
          "transient": null
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark environment"
      ],
      "metadata": {
        "id": "Qb6KXY6Z0Re6",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2c792998-afdb-4046-be41-40a0c63b27eb"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sets the basic configurations  options for running a PySpark application\n",
        "conf = SparkConf().setAppName(\"PySpark App\").setMaster(\"spark://master:7077\")\n",
        "# parameters passed to spark context, entry point for spark environment\n",
        "sc = SparkContext(conf=conf)"
      ],
      "metadata": {
        "id": "GEXghjzzRB3Y",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "304b1da9-305c-4c52-8a52-55f6218165dd"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CCF-Iterate"
      ],
      "metadata": {
        "id": "GGNe8Mg0yZIx",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0e2b9f16-cc4b-4edd-ab2d-8450306cd532"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User Defined Map Reduce function"
      ],
      "metadata": {
        "id": "D1C2hCiQz_wa",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8934b963-f33e-4632-af24-f8088ca602c9"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Map"
      ],
      "metadata": {
        "id": "dI4epKlWIXsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# turn rdd into key-value tuples map-job-ready\n",
        "edgesList = lines.map(lambda #)"
      ],
      "metadata": {
        "id": "2xGKPRu7E7VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MapReduce multiple passes single sort"
      ],
      "metadata": {
        "id": "8CLZRH3Ez5Pa",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "622e515b-e418-419b-bf40-bac67f89fc4a"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# break the adjency list into pairs\n",
        "def CCFIterate(adjacencyList):\n",
        "  # abstract adjacency list to be reduced by UDF\n",
        "  key, values = adjacencyList\n",
        "  # assume that the key's node is the minimal value of its adjacency list\n",
        "  min = key\n",
        "  # initialize an non iterated adjacency list to search for the min valued edge from \n",
        "  iterable_edges = []\n",
        "  # copy each node's adjacency list to look for the min node inside\n",
        "  for value in values:\n",
        "    # search for the minimum valued edge\n",
        "    if value < min:\n",
        "      # update min value\n",
        "      min = value\n",
        "    # add each value of the adjency list into the copy\n",
        "    iterable_edges.append(value)\n",
        "    # nothing is emitted if the pair's value is not < key\n",
        "    \n",
        "  # if the root, being the key of the adjency list, is not the minimal value:\n",
        "  if min < key:\n",
        "# emit a spark-optimized reduceable pair with min node as value for key as key\n",
        "    yield((key, min)) # vérifier conformité à version bipass avec Mohamed Amine\n",
        "    # check for each values in the adjacency list to be compared to its confirmed min\n",
        "    for value in iterable_edges:\n",
        "      # compare this min with the value to prepare a symetric tuple so that the right node of the pair will be mapped as well\n",
        "      if min != value:\n",
        "        # make sure you the stop criterion is not activated, as long as it's value's false, as long as the adjencency list root is not starting with the minimal value\n",
        "        accum.add(1)\n",
        "        # emit a second spark-optimized distribution from the adjacency list, with min as the MapReduce value so that it can be mapped for CCF again\n",
        "        yield((value, min)) # vérifier conformité à version bipass"
      ],
      "metadata": {
        "id": "IL4JUAUj7hcs",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "203a143e-ae61-4ca3-87b4-b99f14751293"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complexity-optimized by Spark-memory-ready and merge-sort"
      ],
      "metadata": {
        "id": "R7FuPAwE0Gfq",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "99ce3d26-e643-48ea-b5a9-cc969cdc6115"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# break the adjency list into pairs\n",
        "def CCFIterate(adjacency_list):\n",
        "  # abstract adjacency list to be reduced by UDF\n",
        "  key, values = adjacency_list\n",
        "  # Add minimal value to key to be considered in sortByKey\n",
        "  minValue = sort(values)[0]\n",
        "  # copy each node's adjacency list to look for the min node inside\n",
        "  if minValue < key:\n",
        "    # emit a spark-optimized reduceable pair with min node as value for key as key\n",
        "    yield((key, minValue))\n",
        "    # check for each values in the adjacency list to be compared to its confirmed min\n",
        "    for value in iterable_edges:\n",
        "      # make sure you the stop criterion is not activated, as long as it's value's false, as long as the adjencency list root is not starting with the minimal value\n",
        "      accum.add(1)\n",
        "      # add node value, from the adjacency list, to be mapped as the minimal value MapReduce key, so that the minimal value can be mapped for CCF again\n",
        "      yield((value, minValue))\n",
        "      "
      ],
      "metadata": {
        "id": "WS3mXFxj1BA8",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "58ee816f-c3b7-4551-a9b8-eb2ed9759cbd"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Map-reduce job"
      ],
      "metadata": {
        "id": "KpC1Z4zoyhMw",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "97dae5d4-c5a9-4d53-b28d-13e6ee65c8cd"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare complexities\n",
        "begin = time()"
      ],
      "metadata": {
        "id": "f3SPweZnAyvG",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9806c288-78cb-44b5-9d2d-f79d5a32f226"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# global variables tbd\n",
        "\n",
        "# Spark Context Accumulator initialized outside the loop\n",
        "globalAccumulator = sc.accumulator(0)\n",
        "# stop criterion turned on"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "78ec6a40-29e3-4905-b576-9f67e57bd8f0"
        },
        "id": "aK3RJ8XSte-W",
        "outputId": "1a0b032f-2eeb-4354-fe1c-5b05a3f19576"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "data": "",
              "errorSummary": "",
              "metadata": {},
              "errorTraceType": null,
              "type": "ipynbError",
              "arguments": {}
            }
          },
          "data": {
            "text/html": [
              "<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"
            ]
          },
          "transient": null
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We turn Spark readable objects into  MapReduce readable tuples"
      ],
      "metadata": {
        "id": "0s92EjpTyw3B",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "90e5d57a-7548-42a7-97d3-743a0be328f1"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "la7hheCIBjkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check CCF-Iterate stop criterion\n",
        "while True: #to b completed w/ the stop criterion\n",
        "    # Iterate\n",
        "    # Stop criterion\n",
        "    # accumulator's value to be initialized to 0 to maintain boolean range\n",
        "\n",
        "    # Map to create a symetry of each tuple so both elements can be grouped by key\n",
        "    iterateMap = edgesList.groupByKey().flatMap(lambda edge: (e, e[::-1]))\n",
        "  \n",
        "    # Iterate-Reducer\n",
        "      iterateReduce = iterateMap.groupByKey().flatMap(\n",
        "          lambda adjacencyList: CCFIterate(adjacencyList)).sortByKey()\n",
        "\n",
        "    # CCF-dedup Map Reduce job\n",
        "    deduplicatedEdges = iterateReduce.distinct()\n",
        "\n",
        "    edges = deduplicatedEdges # Replace edges with the double removed version\n",
        "    newPair = # tbd reflecting check wether a pair had to be created or not\n",
        "    #"
      ],
      "metadata": {
        "id": "geDoXV7W5ONR",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "274b622b-7ec7-4a9e-810c-c37e0a756ff5"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tHdKQfSi6rw1",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "3140ac72-7b1b-4243-b75b-c5a7cc5ea251"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Timer"
      ],
      "metadata": {
        "id": "yZBlq03U0bCL",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "dc631b35-0820-4b6a-a54d-be1770f76950"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7BvCDb1zaxTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# end complexity evaluation\n",
        "ending = time() - begin\n",
        "print(\"The algorithm took {time:.2f} seconds: \".format(time = ending))"
      ],
      "metadata": {
        "id": "Thmij-fO0brY",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8728cfbb-7a1e-4bce-847f-3ba6fa6e0836"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shuffle & sort"
      ],
      "metadata": {
        "id": "0C2uGvuqozAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# intermediated key – value generated by mapper is sorted automatically by key\n",
        "shuffledEdges = list(map(lambda t: t[::-1], edgesList.collect())) # reverse map tuples for intermediary sort"
      ],
      "metadata": {
        "id": "1pbJc3txPfUr",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "1c27a928-201c-453e-a70d-8e8b49ed048e"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# sort\n",
        "sortedEdges = sc.parallelize(\n",
        "    shuffledEdges).groupByKey().map(lambda s: (a[0], list(a[1]))).collect()"
      ],
      "metadata": {
        "id": "-EyuveXSPfhU",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9993571a-a5e5-4163-a53a-a0ab8c47edea"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity checks"
      ],
      "metadata": {
        "id": "mDtHogg5TPcZ",
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "dcc45672-b702-4d5d-b84f-1d06d48c4cf5"
        }
      },
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "name": "CCF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "CCF",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 769686833560663
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}