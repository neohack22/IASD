{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neohack22/IASD/blob/adversarial/adversarial/Notebook_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FNLmXBHT1Ix"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "QoR1htWy4J0u"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "0s9tYqyrdCtD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8785huAT1I1"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "As we seek to deploy machine learning systems in the real world, it becomes critical that we examine not only whether the systems don't simply work \"most of the time\", but which are truly robust and reliable. Although many notions of robustness and reliability exist, one particular topic has raised a great deal of interest in recent years: robustness to adversarial examples. More precisely, can we develop classifiers that are robust to (test time) perturbations of their inputs, by an adversary intending to fool the classifier? \n",
        "A few years ago, researchers have shown that neural networks are vulnerable to <em> adversarial examples</em>, i.e. imperceptible variations of natural examples, crafted to deliberately mislead the models.\n",
        "\n",
        "<img width=\"500\" src=\"https://languagelog.ldc.upenn.edu/myl/adversarial-example.png\">\n",
        "\n",
        "The purpose of this project is to teach you how these pertubations are crafted and how can we create robust classifier. \n",
        "\n",
        "[1] Nightmare at test time: robust learning by feature deletion <br>\n",
        "[2] Evasion attacks against machine learning at test time <br>\n",
        "[3] Intriguing properties of neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcgxdYFaT1I1"
      },
      "source": [
        "## Some introductory notation\n",
        "\n",
        "Let us consider a model $h_\\theta : \\mathcal{X} \\rightarrow \\mathbb{R}^k$ mapping an input $x$, to a vector $h_\\theta(x)$ in $\\mathbb{R}^k$ (where $k$ is the number of classes being predicted). $h_\\theta(x)$ is called a logit vector of the model $h_\\theta$ where each coordinate can be positive or negative. The $\\theta$ vector represents all the parameters defining this model (for example in a linear model it would represent the slope and the intercept). to get a prediction out of $h_\\theta$ for a given $x$, the classical technique is to output the biggest coordinate of \\begin{equation}\n",
        "\\DeclareMathOperator*{\\argmax}{argmax}\n",
        "\\argmax_{j} h_\\theta(x)_j \n",
        "\\end{equation} where $h_\\theta(x)_j$ denotes the $j$th elements of the vector $h_\\theta(x)$.\n",
        "\n",
        "In order the evaluate the quality of prediction of the model on a given example, we use a loss function $\\ell: \\mathbb{R}^k \\times \\mathbb{Z}_+ \\rightarrow \\mathbb{R}_+$ mapping the model predictions and true labels to a non-negative number. The semantics of this loss function are that the first argument is the model output (logits which can be positive or negative), and the second argument is the _index_ of the true class (that is, a number from 1 to $k$ denoting the index of the true label). Thus, the notation\n",
        "\\begin{equation}\n",
        "\\ell(h_\\theta(x), y)\n",
        "\\end{equation}\n",
        "for $x \\in \\mathcal{X}$ the input and $y \\in \\mathbb{Z}$ the true class, denotes the loss that the classifier achieves in its predictions on $x$, assuming the true class is $y$.  By far the most common form of loss used in machine learning (especially in deep learning) is the cross entropy loss (also sometimes called the softmax loss), defined as\n",
        "\\begin{equation}\n",
        "\\ell(h_\\theta(x), y) = \\log \\left ( \\sum_{j=1}^k \\exp(h_\\theta(x)_j) \\right ) - h_\\theta(x)_y\n",
        "\\end{equation}\n",
        "where $h_\\theta(x)_j$ denotes the $j$th elements of the vector $h_\\theta(x)$.\n",
        "\n",
        "Recall that a common approach to training a classifier is to optimize the _parameters_ $\\theta$, so as to minimize the average loss over some training set $\\{x_i \\in \\mathcal{X}, y_i \\in \\mathbb{Z}\\}$, $i=1,\\ldots,m$, which we write as the optimization problem\n",
        "\\begin{equation}\n",
        "\\DeclareMathOperator*{\\minimize}{minimize}\n",
        "\\minimize_\\theta \\frac{1}{m} \\sum_{i=1}^m \\ell(h_\\theta(x_i), y_i)\n",
        "\\end{equation}\n",
        "which we typically solve by (stochastic) gradient descent.  I.e., for some minibatch $\\mathcal{B} \\subseteq \\{1,\\ldots,m\\}$, we compute the gradient of our loss with respect to the parameters $\\theta$, and make a small adjustment to $\\theta$ in this negative direction\n",
        "\\begin{equation}\n",
        "\\theta := \\theta - \\frac{\\alpha}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\nabla_\\theta \\ell(h_\\theta(x_i), y_i)\n",
        "\\end{equation}\n",
        "where $\\alpha$ is some step size, and we repeat this process for different minibatches covering the entire training set, until the parameters convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x23yruXaT1I2"
      },
      "source": [
        "## Creating an adversarial example\n",
        "\n",
        "To create an adversarial example, instead of adjusting the parameters $\\theta$ with the gradient define above: $\\nabla_\\theta \\ell(h_\\theta(x_i), y_i)$, we are going to adjust the image in order to _maximize_ the loss. Therefore, we want to solve the optimization problem\n",
        "\\begin{equation}\n",
        "\\DeclareMathOperator*{\\maximize}{maximize}\n",
        "\\maximize_{\\hat{x}} \\ell(h_\\theta(\\hat{x}), y)\n",
        "\\end{equation}\n",
        "where $\\hat{x}$ denotes our adversarial example that is attempting to maximize the loss.\n",
        "\n",
        "Of course, we cannot just optimize arbitrarily over $\\hat{x}$ (there do exist, after all, some images that are _not_ pandas, and if we change the image entirely, say to a dog, then it's not particularly impressive that we can \"fool\" the classifier into thinking it's not a panda). So we instead need to ensure that $\\hat{x}$ is close to our original input $x$. By convention, we typically do this by optimizing over the _perturbation_ to $x$, which we will denote $\\delta$\n",
        "\\begin{equation}\n",
        "\\maximize_{\\delta \\in \\Delta} \\ell(h_\\theta(x +\\delta), y)\n",
        "\\end{equation}\n",
        "where $\\Delta$ represents an allowable set of perturbations (typically perturbation that make $x + \\tau$ sufficiently close to $x$).  \n",
        "\n",
        "Characterizing the \"correct\" set of allowable perturbations is actually quite difficult: in theory, we would like $\\Delta$ to capture anything that humans visually feel to be the \"same\" as the original input $x$.  This can include anthing ranging from adding slight amounts of noise, to rotating, translating, scaling, or performing some 3D transformation on the underlying model, or even completely changing the image in the \"non-panda\" locations.  Needless to say, it is not possible to give a mathematically rigorous definition of all the perturbations that _should_ be allowed, but the philosophy behind adversarial examples is that we can consider some _subset_ of the possible space of allowed perturbations, such that by any \"reasonable\" definition, the actual semantic content of the image could not change under this perturbation.\n",
        "\n",
        "A common perturbation set to use, though by no means the only reasonable choice, is the $\\ell_\\infty$ ball, defined by the set\n",
        "\\begin{equation}\n",
        "\\Delta = \\{\\delta : \\|\\delta\\|_\\infty \\leq \\epsilon\\}\n",
        "\\end{equation}\n",
        "where the $\\ell_\\infty$ norm a vector $z$ is defined as\n",
        "\\begin{equation}\n",
        "\\|z\\|_\\infty = \\max_i |z_i|\n",
        "\\end{equation}\n",
        "i.e., we allow the perturbation to have magnitude between $[-\\epsilon, \\epsilon]$ in each of its components (it is a slightly more complex, as we also need to ensure that $x + \\delta$ is also bounded between $[0,1]$ so that it is still a valid image).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXFQ3d_AT1I2"
      },
      "source": [
        "## Binary classification\n",
        "\n",
        "Let's begin first by considering a simplified problem where $h_\\theta$ is a linear classifier (see below) and k=2.  In this case called binary classification, rather than use multi-class cross entropy loss, we'll be adopting the more common approach and using the binary cross entropy, or logistic loss. In this setting, we have our hypothesis function\n",
        "\\begin{equation}\n",
        "h_\\theta(x) = w^T x + b\n",
        "\\end{equation}\n",
        "for $\\theta = \\{w \\in \\mathbb{R}^n, b \\in \\mathbb{R}\\}$, class label $y \\in \\{+1,-1\\}$, and loss function\n",
        "\\begin{equation}\n",
        "\\ell(h_\\theta(x), y) = \\log(1+\\exp(-y\\cdot h_\\theta(x))) \\equiv L(y \\cdot h_\\theta(x))\n",
        "\\end{equation}\n",
        "where for convience below we define the function $L(z) = \\log(1+\\exp(-z))$ which we will use below when discussing how to solve the optimization problems involving this loss.  The semantics of this setup are that for a data point $x$, the classifier predicts class $+1$ with probability\n",
        "\\begin{equation}\n",
        "p(y=+1|x) = \\frac{1}{1 + \\exp(-h_\\theta(x))}.\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6W4asxRT1I3"
      },
      "source": [
        "### Computing an Adversarial pertubation\n",
        "\n",
        "Let us consider the creation of an adversarial example in the context of binary classification:\n",
        "\\begin{equation}\n",
        "\\DeclareMathOperator*{\\maximize}{maximize}\n",
        "\\maximize_{\\|\\delta\\| \\leq \\epsilon} \\ell(w^T (x+\\delta), y) \\equiv \\maximize_{\\|\\delta\\| \\leq \\epsilon} L(y \\cdot (w^T(x+\\delta) + b)).\n",
        "\\end{equation}\n",
        "The key point we need to make here is that in this setting, it is actually possible to solve this maximization problem exactly. \n",
        "\n",
        "Because the function is monotonic decreasing, if we want to maximize this function applied to a scalar, that is equivalent to just minimizing the scalar quantity.  That is\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\DeclareMathOperator*{\\minimize}{minimize}\n",
        "\\max_{\\|\\delta\\| \\leq \\epsilon} L \\left(y \\cdot (w^T(x+\\delta) + b) \\right) & =\n",
        "L\\left( \\min_{\\|\\delta\\| \\leq \\epsilon}  y \\cdot (w^T(x+\\delta) + b) \\right) \\\\\n",
        "& = L\\left(y\\cdot(w^Tx + b) + \\min_{\\|\\delta\\| \\leq \\epsilon} y \\cdot w^T\\delta  \\right)\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "where we get the second line by just distributing out the linear terms.\n",
        "\n",
        "Thanks to this simplification of the initial problem, we only need to consider how to solve the problem\n",
        "\\begin{equation}\n",
        "\\min_{\\|\\delta\\| \\leq \\epsilon} y \\cdot w^T\\delta.\n",
        "\\end{equation}\n",
        "To get the intuition here, let's just consider the case that $y = +1$, and consider an $\\ell_\\infty$ norm constraint $\\|\\delta\\|_\\infty \\leq \\epsilon$.  Since the $\\ell_\\infty$ norm says that each element in $\\delta$ must have magnitude less than or equal $\\epsilon$, we clearly minimize this quantity when we set $\\delta_i = -\\epsilon$ for $w_i \\geq 0$ and $\\delta_i = \\epsilon$ for $w_i < 0$.  For $y = -1$, we would just flip these quantities.  That is, the optimal solution to the above optimization problem for the $\\ell_\\infty$ norm is given by\n",
        "\\begin{equation}\n",
        "\\delta^\\star = - y \\epsilon \\cdot \\mathrm{sign}(w)\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SvwRodvT1I3"
      },
      "source": [
        "### Exercice 1\n",
        "\n",
        "Let's see what this looks like for an actual linear classifier.  In doing so, we can also get a sense of how well traditional linear models might work to also prevent adversarial examples.\n",
        "\n",
        "1. Define and train with gradient descent a linear model using the PyTorch library a binarize version of the MNIST dataset\n",
        "2. Evaluate your model on the train and test dataset \n",
        "3. Create adversarial pertubation on the images of the test set and evaluate the performance of your model <em>under attack</em>\n",
        "4. Evaluate the visibility of the pertubation by ploting the natural image next to the adversarial image \n",
        "5. Make a small analysis and conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEPn1Fg7T1I4"
      },
      "outputs": [],
      "source": [
        "# binary MNIST dataset\n",
        "def load_binary_mnist(split, batch_size):\n",
        "  train = True if split == 'train' else False\n",
        "  dataset = datasets.MNIST(\"./data\", train=split, download=True, transform=transforms.ToTensor())\n",
        "  idx = dataset.targets <= 1\n",
        "  dataset.data = dataset.data[idx]\n",
        "  dataset.targets = dataset.targets[idx]\n",
        "  return DataLoader(dataset, batch_size=batch_size, shuffle=train)\n",
        "\n",
        "batch_size = 100\n",
        "train_loader = load_binary_mnist('train', batch_size)\n",
        "test_loader = load_binary_mnist('test', batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# binary MNIST dataset\n",
        "def load_binary_mnist(split, batch_size):\n",
        "  train = True if split == 'train' else False\n",
        "  dataset = datasets.MNIST(\n",
        "      \"./data\", train=split, download=True, transform=transforms.ToTensor() #.ToTensor()\n",
        "  )\n",
        "  idx = dataset.targets <= 1\n",
        "  dataset.data = dataset.data[\n",
        "                              idx\n",
        "  ]\n",
        "  dataset.targets = dataset.targets[\n",
        "                                    idx\n",
        "  ]\n",
        "  return DataLoader(\n",
        "      dataset, batch_size=batch_size, shuffle=train\n",
        "  )\n",
        "\n",
        "batch_size = 100\n",
        "train_loader = load_binary_mnist(\n",
        "    'train', batch_size\n",
        ")  \n",
        "test_loader = load_binary_mnist(\n",
        "    'test', batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "fnvl3lgNIaH6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your model\n",
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(\n",
        "        Model, self\n",
        "    ).__init__()\n",
        "    "
      ],
      "metadata": {
        "id": "O745R-MiPUBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_iters = 3000\n",
        "epochs = n_iters / (\n",
        "    len(\n",
        "        train_loader#_dataset\n",
        "    ) / batch_size\n",
        ")\n",
        "input_dim = 784\n",
        "output_dim = 10 #0\n",
        "lr_rate = 0.001"
      ],
      "metadata": {
        "id": "aGVqxPZ4cAs8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kotSpgOIT1I7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98706098-09d1-4953-f38f-8a434b412af1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500. Loss: 0.23249290883541107. Accuracy: 99.39202880859375.\n",
            "Iteration: 1000. Loss: 0.10978590697050095. Accuracy: 99.52625274658203.\n"
          ]
        }
      ],
      "source": [
        "# Define your model\n",
        "class Model(torch.nn.Module):  \n",
        "  \n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    # code here ...\n",
        "    self.linear = torch.nn.Linear(\n",
        "        input_dim, output_dim\n",
        "    )\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # code here ...\n",
        "    outputs = self.linear(x)\n",
        "    return outputs\n",
        "\n",
        "model = Model()\n",
        "\n",
        "# define your loss\n",
        "criterion = torch.nn.CrossEntropyLoss() # code here ...\n",
        "\n",
        "# define your optimizer\n",
        "opt = torch.optim.SGD(\n",
        "    model.parameters(), lr=lr_rate) # code here ...\n",
        "\n",
        "epochs = 10\n",
        "# train your model\n",
        "iter = 0\n",
        "for epoch in range(\n",
        "    int(\n",
        "        epochs\n",
        "    )\n",
        "): #epochs):\n",
        "  for batch_n, (\n",
        "      images, labels\n",
        "  ) in enumerate(\n",
        "      train_loader\n",
        "  ): #imgs, labels) in enumerate(train_loader):\n",
        "    # code here ...\n",
        "    images = Variable(\n",
        "        images.view(\n",
        "            -1, 28 * 28\n",
        "        )\n",
        "    )\n",
        "    labels = Variable(labels)\n",
        "\n",
        "    opt.zero_grad() #imizer.zero_grad()\n",
        "    outputs = model(\n",
        "        images\n",
        "    )\n",
        "    loss = criterion(\n",
        "        outputs, labels\n",
        "    )\n",
        "    loss.backward()\n",
        "    opt.step() #imizer.step()\n",
        "\n",
        "    iter+=1\n",
        "    if iter%500==0:\n",
        "      # calculate Accuracy\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for images, labels in test_loader:\n",
        "        images = Variable(\n",
        "            images.view(\n",
        "                -1, 28*28\n",
        "            )\n",
        "        )\n",
        "        outputs = model(\n",
        "            images\n",
        "        )\n",
        "        _, predicted = torch.max(\n",
        "            outputs.data, 1\n",
        "        )\n",
        "        total+= labels.size(\n",
        "            0\n",
        "        )\n",
        "\n",
        "        correct+= (\n",
        "            predicted == labels\n",
        "        ).sum()\n",
        "      accuracy = 100 * correct/total\n",
        "      print(\n",
        "          \"Iteration: {}. Loss: {}. Accuracy: {}.\".format(\n",
        "              iter, loss.item(), accuracy\n",
        "          )\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvgHd9cuT1I-"
      },
      "outputs": [],
      "source": [
        "# evalutation on test set\n",
        "accuracy = 0.\n",
        "n_inputs = 0.\n",
        "for n_batch, (imgs, labels) in enumerate(test_loader):\n",
        "  # code here ..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation on test set\n",
        "accuracy1 = 0.\n",
        "n_inputs = 0.\n",
        "for n_batch, (\n",
        "    imgs, labels\n",
        ") in enumerate(\n",
        "    test_loader):\n",
        "  imgs = Variable( #images = Variable(\n",
        "      imgs.view( #images.view(\n",
        "          -1, 28*28\n",
        "      )\n",
        "  )\n",
        "  outputs = model(\n",
        "      imgs #images\n",
        "  )\n",
        "  _, predicted = torch.max(\n",
        "      outputs.data, 1\n",
        "  )\n",
        "  total+= labels.size(\n",
        "      0\n",
        "  )\n",
        "\n",
        "  correct+= (\n",
        "      predicted == labels\n",
        "  ).sum()\n",
        "accuracy1 = 100 * correct/total\n",
        "print(\n",
        "    \"Iteration : {}. Loss: {}. Accuracy1: {}.\".format(\n",
        "        iter, loss.item(), accuracy1\n",
        "    )\n",
        ")\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuubtkwnhKmG",
        "outputId": "1e0c45f4-9be3-4a04-f23a-545c99aad59b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration : 1270. Loss: 0.07534432411193848. Accuracy1: 99.55388641357422.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs3dUPwaT1JA"
      },
      "outputs": [],
      "source": [
        "# compute and plot the adversairal pertubation\n",
        "eps = 0.2\n",
        "delta = eps * model.linear.weight.detach().sign()\n",
        "plt.imshow(1-delta.reshape(28, 28), cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute and plot the adversarial perturbation\n",
        "eps = 0.2\n",
        "delta = eps * model.linear.weight.detach().sign()\n",
        "plt.imshow(\n",
        "    1-delta[0].reshape(\n",
        "        28, 28\n",
        "    ), cmap=\"gray\"\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "sPrV8qTCo5Xr",
        "outputId": "fe1cf5b3-2f18-4390-9eec-e2a66cc9365f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANVUlEQVR4nO3dX8hkdR3H8c8nqxvrYk17WNalf3gjQdozLEEShhTmzdpNtBexkfR0kVHQRWIXChFIZOFFBGtKW5QRqLgXUtkSWTfh88imq1KarLjLuqt4oV6V+u1ijvG4zcyZnd858zvzfN8vGJ55zjPnzPecmc9zzpzf/M7PESEAO987ahcAYDkIO5AEYQeSIOxAEoQdSOKdy3wy2zvy1P/6+nrtEqba2toqmr/murXV3lbbrPlL16t0u85Ssl6SFBGeNN0lTW+2r5V0h6QLJP0sIm5refyODPuQmy/tia/73GquW1vtbbXNmr90vUq36ywl69XMP/EBCx/G275A0k8kfU7S5ZIO2L580eUB6FfJZ/Z9kp6JiGcj4t+SfiNpfzdlAehaSdj3SHp+2+8nm2lvY3vD9qbtzYLnAlCo9xN0EXFI0iFp535mB1ZByZ79lKS9236/tJkGYIBKwv6IpMtsf8j2uyV9UdKRbsoC0LWFD+Mj4nXbN0r6vcZNb3dHxBOz5llfX9fm5mp+dJ/V3NFnM4xUt4mpTWnzWC19v2ZtZm2Xvmor+sweEQ9KerCjWgD0iK/LAkkQdiAJwg4kQdiBJAg7kARhB5JYan/2NjW7Dfa57NL1qt0mPEuf695BV8+Fn7vmNu9rm7JnB5Ig7EAShB1IgrADSRB2IAnCDiQxqKa3PpvHhtx81abvpr0SfTat1TTkq88uij07kARhB5Ig7EAShB1IgrADSRB2IAnCDiQxqHb2Ve3i2nebap/Lr3kp6CFvt53YvZY9O5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4k4WX2KbY988lqDk1c0m5a2i7aZ5vuEPtVr4Kafe07eD9NXEDRl2psn5D0qqQ3JL0eEaOS5QHoTxffoPt0RLzUwXIA9IjP7EASpWEPSX+wvWV7Y9IDbG/Y3rS9WfhcAAoUnaCzvSciTtl+v6SHJH0jIh6e8XhO0J3nsudZPifourcTT9AV7dkj4lTz86yk+yXtK1kegP4sHHbbF9p+71v3JX1W0vGuCgPQrZKz8WuS7m8OOd4p6dcR8buSYmr2re7zI0TbepWuN4fq56/mx7555l902aPR9NbvhcMeEc9K+tii8wNYLpregCQIO5AEYQeSIOxAEoQdSGJQl5JeVUO+JDImq/2a1bjUNHt2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUhiUO3stboF9j0/7eQ7T59dZPvqys2eHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSWGrY19fXFRFTb7Zn3kqs6rKxmtreE7Ny0GbWvOvr61PnY88OJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kstT/71tbWYNudh1rX0NUc6rqmPodsrtaf3fbdts/aPr5t2kW2H7L9dPNzVy/VAejMPIfxP5d07TnTbpJ0NCIuk3S0+R3AgLWGPSIelvTyOZP3Szrc3D8s6fqO6wLQsUU/s69FxOnm/guS1qY90PaGpI0FnwdAR4pP0EVE2J56RiEiDkk6JEmzHgegX4s2vZ2xvVuSmp9nuysJQB8WDfsRSQeb+wclPdBNOQD60noYb/seSVdLutj2SUm3SLpN0m9t3yDpOUlf6LPIIZjV9jnk9uDa2G7D0Rr2iDgw5U/XdFwLgB7xdVkgCcIOJEHYgSQIO5AEYQeSGNSlpNtuJUqet68uh6ugdLtwie3JarzX2LMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBJeZhtyn1eqWeXLEg9Zn++PzK9JyXad4zLWEx/Anh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkhhUf/aadmp/9lXuxz/k2krV6OfPnh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkmgdxXVVlLZPrnLf6pJhkYfcXr1TX5NaWvfstu+2fdb28W3TbrV9yvax5nZdv2UCKDXPYfzPJV07YfqPI+KK5vZgt2UB6Fpr2CPiYUkvL6EWAD0qOUF3o+3HmsP8XdMeZHvD9qbtzRdffLHg6QCUWDTsP5X0EUlXSDot6fZpD4yIQxExiojRJZdcsuDTASi1UNgj4kxEvBERb0q6U9K+bssC0LWFwm5797ZfPy/p+LTHAhiG1nZ22/dIulrSxbZPSrpF0tW2r5AUkk5I+lqPNc6ltF1zldt0V9UqX+u/tPaS9+useUej0dS/tYY9Ig5MmHzXXFUBGAy+LgskQdiBJAg7kARhB5Ig7EASg+ri2ucwtlkNsavlW1b5NavZdXjR7caeHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSGFQ7e0nb5Sp3l2xTsm6129lXebuXKFnvttds0S6u7NmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAkvsx3W9swnoz/7YobYd3roal4Kuk3pNo+IiQtgzw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSQyqP3uJndyfvc1OXre+lG6zmtu8t/7stvfa/pPtJ20/YfubzfSLbD9k++nm565FCgewHPMcxr8u6dsRcbmkT0j6uu3LJd0k6WhEXCbpaPM7gIFqDXtEnI6IR5v7r0p6StIeSfslHW4edljS9X0VCaDceX1mt/1BSVdK+puktYg43fzpBUlrU+bZkLSxeIkAujD32Xjb75F0r6RvRcQr2/8W4zMGE88aRMShiBhFxPQzBwB6N1fYbb9L46D/KiLuayafsb27+ftuSWf7KRFAF+Y5G29Jd0l6KiJ+tO1PRyQdbO4flPRAaTG2Z96ArkTEzFvN+fsyz2f2T0r6kqTHbR9rpt0s6TZJv7V9g6TnJH2hnxIBdKE17BHxV0nTdqvXdFsOgL7wdVkgCcIOJEHYgSQIO5AEYQeSWGoX1/X1dW1ubk79e59t6aXtl7Tzr56S17zvS6yXvJ8WnZc9O5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4ksdR29q2trZlthEMesnnIQ/Suqr6/+1CyXfu+NPms5ZcMF110KWkAOwNhB5Ig7EAShB1IgrADSRB2IAnCDiQxqCGbS9oX29Qc0rnPNtm25Zeud83rAJTWVqPP+DLQnx3ATIQdSIKwA0kQdiAJwg4kQdiBJAg7kERrO7vtvZJ+IWlNUkg6FBF32L5V0lclvdg89OaIeLCkmCG3m/bVD79vfa631L7uffXbnmf+Ppc95Nd8Gs+x0rsl7Y6IR22/V9KWpOs1Ho/9tYj44dxPZs98sppffClR80IHXSy/RJ8XHMka9g7eLxMXMM/47KclnW7uv2r7KUl7iqoBsHTn9Znd9gclXSnpb82kG20/Zvtu27umzLNhe9P29HGfAPSu9TD+fw+03yPpz5K+HxH32V6T9JLGn+O/p/Gh/ldalsFh/ACXX4LD+O71dRg/157d9rsk3SvpVxFxX7PAMxHxRkS8KelOSfuKKgTQq9awe/xv5i5JT0XEj7ZN373tYZ+XdLz78gB0ZZ6z8VdJ+oukxyW92Uy+WdIBSVdofBh/QtLXmpN5U41GoygZsrmkGaemmoeMtbuw9tn01qbP5tKal5KeY9kLn43/q6RJMxe1qQNYLr5BByRB2IEkCDuQBGEHkiDsQBKEHUhi7q/LdvJkLV+XLdF3W/aQu7hmra3m9xNK9fW6jEYjbW5uLv51WQCrj7ADSRB2IAnCDiRB2IEkCDuQBGEHklj2kM0vSXpu2+8XN9OK9XDJ5LlrW3Jf+vPaZplrO2f5nb3XutBjbR+Y+pw1v3RhezMiRtUKmGGotQ21LonaFrWs2jiMB5Ig7EAStcN+qPLzzzLU2oZal0Rti1pKbVU/swNYntp7dgBLQtiBJKqE3fa1tv9h+xnbN9WoYRrbJ2w/bvtY7fHpmjH0zto+vm3aRbYfsv1083PiGHuVarvV9qlm2x2zfV2l2vba/pPtJ20/YfubzfSq225GXUvZbkv/zG77Akn/lPQZSSclPSLpQEQ8udRCprB9QtIoIqp/AcP2pyS9JukXEfHRZtoPJL0cEbc1/yh3RcR3BlLbrTrPYbx7qm3aMONfVsVt1+Xw54uosWffJ+mZiHg2Iv4t6TeS9leoY/Ai4mFJL58zeb+kw839wxq/WZZuSm2DEBGnI+LR5v6rkt4aZrzqtptR11LUCPseSc9v+/2khjXee0j6g+0t2xu1i5lgbdswWy9IWqtZzAStw3gv0znDjA9m2y0y/HkpTtD9v6si4uOSPifp683h6iDF+DPYkNpOfyrpIxqPAXha0u01i2mGGb9X0rci4pXtf6u57SbUtZTtViPspyTt3fb7pc20QYiIU83Ps5Lu1/CGoj7z1gi6zc+zlev5nyEN4z1pmHENYNvVHP68RtgfkXSZ7Q/ZfrekL0o6UqGO/2P7wubEiWxfKOmzGt5Q1EckHWzuH5T0QMVa3mYow3hPG2Zclbdd9eHPI2LpN0nXaXxG/l+Svlujhil1fVjS35vbE7Vrk3SPxod1/9H43MYNkt4n6aikpyX9UdJFA6rtlxoP7f2YxsHaXam2qzQ+RH9M0rHmdl3tbTejrqVsN74uCyTBCTogCcIOJEHYgSQIO5AEYQeSIOxAEoQdSOK/nvyLyXBnFCEAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4rUG-EoT1JD"
      },
      "outputs": [],
      "source": [
        "# [CELL ID] 11\n",
        "\n",
        "# make an evaluation \"under attack\"\n",
        "accuracy = 0.\n",
        "n_inputs = 0.\n",
        "for n_batch, (imgs, labels) in enumerate(test_loader):\n",
        "  # code here ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ebUf1BxrT1JG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3812460d-c49b-47a0-be8f-19f0d0c7d709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration : 1270. Loss: 0.07534432411193848. Accuracy2: 75.9626235961914.\n"
          ]
        }
      ],
      "source": [
        "# evaluation on test set\n",
        "accuracy2 = 0.\n",
        "n_inputs = 0.\n",
        "for n_batch, (\n",
        "    imgs, labels\n",
        ") in enumerate(\n",
        "    test_loader):\n",
        "  imgs = Variable( #images = Variable(\n",
        "      imgs.view( #images.view(\n",
        "          -1, 28*28\n",
        "      )\n",
        "  )\n",
        "  outputs = model(\n",
        "      imgs + (2* labels.view(-1,1) - 1) * delta[0] #images\n",
        "  )\n",
        "  _, predicted = torch.max(\n",
        "      outputs.data, 1\n",
        "  )\n",
        "  total+= labels.size(\n",
        "      0\n",
        "  )\n",
        "\n",
        "  correct+= (\n",
        "      predicted == labels\n",
        "  ).sum()\n",
        "accuracy2 = 100 * correct/total\n",
        "print(\n",
        "    \"Iteration : {}. Loss: {}. Accuracy2: {}.\".format(\n",
        "        iter, loss.item(), accuracy2\n",
        "    )\n",
        ")\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Wbsec2V8T1JK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "06f51ffb-6ab8-437e-bec0-08471b1dbeb7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAO9UlEQVR4nO3db4hd9Z3H8c9Ht+aBzQRdd0NIxu2f+MQ/rJUQNowsStmieaJ9YsyDEkEaH1RpoZIV90EFWZHVtkSyFKarmEo3UmirPohsXSlIRigmIRsnurv+QTEhJhVhxj7Kqt99MEeZ6txzxvu7554z832/IMyd87vnnO89935y7tzf/Z2fI0IAVr/zui4AwHgQdiAJwg4kQdiBJAg7kMRfjHNntlflR/8TExNdlzDQ/Px80fpdPram2ptqq1u/9HGVHtc6JY9LkiLCSy0vCrvtGyTtlXS+pH+LiAdLtrdSTU1NdV3CQM8++2zR+l0+tqbam2qrW7/0cZUe1zolj6vO0G/jbZ8v6V8l3Sjpckk7bV8+7PYAtKvkb/atkl6PiDcj4pykJyXdNJqyAIxaSdg3Snpn0e8nq2V/xvZu24dtHy7YF4BCrX9AFxHTkqal1fsBHbASlJzZT0maXPT7pmoZgB4qCftLki6z/VXbF0i6VdIzoykLwKgN/TY+Ij60faek/9BC19tjEXGibp2JiYled1PVqevuaLMbRpJuvPHG2va6/TetW6rpsbe9/2G1/Zw1qTsubdVW9Dd7RByUdHBEtQBoEV+XBZIg7EAShB1IgrADSRB2IAnCDiQx1vHsTdrs+2yzv7ekH3w5uu4TrtPmYy/ddslz3uUxb+uYcmYHkiDsQBKEHUiCsANJEHYgCcIOJNGrrrc2u8f63H3VpO2uvRJtdq11qbS2Pr7eOLMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBK96mdfqUNc2+5TbXP7XV4Kus/HbTUOr+XMDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJOCLGtzO7dmddTk1c0m9a2i/aZp9uH8dVf2LNmjW17Tt27Khtv/XWW2vbDx4cPMHwvn37atftcqx96XMWEV5qedGXamy/JekDSR9J+jAitpRsD0B7RvENuusj4r0RbAdAi/ibHUiiNOwh6Xe2j9jevdQdbO+2fdj24cJ9AShQ+jb+2og4ZfuvJT1n+78j4oXFd4iIaUnTUvMHdADaU3Rmj4hT1c+zkn4raesoigIwekOH3faFttd+clvStyTNjqowAKM1dD+77a9p4WwuLfw58O8R8c8N6xS9je9r32fb13Xv83XjS1x//fW17XfffXfR9ufm5ga2PfHEE0XbLh3P3tZY+5mZGc3NzY22nz0i3pT0t8OuD2C86HoDkiDsQBKEHUiCsANJEHYgiV4NcW3S1643DLZu3bqBbQcOHKhdt+m1aS/Zw/Spuq63nTt31q7btjYvNT1oiCtndiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IoldTNnc1LLDt9TP30dcNUy39jkfT+jMzM0Xbr9Pmpcvb+j4JZ3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSGKs/ewTExOampoa2N5mf3TppX9Ltr2abd68ubb9mmuuGdjWNB69qR/9vffq5xM9evRobXub2ny9NV1KehDO7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQxIq6bjzGb3Jysrb9oYceqm1fu3btwLbt27fXrnvw4MHa9j179tS2nzhxora9TSXXZijpg6+bsrnxzG77Mdtnbc8uWnax7edsv1b9vGjo6gCMxXLexj8u6YbPLLtH0vMRcZmk56vfAfRYY9gj4gVJ739m8U2S9le390u6ecR1ARixYb8bvz4iTle335W0ftAdbe+WtHvI/QAYkeKBMBERdR+8RcS0pGmJD+iALg3b9XbG9gZJqn6eHV1JANowbNifkbSrur1L0tOjKQdAWxrfxts+IOk6SZfYPinpR5IelPQr27dLelvSLW0W2Qd1fZ+reTx73fUHJGnHjh1Db7vpuD388MO17V32o69EjWGPiEGz1n9zxLUAaBFflwWSIOxAEoQdSIKwA0kQdiCJXl1KukmXUzavVuvWratt37t3b9H264apzs/P1647Oztb276SdfF65MwOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0msmktJl1y6N7O77rqrtv2RRx5pbd+XXnppbfs777zT2r671uYU4REx3KWkAawOhB1IgrADSRB2IAnCDiRB2IEkCDuQRK/Gs3fZF75a++mbHtf9999f2970PQx7yS7dTx06dGhgW1M/+mp9TqT2pmyuw5kdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5IYaz97m0r7XFdyn21dv+yVV15Zu27TdeObHDlypLb9gQceGHrbq/U56Urjmd32Y7bP2p5dtOw+26dsH6v+bW+3TACllvM2/nFJNyyx/KcRcXX1b/C0HwB6oTHsEfGCpPfHUAuAFpV8QHen7ePV2/yLBt3J9m7bh20fPnfuXMHuAJQYNuw/k/R1SVdLOi3px4PuGBHTEbElIrZccMEFQ+4OQKmhwh4RZyLio4j4WNLPJW0dbVkARm2osNvesOjXb0tavXPrAqtEYz+77QOSrpN0ie2Tkn4k6TrbV0sKSW9JuqPFGpeltF9zJffpTk5ODmzbtm1b0babxrO/+OKLQ297JY9XL6295PVat+7MzMzAtsawR8TOJRY/uqyqAPQGX5cFkiDsQBKEHUiCsANJEHYgibFO2bxu3bqou5R0iT5307St7pLMmzZtKtr27Gz9Vyj27Nkz9LZX83PW5hBXpmwGUIuwA0kQdiAJwg4kQdiBJAg7kARhB5Lo1aWkS4YFruThkk2aHtvx48cHtm3cuLF23aYpl0uGsEor+7iXKHncTc/3sENcObMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBJj7Wefn5+v7X8sGQO8mvtzm6Zd3r59+El0n3rqqdr2O+7o/CrhrejyUtBN2notc2YHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSTGet1427U7W4l9l6NQN+WyJO3du7e2fc2aNUPve9++fbXtfT5uWTWNZ5+bmxvuuvG2J23/3vYrtk/Y/n61/GLbz9l+rfp50dDVA2jdct7GfyjphxFxuaS/k/Q925dLukfS8xFxmaTnq98B9FRj2CPidEQcrW5/IOlVSRsl3SRpf3W3/ZJubqtIAOW+0HfjbX9F0jck/UHS+og4XTW9K2n9gHV2S9o9fIkARmHZn8bb/rKkX0v6QUTML26LhU/5lvzwLSKmI2JLRGwpqhRAkWWF3faXtBD0X0bEb6rFZ2xvqNo3SDrbTokARqHxbbwXrjX8qKRXI+Ini5qekbRL0oPVz6dLi+lyWGGXtmypf9PT1LV23nmD/88+dOhQ7bpZu9baHuJasn5bz8ly/mafkvQdSS/bPlYtu1cLIf+V7dslvS3pllYqBDASjWGPiEOSBs0k8M3RlgOgLXxdFkiCsANJEHYgCcIOJEHYgSTGeinpiYkJTU1NDWxvs8+3tI++zdq2bdtWtP7HH388sO3JJ58s2vZKVvKct/2djpLX07DrcmYHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSSYsnmZ2ux3nZ2drW2/4ooratvrxrP3WdvffSh5TZSOVy/ZfslY+JmZmYFtK/NVAuALI+xAEoQdSIKwA0kQdiAJwg4kQdiBJMbaz96kzevGt91vWrLtzZs317Y//vjjte233XbbwLarrrqqdt033nijtr3L6wC0fe32ttZtG+PZAdQi7EAShB1IgrADSRB2IAnCDiRB2IEkHBH1d7AnJf1C0npJIWk6Ivbavk/SdyX9sbrrvRFxsGFbtTvrsi+8RJdjn0ex/RJtXoOgzcfd9vzsJUqfz4hYctbl5Xyp5kNJP4yIo7bXSjpi+7mq7acR8XBRZQDGYjnzs5+WdLq6/YHtVyVtbLswAKP1hf5mt/0VSd+Q9Idq0Z22j9t+zPZFA9bZbfuw7cNFlQIosuyw2/6ypF9L+kFEzEv6maSvS7paC2f+Hy+1XkRMR8SWiNgygnoBDGlZYbf9JS0E/ZcR8RtJiogzEfFRRHws6eeStrZXJoBSjWG3bUmPSno1In6yaPmGRXf7tqT6S6QC6NRyPo2fkvQdSS/bPlYtu1fSTttXa6E77i1JdzRtqHTK5pLL77apyy6kpu233YXUZRdVn7vmSrffhuV8Gn9I0lL9drV96gD6hW/QAUkQdiAJwg4kQdiBJAg7kARhB5JoHOI60p01DHEt0faQxbammh6FrLW12ce/UqcAn5mZ0dzc3JJDXDmzA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAS4+5n/6OktxctukTSe2Mr4Ivpa219rUuitmGNsra/iYi/WqphrGH/3M7tw329Nl1fa+trXRK1DWtctfE2HkiCsANJdB326Y73X6evtfW1LonahjWW2jr9mx3A+HR9ZgcwJoQdSKKTsNu+wfb/2H7d9j1d1DCI7bdsv2z7WNfz01Vz6J21Pbto2cW2n7P9WvVzyTn2OqrtPtunqmN3zPb2jmqbtP1726/YPmH7+9XyTo9dTV1jOW5j/5vd9vmS/lfSP0g6KeklSTsj4pWxFjKA7bckbYmIzr+AYfvvJf1J0i8i4spq2b9Iej8iHqz+o7woIv6xJ7XdJ+lPXU/jXc1WtGHxNOOSbpZ0mzo8djV13aIxHLcuzuxbJb0eEW9GxDlJT0q6qYM6ei8iXpD0/mcW3yRpf3V7vxZeLGM3oLZeiIjTEXG0uv2BpE+mGe/02NXUNRZdhH2jpHcW/X5S/ZrvPST9zvYR27u7LmYJ6yPidHX7XUnruyxmCY3TeI/TZ6YZ782xG2b681J8QPd510bENZJulPS96u1qL8XC32B96jtd1jTe47LENOOf6vLYDTv9eakuwn5K0uSi3zdVy3ohIk5VP89K+q36NxX1mU9m0K1+nu24nk/1aRrvpaYZVw+OXZfTn3cR9pckXWb7q7YvkHSrpGc6qONzbF9YfXAi2xdK+pb6NxX1M5J2Vbd3SXq6w1r+TF+m8R40zbg6PnadT38eEWP/J2m7Fj6Rf0PSP3VRw4C6vibpv6p/J7quTdIBLbyt+z8tfLZxu6S/lPS8pNck/aeki3tU2xOSXpZ0XAvB2tBRbddq4S36cUnHqn/buz52NXWN5bjxdVkgCT6gA5Ig7EAShB1IgrADSRB2IAnCDiRB2IEk/h8uQoJrj7H8BgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# compute and plot the adversarial perturbation\n",
        "eps = 0.2\n",
        "delta = eps * model.linear.weight.detach().sign()\n",
        "plt.imshow(\n",
        "    imgs[0].view(28,28)-delta[0].reshape( \n",
        "        28, 28\n",
        "    ), cmap=\"gray\"\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kf5vDuVET1JM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDmGlpCPT1JO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFeWct1DT1JQ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Notebook_1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}